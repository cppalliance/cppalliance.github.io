<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://cppalliance.org/feed.xml" rel="self" type="application/atom+xml" /><link href="http://cppalliance.org/" rel="alternate" type="text/html" /><updated>2020-11-09T22:19:30+00:00</updated><id>http://cppalliance.org/feed.xml</id><title type="html">The C++ Alliance</title><subtitle>The C++ Alliance is dedicated to helping the C++ programming language evolve. We see it developing as an ecosystem of open source libraries and as a growing community of those who contribute to those libraries..</subtitle><entry><title type="html">Richard’s October Update</title><link href="http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate.html" rel="alternate" type="text/html" title="Richard’s October Update" /><published>2020-10-31T00:00:00+00:00</published><updated>2020-10-31T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate.html">&lt;h1 id=&quot;asio-coroutines-in-qt-applications&quot;&gt;Asio Coroutines in Qt applications!&lt;/h1&gt;

&lt;p&gt;I started this train of thought when I wanted to hook up some back-end style code that I had written to a gui front end.
One way to do this would be to have a web front end subscribing to a back-end service, but I am no expert in modern web 
technologies so rather than spend time learning something that wasn’t C++ I decided to reach for the 
popular-but-so-far-unused-by-me C++ GUI framework, Qt.&lt;/p&gt;

&lt;p&gt;The challenge was how to hook up Qt, which is an event driven framework to a service written with Asio C++ coroutines.&lt;/p&gt;

&lt;p&gt;In the end it turned out to be easier than I had expected. Here’s how.&lt;/p&gt;

&lt;h2 id=&quot;a-simple-executor&quot;&gt;A simple Executor&lt;/h2&gt;

&lt;p&gt;As mentioned in a previous blog, Asio comes with a full implementation of the 
&lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p0443r12.html&quot;&gt;Unified Executors proposal&lt;/a&gt;. Asio coroutines
are designed to be initiated and continued within an executor’s execution context. So let’s build an executor that will
perform work in a Qt UI thread.&lt;/p&gt;

&lt;p&gt;The executor I am going to build will have to invoke completion handlers to Asio IO objects, so we need to make it 
compatible with &lt;code&gt;asio::any_io_executor&lt;/code&gt;. This means it needs to have an associated 
&lt;a href=&quot;https://www.boost.org/doc/libs/1_74_0/doc/html/boost_asio/reference/execution_context.html&quot;&gt;execution context&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The execution context is going to ultimately perform work on a Qt Application, so it makes sense to capture a reference
to the Application. Although Qt defines the macro &lt;code&gt;qApp&lt;/code&gt; to resolve to a pointer to the “current” application, for 
testing and sanity purposes I prefer that all services I write allow dependency injection, so I’ll arrange things so 
that the execution_context’s constructor takes an optional pointer to an application. In addition, it will be convenient
when writing components to not have to specifically create and pass an an execution context to windows within the Qt 
application so it makes sense to be able to provide access to a default context which references the default application.
Here’s a first cut:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_execution_context : net::execution_context
    , boost::noncopyable
{
    qt_execution_context(QApplication *app = qApp)
        : app_(app)
    {
        instance_ = this;
    }

    template&amp;lt;class F&amp;gt;
    void
    post(F f)
    {
        // todo
    }

    static qt_execution_context &amp;amp;
    singleton()
    {
        assert(instance_);
        return *instance_;
    }

private:
    static qt_execution_context *instance_;
    QApplication *app_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This class will provide two services. The first is to provide the asio service infrastructure so that we can create 
timers, sockets etc that use executors associated with this context and the second is to allow the executor to actually
dispatch work in a Qt application. This is the purpose of the &lt;code&gt;post&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;Now a Qt application is itself a kind of execution context - in that it dispatches QEvent objects to be handled by 
children of the application. We can use this infrastructure to ensure that work dispatched by this execution context
actually takes place on the correct thread and at the correct time.&lt;/p&gt;

&lt;p&gt;In order for us to dispatch work to the application, we need to wrap our function into a QEvent:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class qt_work_event_base : public QEvent
{
public:
    qt_work_event_base()
        : QEvent(generated_type())
    {
    }

    virtual void
    invoke() = 0;

    static QEvent::Type
    generated_type()
    {
        static int event_type = QEvent::registerEventType();
        return static_cast&amp;lt;QEvent::Type&amp;gt;(event_type);
    }
};

template&amp;lt;class F&amp;gt;
struct basic_qt_work_event : qt_work_event_base
{
    basic_qt_work_event(F f)
        : f_(std::move(f))
    {}

    void
    invoke() override
    {
        f_();
    }

private:
    F f_;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As opposed to using a &lt;code&gt;std::function&lt;/code&gt;, the &lt;code&gt;basic_qt_work_event&lt;/code&gt; allows us to wrap a move-only function object, which is 
important when that object is actually an Asio completion handler. Completion handlers benefit from being move-only as 
it means they can carry move-only state. This makes them more versatile, and can often lead to improvements in 
execution performance.&lt;/p&gt;

&lt;p&gt;Now we just need to fill out the code for &lt;code&gt;qt_execution_context::post&lt;/code&gt; and provide a mechanism in the Qt application to
detect and dispatch these messages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template&amp;lt;class F&amp;gt;
    void
    post(F f)
    {
        // c++20 auto template deduction
        auto event = new basic_qt_work_event(std::move(f));
        QApplication::postEvent(app_, event);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class qt_net_application : public QApplication
{
    using QApplication::QApplication;

protected:
    bool
    event(QEvent *event) override;
};

bool
qt_net_application::event(QEvent *event)
{
    if (event-&amp;gt;type() == qt_work_event_base::generated_type())
    {
        auto p = static_cast&amp;lt;qt_work_event_base*&amp;gt;(event);
        p-&amp;gt;accept();
        p-&amp;gt;invoke();
        return true;
    }
    else
    {
        return QApplication::event(event);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that I have seen on stack overflow the technique of invoking a function object in the destructor of the 
&lt;code&gt;QEvent&lt;/code&gt;-derived event. This would mean no necessity of custom event handling in the &lt;code&gt;QApplication&lt;/code&gt; but there are two
problems that I can see with this approach:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;I don’t know enough about Qt to know that this is safe and correct, and&lt;/li&gt;
  &lt;li&gt;Executors-TS executors can be destroyed while there are still un-invoked handlers within them. The correct behaviour
is to destroy these handlers without invoking them. If we put invocation code in the destructors, they will actually
mass-invoke when the executor is destroyed, leading most probably to annihilation of our program by segfault.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, that being done, we can now write the executor to meet the minimal expectations of an asio executor which can 
be used in an &lt;code&gt;any_io_executor&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_executor
{
    qt_executor(qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept
        : context_(std::addressof(context))
    {
    }

    qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept
    {
        return *context_;
    }

    static constexpr net::execution::blocking_t
    query(net::execution::blocking_t) noexcept
    {
        return net::execution::blocking.never;
    }

    static constexpr net::execution::relationship_t
    query(net::execution::relationship_t) noexcept
    {
        return net::execution::relationship.fork;
    }

    static constexpr net::execution::outstanding_work_t
    query(net::execution::outstanding_work_t) noexcept
    {
        return net::execution::outstanding_work.tracked;
    }

    template &amp;lt; typename OtherAllocator &amp;gt;
    static constexpr auto query(
    net::execution::allocator_t&amp;lt; OtherAllocator &amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    static constexpr auto
    query(net::execution::allocator_t&amp;lt; void &amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    template&amp;lt;class F&amp;gt;
    void
    execute(F f) const
    {
        context_-&amp;gt;post(std::move(f));
    }

    bool
    operator==(qt_executor const &amp;amp;other) const noexcept
    {
        return context_ == other.context_;
    }

    bool
    operator!=(qt_executor const &amp;amp;other) const noexcept
    {
        return !(*this == other);
    }

private:
    qt_execution_context *context_;
};


static_assert(net::execution::is_executor_v&amp;lt;qt_executor&amp;gt;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now all that remains is to write a subclass of some Qt Widget so that we can dispatch some work against it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class test_widget : public QTextEdit
{
    Q_OBJECT
public:
    using QTextEdit::QTextEdit;

private:
    void
    showEvent(QShowEvent *event) override;

    void
    hideEvent(QHideEvent *event) override;

    net::awaitable&amp;lt;void&amp;gt;
    run_demo();
};

void
test_widget::showEvent(QShowEvent *event)
{
    net::co_spawn(
    qt_executor(), [this] {
        return run_demo();
    },
    net::detached);

    QTextEdit::showEvent(event);
}

void
test_widget::hideEvent(QHideEvent *event)
{
    QWidget::hideEvent(event);
}

net::awaitable&amp;lt;void&amp;gt;
test_widget::run_demo()
{
    using namespace std::literals;

    auto timer = net::high_resolution_timer(co_await net::this_coro::executor);

    for (int i = 0; i &amp;lt; 10; ++i)
    {
        timer.expires_after(1s);
        co_await timer.async_wait(net::use_awaitable);
        this-&amp;gt;setText(QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;));
    }
    co_return;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-1&quot;&gt;stage 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And here is a screenshot of the app running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-october/stage-1.png&quot; alt=&quot;app running&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;all-very-well&quot;&gt;All very well…&lt;/h2&gt;

&lt;p&gt;OK, so we have a coroutine running in a Qt application. This is nice because it allows us to express an event-driven 
system in terms of procedural expression of code in a coroutine.&lt;/p&gt;

&lt;p&gt;But what if the user closes the window before the coroutine completes?&lt;/p&gt;

&lt;p&gt;This application has created the window on the stack, but in a larger application, there will be multiple windows and 
they may open and close at any time. It is not unusual in Qt to delete a closed window. If the coroutine continues to 
run once the windows that’s hosting it is deleted, we are sure to get a segfault.&lt;/p&gt;

&lt;p&gt;One answer to this is to maintain a sentinel in the Qt widget implementation, which prevents the continuation of the 
coroutine if destroyed. A &lt;code&gt;std::shared_ptr/weak_ptr&lt;/code&gt; pair would seem like a sensible solution. Let’s create an updated
version of the executor:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_guarded_executor
{
    qt_guarded_executor(std::weak_ptr&amp;lt;void&amp;gt; guard,
                        qt_execution_context &amp;amp;context
                        = qt_execution_context::singleton()) noexcept
        : context_(std::addressof(context))
        , guard_(std::move(guard))
    {}

    qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept
    {
        return *context_;
    }

    static constexpr net::execution::blocking_t
    query(net::execution::blocking_t) noexcept
    {
        return net::execution::blocking.never;
    }

    static constexpr net::execution::relationship_t
    query(net::execution::relationship_t) noexcept
    {
        return net::execution::relationship.fork;
    }

    static constexpr net::execution::outstanding_work_t
    query(net::execution::outstanding_work_t) noexcept
    {
        return net::execution::outstanding_work.tracked;
    }

    template&amp;lt;typename OtherAllocator&amp;gt;
    static constexpr auto
    query(net::execution::allocator_t&amp;lt;OtherAllocator&amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    static constexpr auto query(net::execution::allocator_t&amp;lt;void&amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    template&amp;lt;class F&amp;gt;
    void
    execute(F f) const
    {
        if (auto lock1 = guard_.lock())
        {
            context_-&amp;gt;post([guard = guard_, f = std::move(f)]() mutable {
                if (auto lock2 = guard.lock())
                    f();
            });
        }
    }

    bool
    operator==(qt_guarded_executor const &amp;amp;other) const noexcept
    {
        return context_ == other.context_ &amp;amp;&amp;amp; !guard_.owner_before(other.guard_)
            &amp;amp;&amp;amp; !other.guard_.owner_before(guard_);
    }

    bool
    operator!=(qt_guarded_executor const &amp;amp;other) const noexcept
    {
        return !(*this == other);
    }

private:
    qt_execution_context *context_;
    std::weak_ptr&amp;lt;void&amp;gt; guard_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’ll make a little boilerplate class that we can use as a base class in any executor-enabled object in Qt:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct has_guarded_executor
{
    using executor_type = qt_guarded_executor;

    has_guarded_executor(qt_execution_context &amp;amp;ctx
                         = qt_execution_context::singleton())
        : context_(std::addressof(ctx))
    {
        new_guard();
    }

    void
    new_guard()
    {
        static int x = 0;
        guard_ = std::shared_ptr&amp;lt;int&amp;gt;(std::addressof(x),
                                      // no-op deleter
                                      [](auto *) {});
    }

    void
    reset_guard()
    {
        guard_.reset();
    }

    executor_type
    get_executor() const
    {
        return qt_guarded_executor(guard_, *context_);
    }

private:
    qt_execution_context *context_;
    std::shared_ptr&amp;lt;void&amp;gt; guard_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we can modify the &lt;code&gt;test_widget&lt;/code&gt; to use it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class test_widget
    : public QTextEdit
    , public has_guarded_executor
{
    ...
};

void
test_widget::showEvent(QShowEvent *event)
{
    // stop all existing coroutines and create a new guard
    new_guard();

    // start our coroutine
    net::co_spawn(
        get_executor(), [this] { return run_demo(); }, net::detached);

    QTextEdit::showEvent(event);
}

void
test_widget::hideEvent(QHideEvent *event)
{
    // stop all coroutines
    reset_guard();
    QWidget::hideEvent(event);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’ll update the application to allow the creation and deletion of our widget. For this I’ll use the QMdiWindow
and add a menu with an action to create new widgets.&lt;/p&gt;

&lt;p&gt;We are now able to create and destroy widgets at will, with no segfaults.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-october/stage-2.png&quot; alt=&quot;MDI app running&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you look at the code, you’ll also see that I’ve wired up a rudimentary signal/slot device to allow the coroutine to 
be cancelled early.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    // test_widget.hpp

    void
    listen_for_stop(std::function&amp;lt;void()&amp;gt; slot);

    void
    stop_all();

    std::vector&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; stop_signals_;
    bool stopped_ = false;

    // test_widget.cpp

    void
    test_widget::listen_for_stop(std::function&amp;lt;void()&amp;gt; slot)
    {
        if (stopped_)
            return slot();
    
        stop_signals_.push_back(std::move(slot));
    }
    
    void
    test_widget::stop_all()
    {
        stopped_ = true;
        auto copy = std::exchange(stop_signals_, {});
        for (auto &amp;amp;slot : copy) slot();
    }
    
    void
    test_widget::closeEvent(QCloseEvent *event)
    {
        stop_all();
        QWidget::closeEvent(event);
    }

    net::awaitable&amp;lt;void&amp;gt;
    test_widget::run_demo()
    {
        using namespace std::literals;
    
        auto timer = net::high_resolution_timer(co_await net::this_coro::executor);
    
        auto done = false;
    
        listen_for_stop([&amp;amp;] {
            done = true;
            timer.cancel();
        });
    
        while (!done)
        {
            for (int i = 0; i &amp;lt; 10; ++i)
            {
                timer.expires_after(1s);
                auto ec = boost::system::error_code();
                co_await timer.async_wait(
                    net::redirect_error(net::use_awaitable, ec));
                if (ec)
                {
                    done = true;
                    break;
                }
                this-&amp;gt;setText(
                    QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;));
            }
    
            for (int i = 10; i--;)
            {
                timer.expires_after(250ms);
                auto ec = boost::system::error_code();
                co_await timer.async_wait(
                    net::redirect_error(net::use_awaitable, ec));
                if (ec)
                {
                    done = true;
                    break;
                }
                this-&amp;gt;setText(QString::fromStdString(std::to_string(i)));
            }
        }
        co_return;
    }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apparently I am told that it’s been a long-believed myth that Asio “doesn’t do cancellation”. This is of course, 
nonsense.&lt;/p&gt;

&lt;p&gt;Here’s the code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-2&quot;&gt;stage 2&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;state-of-the-art&quot;&gt;State of the Art&lt;/h2&gt;

&lt;p&gt;It’s worth mentioning that I wrote and tested this demo using clang-9 and the libc++ version of the standard library. 
I have also successfully tested clang-11 with coroutines (and concepts). As I understand it, recent versions of 
Visual Studio support both well. GCC 10 - although advertising support for coroutines - has given me trouble, exhibiting 
segfaults at run time.&lt;/p&gt;

&lt;p&gt;Apple Clang, of course, is as always well behind the curve with no support for coroutines. If you want to try this code 
on a mac, it’s entirely possible as long as you ditch the Apple compiler and use the homebrew’s clang:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install llvm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clang will then be available in &lt;code&gt;/usr/local/opt/bin&lt;/code&gt; and you will need to set your &lt;code&gt;CMAKE_CXX_COMPILER&lt;/code&gt; CMake variable
appropriately. For completeness, it’s worth mentioning that I also installed Qt5 using homebrew. You will need to
set &lt;code&gt;Qt5_DIR&lt;/code&gt;. Something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; cmake -H. -Bmy_build_dir -DCMAKE_CXX_COMPILER=/usr/local/opt/llvm/clang++ -DQt5_DIR=/usr/local/opt/qt5/lib/cmake/Qt5
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;going-further&quot;&gt;Going further&lt;/h3&gt;

&lt;p&gt;Ok, so what if we want our Qt application to interact with some asio-based service running in another thread?&lt;/p&gt;

&lt;p&gt;For this I’m going to create a few boilerplate classes. The reason is that we’re going to have multiple threads running
and each thread is going to be executing multiple coroutines. Each coroutine has an associated executor and that 
executor is dispatching completion handlers (which for our purposes advance the progress of the coroutines) in one of
the threads assigned to it.&lt;/p&gt;

&lt;p&gt;It is important that coroutines are able to synchronise with each other, similar to the way that threads synchronise
with each other.&lt;/p&gt;

&lt;p&gt;In fact, it’s reasonable to use the mental model that a coroutine is a kind of “thread”.&lt;/p&gt;

&lt;p&gt;In standard C++, we have the class &lt;code&gt;std::condition_variable&lt;/code&gt; which we can wait on for some condition to be fulfilled.
If we were to produce a similar class for coroutines, then coroutines could co_await on each other. This could form the 
basis of an asynchronous event queue.&lt;/p&gt;

&lt;p&gt;First the condition_variable, implemented in terms of cancellation of an Asio timer to indicate readiness (thanks
to Chris Kohlhoff - the author of Asio - for suggesting this and saving me having reach for another library or worse, 
write my own awaitable type!):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct async_condition_variable
{
private:
    using timer_type = net::high_resolution_timer;

public:
    using clock_type = timer_type::clock_type;
    using duration = timer_type::duration;
    using time_point = timer_type::time_point;
    using executor_type = timer_type::executor_type;

    /// Constructor
    /// @param exec is the executor to associate with the internal timer.
    explicit inline async_condition_variable(net::any_io_executor exec);

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;;

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;;

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;;

    auto
    get_executor() noexcept -&amp;gt; executor_type
    {
        return timer_.get_executor();
    }

    inline void
    notify_one();

    inline void
    notify_all();

    /// Put the condition into a stop state so that all future awaits fail.
    inline void
    stop();

    auto
    error() const -&amp;gt; error_code const &amp;amp;
    {
        return error_;
    }

    void
    reset()
    {
        error_ = {};
    }

private:
    timer_type timer_;
    error_code error_;
    std::multiset&amp;lt;timer_type::time_point&amp;gt; wait_times_;
};

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait_until(Pred pred, time_point limit)
    -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;
{
    assert(co_await net::this_coro::executor == timer_.get_executor());

    while (not error_ and not pred())
    {
        if (auto now = clock_type::now(); now &amp;gt;= limit)
            co_return std::cv_status::timeout;

        // insert our expiry time into the set and remember where it is
        auto where = wait_times_.insert(limit);

        // find the nearest expiry time and set the timeout for that one
        auto when = *wait_times_.begin();
        if (timer_.expiry() != when)
            timer_.expires_at(when);

        // wait for timeout or cancellation
        error_code ec;
        co_await timer_.async_wait(net::redirect_error(net::use_awaitable, ec));

        // remove our expiry time from the set
        wait_times_.erase(where);

        // any error other than operation_aborted is unexpected
        if (ec and ec != net::error::operation_aborted)
            if (not error_)
                error_ = ec;
    }

    if (error_)
        throw system_error(error_);

    co_return std::cv_status::no_timeout;
}

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    auto stat = co_await wait_until(std::move(pred), time_point::max());
    boost::ignore_unused(stat);
    co_return;
}

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait_for(Pred pred, duration d)
    -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;
{
    return wait_until(std::move(pred), clock_type::now() + d);
}

async_condition_variable::async_condition_variable(net::any_io_executor exec)
    : timer_(std::move(exec))
    , error_()
{}

void
async_condition_variable::notify_one()
{
    timer_.cancel_one();
}

void
async_condition_variable::notify_all()
{
    timer_.cancel();
}

void
async_condition_variable::stop()
{
    error_ = net::error::operation_aborted;
    notify_all();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For our purposes this one is a little too all-singing and all-dancing as it allows for timed waits from multiple
coroutines. This is not needed in our example, but I happened to have the code handy from previous experiments.
You will notice that I have marked the coroutines as &lt;code&gt;[[nodiscard]]&lt;/code&gt;. This is to ensure that I don’t forget to 
&lt;code&gt;co_await&lt;/code&gt; them at the call site. I can’t tell you how many times I have done that and then wondered why my program
mysteriously freezes mid run.&lt;/p&gt;

&lt;p&gt;Having built the condition_variable, we now need some kind of waitable queue. I have implemented this in terms of some
shared state which contains an  &lt;code&gt;async_condition_variable&lt;/code&gt; and some kind of queue. I have made the implementation of the 
queue a template function (another over-complication for our purposes). The template represents the strategy for 
accumulating messages before they have been consumed by the client. The strategy I have used here is a FIFO, which means 
that every message posted will be consumed in the order in which they were posted. But it could just as easily be a 
priority queue, or a latch - i.e. only storing the most recent message.&lt;/p&gt;

&lt;p&gt;The code to describe this machinery is a little long to put inline, but by all means look at the code:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_connection.hpp&quot;&gt;basic_connection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_distributor.hpp&quot;&gt;basic_distributor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_shared_state.hpp&quot;&gt;basic_shared_state&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next piece of machinery we need is the actual service that will be delivering messages. The code is more-or-less
a copy/paste of the code that was in our widget because it’s doing the same job - delivering messages, but this time
via the basic_distributor.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/message_service.hpp&quot;&gt;message_service.hpp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/message_service.cpp&quot;&gt;message_service.cpp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the message_service class is a pimpl. Although it uses a shared_ptr to hold the impl’s lifetime, it is itself
non-copyable. When the message_service is destroyed, it will signal its impl to stop. The impl will last a little longer
than the handle, while it shuts itself down.&lt;/p&gt;

&lt;p&gt;The main coroutine on the impl is called &lt;code&gt;run()&lt;/code&gt; and it is initiated when the impl is created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;message_service::message_service(const executor_type &amp;amp;exec)
    : exec_(exec)
    , impl_(std::make_shared&amp;lt;message_service_impl&amp;gt;(exec_))
{
    net::co_spawn(
        impl_-&amp;gt;get_executor(),
        [impl = impl_]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;run(); },
        net::detached);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;impl&lt;/code&gt; shared_ptr has been captured in the lambda. Normally we’d need to be careful here because the
lambda is just a class who’s &lt;code&gt;operator()&lt;/code&gt; happens to be a coroutine. This means that the actual coroutine can outlive the
lambda that initiated it, which means that &lt;code&gt;impl&lt;/code&gt; could be destroyed before the coroutine finishes. For this reason
it’s generally safer to pass the impl to the coroutine as an argument, so that it gets decay_copied into the 
coroutine state.
However, in this case we’re safe. &lt;code&gt;net::co_spawn&lt;/code&gt; will actually copy the lambda object before invoking it, guaranteeing&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;with asio at least - that the impl will survive the execution of the coroutine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And here’s the &lt;code&gt;run()&lt;/code&gt; coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt;
message_service_impl::run()
{
    using namespace std::literals;

    auto timer
        = net::high_resolution_timer(co_await net::this_coro::executor);

    auto done = false;

    listen_for_stop([&amp;amp;] {
      done = true;
      timer.cancel();
    });

    while (!done)
    {
        for (int i = 0; i &amp;lt; 10 &amp;amp;&amp;amp; !done; ++i)
        {
            timer.expires_after(1s);
            auto ec = boost::system::error_code();
            co_await timer.async_wait(
                net::redirect_error(net::use_awaitable, ec));
            if (ec)
                break;
            message_dist_.notify_value(std::to_string(i + 1) + &quot; seconds&quot;);
        }

        for (int i = 10; i-- &amp;amp;&amp;amp; !done;)
        {
            timer.expires_after(250ms);
            auto ec = boost::system::error_code();
            co_await timer.async_wait(
                net::redirect_error(net::use_awaitable, ec));
            if (ec)
                break;
            message_dist_.notify_value(std::to_string(i));
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the &lt;code&gt;done&lt;/code&gt; machinery allowing detection of a stop event. Remember that a stop event can arrive at any time. The
first this coroutine will hear of it is when one of the timer &lt;code&gt;async_wait&lt;/code&gt; calls is canceled. Note that the lambda
passed to &lt;code&gt;listen_for_stop&lt;/code&gt; &lt;em&gt;is not actually part of the coroutine&lt;/em&gt;. It is a separate function that just happens to 
refer to the same state that the coroutine refers to. The communication between the two is via the timer cancellation
and the &lt;code&gt;done&lt;/code&gt; flag. This communication is guaranteed not to race because both the coroutine and the lambda are executed 
by the same &lt;code&gt;strand&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finally we need to modify the widget:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt;
test_widget::run_demo()
{
    using namespace std::literals;

    auto service = message_service(ioexec_);
    auto conn = co_await service.connect();

    auto done = false;

    listen_for_stop([&amp;amp;] {
        done = true;
        conn.disconnect();
        service.reset();
    });

    while (!done)
    {
        auto message = co_await conn.consume();
        this-&amp;gt;setText(QString::fromStdString(message));
    }
    co_return;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This coroutine will exit via exception when the distributor feeding the connection is destroyed. This will happen when
the impl of the service is destroyed.&lt;/p&gt;

&lt;p&gt;Here is the final code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-3&quot;&gt;stage 3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ve covered quite a few topics here and I hope this has been useful and interesting for people interested in exploring
coroutines and the think-async mindset.&lt;/p&gt;

&lt;p&gt;There are a number of things I have not covered, the most important of which is improving the (currently very basic)
&lt;code&gt;qt_guarded_executor&lt;/code&gt; to improve its performance. At the present time, whether you call &lt;code&gt;dispatch&lt;/code&gt; or &lt;code&gt;post&lt;/code&gt; referencing
this executor type, a post will actually be performed. Perhaps next month I’ll revisit and add the extra machinery to
allow &lt;code&gt;net::dispatch(e, f)&lt;/code&gt; to offer straight-through execution if we’re already on the correct Qt thread.&lt;/p&gt;

&lt;p&gt;If you have any questions or suggestions I’m happy to hear them. You can generally find me in the &lt;code&gt;#beast&lt;/code&gt; channel 
on &lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;cpplang slack&lt;/a&gt; or if you prefer you can either email &lt;a href=&quot;mailto:hodges.r@gmail.com&quot;&gt;me&lt;/a&gt;
or create an issue on &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/issues&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Asio Coroutines in Qt applications! I started this train of thought when I wanted to hook up some back-end style code that I had written to a gui front end. One way to do this would be to have a web front end subscribing to a back-end service, but I am no expert in modern web technologies so rather than spend time learning something that wasn’t C++ I decided to reach for the popular-but-so-far-unused-by-me C++ GUI framework, Qt. The challenge was how to hook up Qt, which is an event driven framework to a service written with Asio C++ coroutines. In the end it turned out to be easier than I had expected. Here’s how. A simple Executor As mentioned in a previous blog, Asio comes with a full implementation of the Unified Executors proposal. Asio coroutines are designed to be initiated and continued within an executor’s execution context. So let’s build an executor that will perform work in a Qt UI thread. The executor I am going to build will have to invoke completion handlers to Asio IO objects, so we need to make it compatible with asio::any_io_executor. This means it needs to have an associated execution context. The execution context is going to ultimately perform work on a Qt Application, so it makes sense to capture a reference to the Application. Although Qt defines the macro qApp to resolve to a pointer to the “current” application, for testing and sanity purposes I prefer that all services I write allow dependency injection, so I’ll arrange things so that the execution_context’s constructor takes an optional pointer to an application. In addition, it will be convenient when writing components to not have to specifically create and pass an an execution context to windows within the Qt application so it makes sense to be able to provide access to a default context which references the default application. Here’s a first cut: struct qt_execution_context : net::execution_context , boost::noncopyable { qt_execution_context(QApplication *app = qApp) : app_(app) { instance_ = this; } template&amp;lt;class F&amp;gt; void post(F f) { // todo } static qt_execution_context &amp;amp; singleton() { assert(instance_); return *instance_; } private: static qt_execution_context *instance_; QApplication *app_; }; This class will provide two services. The first is to provide the asio service infrastructure so that we can create timers, sockets etc that use executors associated with this context and the second is to allow the executor to actually dispatch work in a Qt application. This is the purpose of the post method. Now a Qt application is itself a kind of execution context - in that it dispatches QEvent objects to be handled by children of the application. We can use this infrastructure to ensure that work dispatched by this execution context actually takes place on the correct thread and at the correct time. In order for us to dispatch work to the application, we need to wrap our function into a QEvent: class qt_work_event_base : public QEvent { public: qt_work_event_base() : QEvent(generated_type()) { } virtual void invoke() = 0; static QEvent::Type generated_type() { static int event_type = QEvent::registerEventType(); return static_cast&amp;lt;QEvent::Type&amp;gt;(event_type); } }; template&amp;lt;class F&amp;gt; struct basic_qt_work_event : qt_work_event_base { basic_qt_work_event(F f) : f_(std::move(f)) {} void invoke() override { f_(); } private: F f_; }; As opposed to using a std::function, the basic_qt_work_event allows us to wrap a move-only function object, which is important when that object is actually an Asio completion handler. Completion handlers benefit from being move-only as it means they can carry move-only state. This makes them more versatile, and can often lead to improvements in execution performance. Now we just need to fill out the code for qt_execution_context::post and provide a mechanism in the Qt application to detect and dispatch these messages: template&amp;lt;class F&amp;gt; void post(F f) { // c++20 auto template deduction auto event = new basic_qt_work_event(std::move(f)); QApplication::postEvent(app_, event); } class qt_net_application : public QApplication { using QApplication::QApplication; protected: bool event(QEvent *event) override; }; bool qt_net_application::event(QEvent *event) { if (event-&amp;gt;type() == qt_work_event_base::generated_type()) { auto p = static_cast&amp;lt;qt_work_event_base*&amp;gt;(event); p-&amp;gt;accept(); p-&amp;gt;invoke(); return true; } else { return QApplication::event(event); } } Note that I have seen on stack overflow the technique of invoking a function object in the destructor of the QEvent-derived event. This would mean no necessity of custom event handling in the QApplication but there are two problems that I can see with this approach: I don’t know enough about Qt to know that this is safe and correct, and Executors-TS executors can be destroyed while there are still un-invoked handlers within them. The correct behaviour is to destroy these handlers without invoking them. If we put invocation code in the destructors, they will actually mass-invoke when the executor is destroyed, leading most probably to annihilation of our program by segfault. However, that being done, we can now write the executor to meet the minimal expectations of an asio executor which can be used in an any_io_executor. struct qt_executor { qt_executor(qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept : context_(std::addressof(context)) { } qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept { return *context_; } static constexpr net::execution::blocking_t query(net::execution::blocking_t) noexcept { return net::execution::blocking.never; } static constexpr net::execution::relationship_t query(net::execution::relationship_t) noexcept { return net::execution::relationship.fork; } static constexpr net::execution::outstanding_work_t query(net::execution::outstanding_work_t) noexcept { return net::execution::outstanding_work.tracked; } template &amp;lt; typename OtherAllocator &amp;gt; static constexpr auto query( net::execution::allocator_t&amp;lt; OtherAllocator &amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } static constexpr auto query(net::execution::allocator_t&amp;lt; void &amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } template&amp;lt;class F&amp;gt; void execute(F f) const { context_-&amp;gt;post(std::move(f)); } bool operator==(qt_executor const &amp;amp;other) const noexcept { return context_ == other.context_; } bool operator!=(qt_executor const &amp;amp;other) const noexcept { return !(*this == other); } private: qt_execution_context *context_; }; static_assert(net::execution::is_executor_v&amp;lt;qt_executor&amp;gt;); Now all that remains is to write a subclass of some Qt Widget so that we can dispatch some work against it. class test_widget : public QTextEdit { Q_OBJECT public: using QTextEdit::QTextEdit; private: void showEvent(QShowEvent *event) override; void hideEvent(QHideEvent *event) override; net::awaitable&amp;lt;void&amp;gt; run_demo(); }; void test_widget::showEvent(QShowEvent *event) { net::co_spawn( qt_executor(), [this] { return run_demo(); }, net::detached); QTextEdit::showEvent(event); } void test_widget::hideEvent(QHideEvent *event) { QWidget::hideEvent(event); } net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); for (int i = 0; i &amp;lt; 10; ++i) { timer.expires_after(1s); co_await timer.async_wait(net::use_awaitable); this-&amp;gt;setText(QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;)); } co_return; } Here is the code for stage 1 And here is a screenshot of the app running: All very well… OK, so we have a coroutine running in a Qt application. This is nice because it allows us to express an event-driven system in terms of procedural expression of code in a coroutine. But what if the user closes the window before the coroutine completes? This application has created the window on the stack, but in a larger application, there will be multiple windows and they may open and close at any time. It is not unusual in Qt to delete a closed window. If the coroutine continues to run once the windows that’s hosting it is deleted, we are sure to get a segfault. One answer to this is to maintain a sentinel in the Qt widget implementation, which prevents the continuation of the coroutine if destroyed. A std::shared_ptr/weak_ptr pair would seem like a sensible solution. Let’s create an updated version of the executor: struct qt_guarded_executor { qt_guarded_executor(std::weak_ptr&amp;lt;void&amp;gt; guard, qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept : context_(std::addressof(context)) , guard_(std::move(guard)) {} qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept { return *context_; } static constexpr net::execution::blocking_t query(net::execution::blocking_t) noexcept { return net::execution::blocking.never; } static constexpr net::execution::relationship_t query(net::execution::relationship_t) noexcept { return net::execution::relationship.fork; } static constexpr net::execution::outstanding_work_t query(net::execution::outstanding_work_t) noexcept { return net::execution::outstanding_work.tracked; } template&amp;lt;typename OtherAllocator&amp;gt; static constexpr auto query(net::execution::allocator_t&amp;lt;OtherAllocator&amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } static constexpr auto query(net::execution::allocator_t&amp;lt;void&amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } template&amp;lt;class F&amp;gt; void execute(F f) const { if (auto lock1 = guard_.lock()) { context_-&amp;gt;post([guard = guard_, f = std::move(f)]() mutable { if (auto lock2 = guard.lock()) f(); }); } } bool operator==(qt_guarded_executor const &amp;amp;other) const noexcept { return context_ == other.context_ &amp;amp;&amp;amp; !guard_.owner_before(other.guard_) &amp;amp;&amp;amp; !other.guard_.owner_before(guard_); } bool operator!=(qt_guarded_executor const &amp;amp;other) const noexcept { return !(*this == other); } private: qt_execution_context *context_; std::weak_ptr&amp;lt;void&amp;gt; guard_; }; Now we’ll make a little boilerplate class that we can use as a base class in any executor-enabled object in Qt: struct has_guarded_executor { using executor_type = qt_guarded_executor; has_guarded_executor(qt_execution_context &amp;amp;ctx = qt_execution_context::singleton()) : context_(std::addressof(ctx)) { new_guard(); } void new_guard() { static int x = 0; guard_ = std::shared_ptr&amp;lt;int&amp;gt;(std::addressof(x), // no-op deleter [](auto *) {}); } void reset_guard() { guard_.reset(); } executor_type get_executor() const { return qt_guarded_executor(guard_, *context_); } private: qt_execution_context *context_; std::shared_ptr&amp;lt;void&amp;gt; guard_; }; And we can modify the test_widget to use it: class test_widget : public QTextEdit , public has_guarded_executor { ... }; void test_widget::showEvent(QShowEvent *event) { // stop all existing coroutines and create a new guard new_guard(); // start our coroutine net::co_spawn( get_executor(), [this] { return run_demo(); }, net::detached); QTextEdit::showEvent(event); } void test_widget::hideEvent(QHideEvent *event) { // stop all coroutines reset_guard(); QWidget::hideEvent(event); } Now we’ll update the application to allow the creation and deletion of our widget. For this I’ll use the QMdiWindow and add a menu with an action to create new widgets. We are now able to create and destroy widgets at will, with no segfaults. If you look at the code, you’ll also see that I’ve wired up a rudimentary signal/slot device to allow the coroutine to be cancelled early. // test_widget.hpp void listen_for_stop(std::function&amp;lt;void()&amp;gt; slot); void stop_all(); std::vector&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; stop_signals_; bool stopped_ = false; // test_widget.cpp void test_widget::listen_for_stop(std::function&amp;lt;void()&amp;gt; slot) { if (stopped_) return slot(); stop_signals_.push_back(std::move(slot)); } void test_widget::stop_all() { stopped_ = true; auto copy = std::exchange(stop_signals_, {}); for (auto &amp;amp;slot : copy) slot(); } void test_widget::closeEvent(QCloseEvent *event) { stop_all(); QWidget::closeEvent(event); } net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); auto done = false; listen_for_stop([&amp;amp;] { done = true; timer.cancel(); }); while (!done) { for (int i = 0; i &amp;lt; 10; ++i) { timer.expires_after(1s); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) { done = true; break; } this-&amp;gt;setText( QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;)); } for (int i = 10; i--;) { timer.expires_after(250ms); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) { done = true; break; } this-&amp;gt;setText(QString::fromStdString(std::to_string(i))); } } co_return; } Apparently I am told that it’s been a long-believed myth that Asio “doesn’t do cancellation”. This is of course, nonsense. Here’s the code for stage 2 State of the Art It’s worth mentioning that I wrote and tested this demo using clang-9 and the libc++ version of the standard library. I have also successfully tested clang-11 with coroutines (and concepts). As I understand it, recent versions of Visual Studio support both well. GCC 10 - although advertising support for coroutines - has given me trouble, exhibiting segfaults at run time. Apple Clang, of course, is as always well behind the curve with no support for coroutines. If you want to try this code on a mac, it’s entirely possible as long as you ditch the Apple compiler and use the homebrew’s clang: brew install llvm Clang will then be available in /usr/local/opt/bin and you will need to set your CMAKE_CXX_COMPILER CMake variable appropriately. For completeness, it’s worth mentioning that I also installed Qt5 using homebrew. You will need to set Qt5_DIR. Something like this: cmake -H. -Bmy_build_dir -DCMAKE_CXX_COMPILER=/usr/local/opt/llvm/clang++ -DQt5_DIR=/usr/local/opt/qt5/lib/cmake/Qt5 Going further Ok, so what if we want our Qt application to interact with some asio-based service running in another thread? For this I’m going to create a few boilerplate classes. The reason is that we’re going to have multiple threads running and each thread is going to be executing multiple coroutines. Each coroutine has an associated executor and that executor is dispatching completion handlers (which for our purposes advance the progress of the coroutines) in one of the threads assigned to it. It is important that coroutines are able to synchronise with each other, similar to the way that threads synchronise with each other. In fact, it’s reasonable to use the mental model that a coroutine is a kind of “thread”. In standard C++, we have the class std::condition_variable which we can wait on for some condition to be fulfilled. If we were to produce a similar class for coroutines, then coroutines could co_await on each other. This could form the basis of an asynchronous event queue. First the condition_variable, implemented in terms of cancellation of an Asio timer to indicate readiness (thanks to Chris Kohlhoff - the author of Asio - for suggesting this and saving me having reach for another library or worse, write my own awaitable type!): struct async_condition_variable { private: using timer_type = net::high_resolution_timer; public: using clock_type = timer_type::clock_type; using duration = timer_type::duration; using time_point = timer_type::time_point; using executor_type = timer_type::executor_type; /// Constructor /// @param exec is the executor to associate with the internal timer. explicit inline async_condition_variable(net::any_io_executor exec); template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;; template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;; template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;; auto get_executor() noexcept -&amp;gt; executor_type { return timer_.get_executor(); } inline void notify_one(); inline void notify_all(); /// Put the condition into a stop state so that all future awaits fail. inline void stop(); auto error() const -&amp;gt; error_code const &amp;amp; { return error_; } void reset() { error_ = {}; } private: timer_type timer_; error_code error_; std::multiset&amp;lt;timer_type::time_point&amp;gt; wait_times_; }; template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt; { assert(co_await net::this_coro::executor == timer_.get_executor()); while (not error_ and not pred()) { if (auto now = clock_type::now(); now &amp;gt;= limit) co_return std::cv_status::timeout; // insert our expiry time into the set and remember where it is auto where = wait_times_.insert(limit); // find the nearest expiry time and set the timeout for that one auto when = *wait_times_.begin(); if (timer_.expiry() != when) timer_.expires_at(when); // wait for timeout or cancellation error_code ec; co_await timer_.async_wait(net::redirect_error(net::use_awaitable, ec)); // remove our expiry time from the set wait_times_.erase(where); // any error other than operation_aborted is unexpected if (ec and ec != net::error::operation_aborted) if (not error_) error_ = ec; } if (error_) throw system_error(error_); co_return std::cv_status::no_timeout; } template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { auto stat = co_await wait_until(std::move(pred), time_point::max()); boost::ignore_unused(stat); co_return; } template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt; { return wait_until(std::move(pred), clock_type::now() + d); } async_condition_variable::async_condition_variable(net::any_io_executor exec) : timer_(std::move(exec)) , error_() {} void async_condition_variable::notify_one() { timer_.cancel_one(); } void async_condition_variable::notify_all() { timer_.cancel(); } void async_condition_variable::stop() { error_ = net::error::operation_aborted; notify_all(); } For our purposes this one is a little too all-singing and all-dancing as it allows for timed waits from multiple coroutines. This is not needed in our example, but I happened to have the code handy from previous experiments. You will notice that I have marked the coroutines as [[nodiscard]]. This is to ensure that I don’t forget to co_await them at the call site. I can’t tell you how many times I have done that and then wondered why my program mysteriously freezes mid run. Having built the condition_variable, we now need some kind of waitable queue. I have implemented this in terms of some shared state which contains an async_condition_variable and some kind of queue. I have made the implementation of the queue a template function (another over-complication for our purposes). The template represents the strategy for accumulating messages before they have been consumed by the client. The strategy I have used here is a FIFO, which means that every message posted will be consumed in the order in which they were posted. But it could just as easily be a priority queue, or a latch - i.e. only storing the most recent message. The code to describe this machinery is a little long to put inline, but by all means look at the code: basic_connection basic_distributor basic_shared_state The next piece of machinery we need is the actual service that will be delivering messages. The code is more-or-less a copy/paste of the code that was in our widget because it’s doing the same job - delivering messages, but this time via the basic_distributor. message_service.hpp message_service.cpp Note that the message_service class is a pimpl. Although it uses a shared_ptr to hold the impl’s lifetime, it is itself non-copyable. When the message_service is destroyed, it will signal its impl to stop. The impl will last a little longer than the handle, while it shuts itself down. The main coroutine on the impl is called run() and it is initiated when the impl is created: message_service::message_service(const executor_type &amp;amp;exec) : exec_(exec) , impl_(std::make_shared&amp;lt;message_service_impl&amp;gt;(exec_)) { net::co_spawn( impl_-&amp;gt;get_executor(), [impl = impl_]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;run(); }, net::detached); } Note that the impl shared_ptr has been captured in the lambda. Normally we’d need to be careful here because the lambda is just a class who’s operator() happens to be a coroutine. This means that the actual coroutine can outlive the lambda that initiated it, which means that impl could be destroyed before the coroutine finishes. For this reason it’s generally safer to pass the impl to the coroutine as an argument, so that it gets decay_copied into the coroutine state. However, in this case we’re safe. net::co_spawn will actually copy the lambda object before invoking it, guaranteeing with asio at least - that the impl will survive the execution of the coroutine. And here’s the run() coroutine: net::awaitable&amp;lt;void&amp;gt; message_service_impl::run() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); auto done = false; listen_for_stop([&amp;amp;] { done = true; timer.cancel(); }); while (!done) { for (int i = 0; i &amp;lt; 10 &amp;amp;&amp;amp; !done; ++i) { timer.expires_after(1s); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) break; message_dist_.notify_value(std::to_string(i + 1) + &quot; seconds&quot;); } for (int i = 10; i-- &amp;amp;&amp;amp; !done;) { timer.expires_after(250ms); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) break; message_dist_.notify_value(std::to_string(i)); } } } Notice the done machinery allowing detection of a stop event. Remember that a stop event can arrive at any time. The first this coroutine will hear of it is when one of the timer async_wait calls is canceled. Note that the lambda passed to listen_for_stop is not actually part of the coroutine. It is a separate function that just happens to refer to the same state that the coroutine refers to. The communication between the two is via the timer cancellation and the done flag. This communication is guaranteed not to race because both the coroutine and the lambda are executed by the same strand. Finally we need to modify the widget: net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto service = message_service(ioexec_); auto conn = co_await service.connect(); auto done = false; listen_for_stop([&amp;amp;] { done = true; conn.disconnect(); service.reset(); }); while (!done) { auto message = co_await conn.consume(); this-&amp;gt;setText(QString::fromStdString(message)); } co_return; } This coroutine will exit via exception when the distributor feeding the connection is destroyed. This will happen when the impl of the service is destroyed. Here is the final code for stage 3. I’ve covered quite a few topics here and I hope this has been useful and interesting for people interested in exploring coroutines and the think-async mindset. There are a number of things I have not covered, the most important of which is improving the (currently very basic) qt_guarded_executor to improve its performance. At the present time, whether you call dispatch or post referencing this executor type, a post will actually be performed. Perhaps next month I’ll revisit and add the extra machinery to allow net::dispatch(e, f) to offer straight-through execution if we’re already on the correct Qt thread. If you have any questions or suggestions I’m happy to hear them. You can generally find me in the #beast channel on cpplang slack or if you prefer you can either email me or create an issue on this repo.</summary></entry><entry><title type="html">Richard’s September Update</title><link href="http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate.html" rel="alternate" type="text/html" title="Richard’s September Update" /><published>2020-09-30T00:00:00+00:00</published><updated>2020-09-30T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate.html">&lt;h1 id=&quot;cancellation-in-beastasio-and-better-compile-performance-with-beastwebsocket&quot;&gt;Cancellation in Beast/Asio and Better Compile Performance with Beast.Websocket&lt;/h1&gt;

&lt;p&gt;This month I will be discussing two issues. One of interest to many people who come to us with questions on the 
&lt;a href=&quot;https://github.com/boostorg/beast/issues&quot;&gt;Github Issue Tracker&lt;/a&gt; and the #beast channel of 
&lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;Cpplang Slack&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compile-times-and-separation-of-concerns&quot;&gt;Compile Times and Separation of Concerns&lt;/h2&gt;

&lt;p&gt;A common complaint about Boost.Beast is that compilation units that use the &lt;code&gt;websocket::stream&lt;/code&gt; template class
often take a long time to compile, and that because websocket::stream is a template, this compilation overhead can
become viral in an application.&lt;/p&gt;

&lt;p&gt;This is a valid complaint and we believe there are some reasonable tradeoffs we can make by refactoring the websocket
stream to use fewer templates internally. Vinnie has started work to express the WebSocket’s 
intermediate completion handlers, buffer sequence and executor in terms of a polymorphic object. This would mean a 
few indirect jumps in the compiled code but would significantly reduce the number of internal template expansions.
In the scheme of things, we don’t believe that the virtual function calls will materially affect runtime performance.
The branch is &lt;a href=&quot;https://github.com/vinniefalco/beast/tree/async-an&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will be continuing work in this area in the coming days.&lt;/p&gt;

&lt;p&gt;In the meantime, our general response is to suggest that users create a base class to handle the transport, and 
communicate important events such as frame received, connection state and the close notification to a derived 
application-layer class through a private polymorphic interface.&lt;/p&gt;

&lt;p&gt;In this way, the websocket transport compilation unit may take a while to compile, but it needs to be done only once
since the transport layer will rarely change during the development life of an application. Whenever there is a change
to the application layer, the transport layer is not affected so websocket-related code is not affected.&lt;/p&gt;

&lt;p&gt;This approach has a number of benefits. Not least of which is that developing another client implementation over 
a different websocket connection in the same application becomes trivial.&lt;/p&gt;

&lt;p&gt;Another benefit is that the application can be designed such that application-level concerns are agnostic of the 
transport mechanism. Such as when the server can be accessed by multiple means - WSS, WS, long poll, direct connection, 
unix sockets and so on.&lt;/p&gt;

&lt;p&gt;In this blog I will present a simplified implementation of this idea. My thanks to the cpplang Slack user &lt;code&gt;@elegracer&lt;/code&gt;
who most recently asked for guidance on reducing compile times. It was (his/her? Slack is silent on the matter) question
which prompted me to finally conjure up a demo. &lt;code&gt;@elegracer&lt;/code&gt;’s problem was needing to connect to multiple cryptocurrency
exchanges in the same app over websocket. In this particular example I’ll demonstrate a simplified connection to
the public FMex market data feed since that was the subject of the original question.&lt;/p&gt;

&lt;h2 id=&quot;correct-cancellation&quot;&gt;Correct Cancellation&lt;/h2&gt;

&lt;p&gt;Our examples in the Beast Repository are rudimentary and don’t cover the issue of graceful shutdown of an application
in response to a SIGINT (i.e. the user pressing ctrl-c). It is common for simple programs to exit suddenly in response
to this signal, which is the default behaviour. For many applications, this is perfectly fine but not all. We may want 
active objects in the program to write data to disk, we may want to ensure that the underlying websocket is 
shut down cleanly and we may want to give the user an opportunity to prevent the shutdown.&lt;/p&gt;

&lt;p&gt;I will further annotate the example by providing this ability to prevent the shutdown. The user will have to confirm the 
first SIGINT with another within 5 seconds to confirm.&lt;/p&gt;

&lt;h1 id=&quot;designing-the-application&quot;&gt;Designing the application&lt;/h1&gt;

&lt;p&gt;When I write IO applications involving Asio and Beast, I prefer to create an “application” object. This has the 
responsibility of monitoring signals and starting the initial connection objects. It also provides the communication
between the two.&lt;/p&gt;

&lt;p&gt;The construction and configuration of the &lt;code&gt;io_context&lt;/code&gt; and &lt;code&gt;ssl::context&lt;/code&gt; stay in &lt;code&gt;main()&lt;/code&gt;. The executor and ssl context
are passed to the application by reference as dependencies. The application can then pass on these refrences as 
required. It is also worth mentioning that I don’t pass the io_context’s executor as a polymorphic &lt;code&gt;any_io_executor&lt;/code&gt; 
type at this stage. The reason is that I may want in future to upgrade my program to be multi-threaded. If I do this, 
then each individual io_enabled object such as a connection or the application will need to have its &lt;em&gt;own&lt;/em&gt; strand.
Getting the strand out of an any_io_executor is not possible in the general case as it will have been type-erased, so 
for top level objects I pass the executor as &lt;code&gt;io_context::executor_type&lt;/code&gt;. It is then up to each object to create its own
strand internally which will have the type &lt;code&gt;strand&amp;lt;io_context::executor_type&amp;gt;&lt;/code&gt;. The &lt;code&gt;strand&lt;/code&gt; type provides the method
&lt;code&gt;get_inner_executor&lt;/code&gt; which allows the application to extract the underlying &lt;code&gt;io_context::executor_type&lt;/code&gt; and pass it to
the constructor of any subordinate but otherwise self-contained io objects. The subordinates can then build their own
strands from this.&lt;/p&gt;

&lt;h2 id=&quot;step-1---a-simple-application-framework-that-supports-ctrl-c&quot;&gt;Step 1 - A Simple Application Framework That Supports ctrl-c&lt;/h2&gt;

&lt;p&gt;OK, let’s get started and build the framework. Here’s a link to 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-1/pre-cxx20/blog-2020-09&quot;&gt;step 1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ssl.hpp&lt;/code&gt; and &lt;code&gt;net.hpp&lt;/code&gt; simply configure the project to use boost.asio. The idea of these little configuration headers
is that they could be generated by the cmake project if necessary to allow the option of upgrading to std networking
if it ever arrives.&lt;/p&gt;

&lt;p&gt;As a matter of style, I like to ensure that no names are created in the global namespace other than &lt;code&gt;main&lt;/code&gt;. This saves
headaches that could occur if I wrote code on one platform, but then happened to port it to another where the name
was already in use by the native system libraries.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;main.cpp&lt;/code&gt; simply creates the io execution context and a default ssl context, creates the application, starts it and
runs the io context.&lt;/p&gt;

&lt;p&gt;At the moment, the only interesting part of our program is the &lt;code&gt;signit_state&lt;/code&gt;. This is a state machine which handles the
behaviour of the program when a &lt;code&gt;SIGINT&lt;/code&gt; is received. Our state machine is doing something a little fancy. Here is the 
state diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-09-sigint-state.png&quot; alt=&quot;sigint_state&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rather than reproduce the code here, please refer to 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-1/pre-cxx20/blog-2020-09&quot;&gt;step 1&lt;/a&gt; 
to see the source code.&lt;/p&gt;

&lt;p&gt;At this point the program will run and successfully handle ctrl-c:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
Interrupt unconfirmed. Ignoring
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
^CInterrupt confirmed. Shutting down
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-2---connecting-to-an-exchange&quot;&gt;Step 2 - Connecting to an Exchange&lt;/h2&gt;

&lt;p&gt;Now we need to create our WebSocket transport class and our FMex exchange protocol class that will derive from it.
For now we won’t worry about cancellation - we’ll retrofit that in Step 3.&lt;/p&gt;

&lt;p&gt;Here is the code for 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-2/pre-cxx20/blog-2020-09&quot;&gt;step 2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This section introduces two new main classes - the &lt;code&gt;wss_transport&lt;/code&gt; and the &lt;code&gt;fmex_connection&lt;/code&gt;. In addition, the connection
phase of the wss_transport is expressed as a composed operation for exposition purposes (and in my opinion it actually 
makes the code easier to read than continuation-passing style code)&lt;/p&gt;

&lt;p&gt;Here is the implementation of the connect coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    struct wss_transport::connect_op : asio::coroutine
    {
        using executor_type = wss_transport::executor_type;
        using websock       = wss_transport::websock;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we define the &lt;em&gt;implementation&lt;/em&gt; of the coroutine - this is an object which will not be moved for the duration of the
execution of the coroutine. This address stability is important because intermediate asynchronous operations will rely
on knowing the address of the resolver (and later perhaps other io objects).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        struct impl_data
        {
            impl_data(websock &amp;amp;   ws,
                      std::string host,
                      std::string port,
                      std::string target)
            : ws(ws)
            , resolver(ws.get_executor())
            , host(host)
            , port(port)
            , target(target)
            {
            }

            layer_0 &amp;amp;
            tcp_layer() const
            {
                return ws.next_layer().next_layer();
            }

            layer_1 &amp;amp;
            ssl_layer() const
            {
                return ws.next_layer();
            }

            websock &amp;amp;                            ws;
            net::ip::tcp::resolver               resolver;
            net::ip::tcp::resolver::results_type endpoints;
            std::string                          host, port, target;
        };
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The constructor merely forwards the arguments to the construction of the &lt;code&gt;impl_data&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        connect_op(websock &amp;amp;   ws,
                   std::string host,
                   std::string port,
                   std::string target)
        : impl_(std::make_unique&amp;lt; impl_data &amp;gt;(ws, host, port, target))
        {
        }

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This coroutine is both a composed operation and a completion handler for sub-operations. This means it must have an
&lt;code&gt;operator()&lt;/code&gt; interface matching the requirements of each sub-operation. During the lifetime of this coroutine we 
will be using the resolver and calling &lt;code&gt;async_connect&lt;/code&gt; on the &lt;code&gt;tcp_stream&lt;/code&gt;. We therefore provide conforming member
functions which store or ignore the and forward the &lt;code&gt;error_code&lt;/code&gt; to the main implementation of the coroutine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        template &amp;lt; class Self &amp;gt;
        void
        operator()(Self &amp;amp;                               self,
                   error_code                           ec,
                   net::ip::tcp::resolver::results_type results)
        {
            impl_-&amp;gt;endpoints = results;
            (*this)(self, ec);
        }

        template &amp;lt; class Self &amp;gt;
        void
        operator()(Self &amp;amp;self, error_code ec, net::ip::tcp::endpoint const &amp;amp;)
        {
            (*this)(self, ec);
        }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the main implementation of the coroutine. Note that the last two parameters provide defaults. This is in order
to allow this member function to match the completion handler signatures of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;void()&lt;/code&gt; - invoked during async_compose in order to start the coroutine.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;void(error_code)&lt;/code&gt; - invoked by the two functions above and by the async handshakes.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;void(error_code, std::size_t)&lt;/code&gt; - invoked by operations such as async_read and async_write although not strictly 
necessary here.
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;      template &amp;lt; class Self &amp;gt;
      void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t = 0)
      {
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;Note that here we are checking the error code before re-entering the coroutine. This is a shortcut which allows us to
omit error checking after each sub-operation. This check will happen on every attempt to re-enter the coroutine, 
including the first entry (at which time &lt;code&gt;ec&lt;/code&gt; is guaranteed to be default constructed).&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;          if (ec)
              return self.complete(ec);

          auto &amp;amp;impl = *impl_;
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;Note the use of the asio yield and unyield headers to create the fake ‘keywords’ &lt;code&gt;reenter&lt;/code&gt; and &lt;code&gt;yield&lt;/code&gt; in avery limited
scope.&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;boost/asio/yield.hpp&amp;gt;
          reenter(*this)
          {
              yield impl.resolver.async_resolve(
                  impl.host, impl.port, std::move(self));

              impl.tcp_layer().expires_after(15s);
              yield impl.tcp_layer().async_connect(impl.endpoints,
                                                   std::move(self));

              if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(),
                                            impl.host.c_str()))
                  return self.complete(
                      error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()),
                                 net::error::get_ssl_category()));

              impl.tcp_layer().expires_after(15s);
              yield impl.ssl_layer().async_handshake(ssl::stream_base::client,
                                                     std::move(self));

              impl.tcp_layer().expires_after(15s);
              yield impl.ws.async_handshake(
                  impl.host, impl.target, std::move(self));
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;If the coroutine is re-entered here, it must be because there was no error (if there was an error, it would have been
caught by the pre-reentry error check above). Since execution has resumed here in the completion handler of the 
&lt;code&gt;async_handshake&lt;/code&gt; initiating function, we are guaranteed to be executing in the correct executor. Therefore we can
simply call &lt;code&gt;complete&lt;/code&gt; directly without needing to post to an executor. Note that the &lt;code&gt;async_compose&lt;/code&gt; call which will
encapsulate the use of this class embeds this object into a wrapper which provides the &lt;code&gt;executor_type&lt;/code&gt; and 
&lt;code&gt;get_executor()&lt;/code&gt; mechanism which asio uses to determine on which executor to invoke completion handlers.&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;              impl.tcp_layer().expires_never();
              yield self.complete(ec);
          }
#include &amp;lt;boost/asio/unyield.hpp&amp;gt;
      }

      std::unique_ptr&amp;lt; impl_data &amp;gt; impl_;
  };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;wss_connection&lt;/code&gt; class provides the bare bones required to connect a websocket and maintain the connection. It 
provides a protected interface so that derived classes can send text frames and it will call private virtual functions
in order to notify the derived class of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;transport up (websocket connection established).&lt;/li&gt;
  &lt;li&gt;frame received.&lt;/li&gt;
  &lt;li&gt;connection error (either during connection or operation).&lt;/li&gt;
  &lt;li&gt;websocket close - the server has requested or agreed to a graceful shutdown.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Connection errors will only be notified once, and once a connection error has been indicated, no other event will reach
the derived class.&lt;/p&gt;

&lt;p&gt;One of the many areas that trips up asio/beast beginners is that care must be taken to ensure that only one &lt;code&gt;async_write&lt;/code&gt;
is in progress at a time on the WebSocket (or indeed any async io object). For this reason we implement a simple 
transmit queue state which can be considered to be an orthogonal region (parallel task) to the read state.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        // send_state - data to control sending data

        std::deque&amp;lt;std::string&amp;gt; send_queue_;
        enum send_state
        {
            not_sending,
            sending
        } send_state_ = not_sending;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will note that I have used a &lt;code&gt;std::deque&lt;/code&gt; to hold the pending messages. Although a deque has theoretically better
complexity when inserting or removing items at the ends than a vector, this is not the reason for choosing this data
structure. The actual reason is that items in a deque are guaranteed to have a stable address, even when other items
are added or removed. This is useful as it means we don’t have to move frames out of the transmit queue in order to
send them. Remember that during an &lt;code&gt;async_write&lt;/code&gt;, the data to which the supplied buffer sequence refers must have a 
stable address.&lt;/p&gt;

&lt;p&gt;Here are the functions that deal with the send state transitions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    wss_transport::send_text_frame(std::string frame)
    {
        if (state_ != connected)
            return;

        send_queue_.push_back(std::move(frame));
        start_sending();
    }

    void
    wss_transport::start_sending()
    {
        if (state_ == connected &amp;amp;&amp;amp; send_state_ == not_sending &amp;amp;&amp;amp;
            !send_queue_.empty())
        {
            send_state_ = sending;
            websock_.async_write(net::buffer(send_queue_.front()),
                                 [this](error_code const &amp;amp;ec, std::size_t bt) {
                                     handle_send(ec, bt);
                                 });
        }
    }

    void
    wss_transport::handle_send(const error_code &amp;amp;ec, std::size_t)
    {
        send_state_ = not_sending;

        send_queue_.pop_front();

        if (ec)
            event_transport_error(ec);
        else
            start_sending();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can implement our specific exchange protocol on top of the &lt;code&gt;wss_connection&lt;/code&gt;. In this case, FMex eschews 
the ping/pong built into websockets and requires a json ping/pong to be initiated by the client.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    fmex_connection::ping_enter_state()
    {
        BOOST_ASSERT(ping_state_ == ping_not_started);
        ping_enter_wait();
    }

    void
    fmex_connection::ping_enter_wait()
    {
        ping_state_ = ping_wait;

        ping_timer_.expires_after(5s);

        ping_timer_.async_wait([this](error_code const &amp;amp;ec) {
            if (!ec)
                ping_event_timeout();
        });
    }

    void
    fmex_connection::ping_event_timeout()
    {
        ping_state_ = ping_waiting_pong;

        auto  frame = json::value();
        auto &amp;amp;o     = frame.emplace_object();
        o[&quot;cmd&quot;]    = &quot;ping&quot;;
        o[&quot;id&quot;]     = &quot;my_ping_ident&quot;;
        o[&quot;args&quot;].emplace_array().push_back(timestamp());
        send_text_frame(json::serialize(frame));
    }

    void
    fmex_connection::ping_event_pong(json::value const &amp;amp;frame)
    {
        ping_enter_wait();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that since we have implemented frame transmission in the base class in terms of a queue, the fmex class has no
need to worry about ensuring the one-write-at-a-time rule. The base class handles it. This makes the application 
developer’s life easy.&lt;/p&gt;

&lt;p&gt;Finally, we implement &lt;code&gt;on_text_frame&lt;/code&gt; and write a little message parser and switch. Note that this function may throw.
The base class will catch any exceptions thrown here and ensure that the &lt;code&gt;on_transport_error&lt;/code&gt; event will be called at
the appropriate time. Thus again, the application developer’s life is improved as he doesn’t need to worry about
handling exceptions in an asynchronous environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    fmex_connection::on_text_frame(std::string_view frame)
    try
    {
        auto jframe =
            json::parse(json::string_view(frame.data(), frame.size()));

        // dispatch on frame type

        auto &amp;amp;type = jframe.as_object().at(&quot;type&quot;);
        if (type == &quot;hello&quot;)
        {
            on_hello();
        }
        else if (type == &quot;ping&quot;)
        {
            ping_event_pong(jframe);
        }
        else if (type.as_string().starts_with(&quot;ticker.&quot;))
        {
            fmt::print(stdout,
                       &quot;fmex: tick {} : {}\n&quot;,
                       type.as_string().subview(7),
                       jframe.as_object().at(&quot;ticker&quot;));
        }
    }
    catch (...)
    {
        fmt::print(stderr, &quot;text frame is not json : {}\n&quot;, frame);
        throw;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compiling and running the program produces output similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
fmex: tick btcusd_p : [1.0879E4,1.407E3,1.0879E4,2.28836E5,1.08795E4,1.13E2,1.0701E4,1.0939E4,1.0663E4,2.51888975E8,2.3378048830533768E4]
fmex: tick btcusd_p : [1.08795E4,1E0,1.0879E4,3.79531E5,1.08795E4,3.518E3,1.0701E4,1.0939E4,1.0663E4,2.51888976E8,2.3378048922449758E4]
fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.7747E5,1.08795E4,7.575E3,1.0701E4,1.0939E4,1.0663E4,2.51888978E8,2.3378049106290182E4]
fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.77468E5,1.08795E4,9.229E3,1.0701E4,1.0939E4,1.0663E4,2.5188898E8,2.337804929013061E4]
fmex: tick btcusd_p : [1.0879E4,1E0,1.0879E4,1.0039E4,1.08795E4,2.54203E5,1.0701E4,1.0939E4,1.0663E4,2.51888981E8,2.3378049382050827E4]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note however, that although pressing ctrl-c is noticed by the application, the fmex feed does not shut down in response.
This is because we have not wired up a mechanism to communicate the &lt;code&gt;stop()&lt;/code&gt; event to the implementation of the 
connection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.8663E4,1.08595E4,4.1457E4,1.07125E4,1.0939E4,1.0667E4,2.58585817E8,2.3968266005011003E4]
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9942E4,1.08595E4,4.3727E4,1.07125E4,1.0939E4,1.0667E4,2.58585819E8,2.3968266189181537E4]
^CInterrupt confirmed. Shutting down
fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9932E4,1.08595E4,4.0933E4,1.07125E4,1.0939E4,1.0667E4,2.58585821E8,2.396826637335208E4]
fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.2722E4,1.08595E4,4.0943E4,1.07125E4,1.0939E4,1.0667E4,2.58585823E8,2.3968266557531104E4]
fmex: tick btcusd_p : [1.08595E4,1.58E2,1.0859E4,6.2732E4,1.08595E4,3.7953E4,1.07125E4,1.0939E4,1.0667E4,2.58585981E8,2.3968281107003917E4]
^Z
[1]+  Stopped                 ./blog_2020_09
$ kill %1

[1]+  Stopped                 ./blog_2020_09
$ 
[1]+  Terminated              ./blog_2020_09
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-3---re-enabling-cancellation&quot;&gt;Step 3 - Re-Enabling Cancellation&lt;/h2&gt;

&lt;p&gt;You will remember from step 1 that we created a little class called &lt;code&gt;sigint_state&lt;/code&gt; which notices that the application 
has received a sigint and checks for a confirming sigint before taking action. We also added a slot to this to pass the 
signal to the fmex connection:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;            fmex_connection_.start();
            sigint_state_.add_slot([this]{
                fmex_connection_.stop();
            });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But we didn’t put any code in &lt;code&gt;wss_transport::stop&lt;/code&gt;. Now all we have to do is provide a function object within 
&lt;code&gt;wss_transport&lt;/code&gt; that we can adjust whenever the current state changes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        // stop signal
        std::function&amp;lt;void()&amp;gt; stop_signal_;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    wss_transport::stop()
    {
        net::dispatch(get_executor(), [this] {
            if (auto sig = boost::exchange(stop_signal_, nullptr))
                sig();
        });
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will also need to provide a way for the connect operation to respond to the stop signal (the user might press
ctrl-c while resolving for example).&lt;/p&gt;

&lt;p&gt;The way I have done this here is a simple approach, merely pass a reference to the &lt;code&gt;wss_transport&lt;/code&gt; into the composed
operation so that the operation can modify the function directly. There are other more scalable ways to do this, but
this is good enough for now.&lt;/p&gt;

&lt;p&gt;The body of the coroutine then becomes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;            auto &amp;amp;impl = *impl_;

            if(ec)
                impl.error = ec;

            if (impl.error)
                return self.complete(impl.error);

#include &amp;lt;boost/asio/yield.hpp&amp;gt;
            reenter(*this)
            {
                transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] {
                    impl.resolver.cancel();
                    impl.error = net::error::operation_aborted;
                };
                yield impl.resolver.async_resolve(
                    impl.host, impl.port, std::move(self));

                //

                transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] {
                    impl.tcp_layer().cancel();
                    impl.error = net::error::operation_aborted;
                };

                impl.tcp_layer().expires_after(15s);
                yield impl.tcp_layer().async_connect(impl.endpoints,
                                                     std::move(self));

                //

                if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(),
                                              impl.host.c_str()))
                    return self.complete(
                        error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()),
                                   net::error::get_ssl_category()));

                //

                impl.tcp_layer().expires_after(15s);
                yield impl.ssl_layer().async_handshake(ssl::stream_base::client,
                                                       std::move(self));

                //

                impl.tcp_layer().expires_after(15s);
                yield impl.ws.async_handshake(
                    impl.host, impl.target, std::move(self));

                //

                transport_-&amp;gt;stop_signal_ = nullptr;
                impl.tcp_layer().expires_never();
                yield self.complete(impl.error);
            }
#include &amp;lt;boost/asio/unyield.hpp&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final source code for 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-3/pre-cxx20/blog-2020-09&quot;&gt;step 3 is here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stopping the program while connecting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
^CInterrupt confirmed. Shutting down
fmex: transport error : system : 125 : Operation canceled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And stopping the program while connected:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
fmex: tick btcusd_p : [1.0882E4,1E0,1.0882E4,3.75594E5,1.08825E4,5.103E3,1.07295E4,1.0939E4,1.06785E4,2.58278146E8,2.3907706652603207E4]
^CInterrupt confirmed. Shutting down
closing websocket
fmex: closed
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;future-development&quot;&gt;Future development&lt;/h1&gt;

&lt;p&gt;Next month I’ll refactor the application to use C++20 coroutines and we can see whether this makes developing
event based systems easier and/or more maintainable.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Cancellation in Beast/Asio and Better Compile Performance with Beast.Websocket This month I will be discussing two issues. One of interest to many people who come to us with questions on the Github Issue Tracker and the #beast channel of Cpplang Slack. Compile Times and Separation of Concerns A common complaint about Boost.Beast is that compilation units that use the websocket::stream template class often take a long time to compile, and that because websocket::stream is a template, this compilation overhead can become viral in an application. This is a valid complaint and we believe there are some reasonable tradeoffs we can make by refactoring the websocket stream to use fewer templates internally. Vinnie has started work to express the WebSocket’s intermediate completion handlers, buffer sequence and executor in terms of a polymorphic object. This would mean a few indirect jumps in the compiled code but would significantly reduce the number of internal template expansions. In the scheme of things, we don’t believe that the virtual function calls will materially affect runtime performance. The branch is here I will be continuing work in this area in the coming days. In the meantime, our general response is to suggest that users create a base class to handle the transport, and communicate important events such as frame received, connection state and the close notification to a derived application-layer class through a private polymorphic interface. In this way, the websocket transport compilation unit may take a while to compile, but it needs to be done only once since the transport layer will rarely change during the development life of an application. Whenever there is a change to the application layer, the transport layer is not affected so websocket-related code is not affected. This approach has a number of benefits. Not least of which is that developing another client implementation over a different websocket connection in the same application becomes trivial. Another benefit is that the application can be designed such that application-level concerns are agnostic of the transport mechanism. Such as when the server can be accessed by multiple means - WSS, WS, long poll, direct connection, unix sockets and so on. In this blog I will present a simplified implementation of this idea. My thanks to the cpplang Slack user @elegracer who most recently asked for guidance on reducing compile times. It was (his/her? Slack is silent on the matter) question which prompted me to finally conjure up a demo. @elegracer’s problem was needing to connect to multiple cryptocurrency exchanges in the same app over websocket. In this particular example I’ll demonstrate a simplified connection to the public FMex market data feed since that was the subject of the original question. Correct Cancellation Our examples in the Beast Repository are rudimentary and don’t cover the issue of graceful shutdown of an application in response to a SIGINT (i.e. the user pressing ctrl-c). It is common for simple programs to exit suddenly in response to this signal, which is the default behaviour. For many applications, this is perfectly fine but not all. We may want active objects in the program to write data to disk, we may want to ensure that the underlying websocket is shut down cleanly and we may want to give the user an opportunity to prevent the shutdown. I will further annotate the example by providing this ability to prevent the shutdown. The user will have to confirm the first SIGINT with another within 5 seconds to confirm. Designing the application When I write IO applications involving Asio and Beast, I prefer to create an “application” object. This has the responsibility of monitoring signals and starting the initial connection objects. It also provides the communication between the two. The construction and configuration of the io_context and ssl::context stay in main(). The executor and ssl context are passed to the application by reference as dependencies. The application can then pass on these refrences as required. It is also worth mentioning that I don’t pass the io_context’s executor as a polymorphic any_io_executor type at this stage. The reason is that I may want in future to upgrade my program to be multi-threaded. If I do this, then each individual io_enabled object such as a connection or the application will need to have its own strand. Getting the strand out of an any_io_executor is not possible in the general case as it will have been type-erased, so for top level objects I pass the executor as io_context::executor_type. It is then up to each object to create its own strand internally which will have the type strand&amp;lt;io_context::executor_type&amp;gt;. The strand type provides the method get_inner_executor which allows the application to extract the underlying io_context::executor_type and pass it to the constructor of any subordinate but otherwise self-contained io objects. The subordinates can then build their own strands from this. Step 1 - A Simple Application Framework That Supports ctrl-c OK, let’s get started and build the framework. Here’s a link to step 1. ssl.hpp and net.hpp simply configure the project to use boost.asio. The idea of these little configuration headers is that they could be generated by the cmake project if necessary to allow the option of upgrading to std networking if it ever arrives. As a matter of style, I like to ensure that no names are created in the global namespace other than main. This saves headaches that could occur if I wrote code on one platform, but then happened to port it to another where the name was already in use by the native system libraries. main.cpp simply creates the io execution context and a default ssl context, creates the application, starts it and runs the io context. At the moment, the only interesting part of our program is the signit_state. This is a state machine which handles the behaviour of the program when a SIGINT is received. Our state machine is doing something a little fancy. Here is the state diagram: Rather than reproduce the code here, please refer to step 1 to see the source code. At this point the program will run and successfully handle ctrl-c: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit Interrupt unconfirmed. Ignoring ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit ^CInterrupt confirmed. Shutting down Step 2 - Connecting to an Exchange Now we need to create our WebSocket transport class and our FMex exchange protocol class that will derive from it. For now we won’t worry about cancellation - we’ll retrofit that in Step 3. Here is the code for step 2. This section introduces two new main classes - the wss_transport and the fmex_connection. In addition, the connection phase of the wss_transport is expressed as a composed operation for exposition purposes (and in my opinion it actually makes the code easier to read than continuation-passing style code) Here is the implementation of the connect coroutine: struct wss_transport::connect_op : asio::coroutine { using executor_type = wss_transport::executor_type; using websock = wss_transport::websock; Here we define the implementation of the coroutine - this is an object which will not be moved for the duration of the execution of the coroutine. This address stability is important because intermediate asynchronous operations will rely on knowing the address of the resolver (and later perhaps other io objects). struct impl_data { impl_data(websock &amp;amp; ws, std::string host, std::string port, std::string target) : ws(ws) , resolver(ws.get_executor()) , host(host) , port(port) , target(target) { } layer_0 &amp;amp; tcp_layer() const { return ws.next_layer().next_layer(); } layer_1 &amp;amp; ssl_layer() const { return ws.next_layer(); } websock &amp;amp; ws; net::ip::tcp::resolver resolver; net::ip::tcp::resolver::results_type endpoints; std::string host, port, target; }; The constructor merely forwards the arguments to the construction of the impl_data. connect_op(websock &amp;amp; ws, std::string host, std::string port, std::string target) : impl_(std::make_unique&amp;lt; impl_data &amp;gt;(ws, host, port, target)) { } This coroutine is both a composed operation and a completion handler for sub-operations. This means it must have an operator() interface matching the requirements of each sub-operation. During the lifetime of this coroutine we will be using the resolver and calling async_connect on the tcp_stream. We therefore provide conforming member functions which store or ignore the and forward the error_code to the main implementation of the coroutine. template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp; self, error_code ec, net::ip::tcp::resolver::results_type results) { impl_-&amp;gt;endpoints = results; (*this)(self, ec); } template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec, net::ip::tcp::endpoint const &amp;amp;) { (*this)(self, ec); } Here is the main implementation of the coroutine. Note that the last two parameters provide defaults. This is in order to allow this member function to match the completion handler signatures of: void() - invoked during async_compose in order to start the coroutine. void(error_code) - invoked by the two functions above and by the async handshakes. void(error_code, std::size_t) - invoked by operations such as async_read and async_write although not strictly necessary here. template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t = 0) { Note that here we are checking the error code before re-entering the coroutine. This is a shortcut which allows us to omit error checking after each sub-operation. This check will happen on every attempt to re-enter the coroutine, including the first entry (at which time ec is guaranteed to be default constructed). if (ec) return self.complete(ec); auto &amp;amp;impl = *impl_; Note the use of the asio yield and unyield headers to create the fake ‘keywords’ reenter and yield in avery limited scope. #include &amp;lt;boost/asio/yield.hpp&amp;gt; reenter(*this) { yield impl.resolver.async_resolve( impl.host, impl.port, std::move(self)); impl.tcp_layer().expires_after(15s); yield impl.tcp_layer().async_connect(impl.endpoints, std::move(self)); if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(), impl.host.c_str())) return self.complete( error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()), net::error::get_ssl_category())); impl.tcp_layer().expires_after(15s); yield impl.ssl_layer().async_handshake(ssl::stream_base::client, std::move(self)); impl.tcp_layer().expires_after(15s); yield impl.ws.async_handshake( impl.host, impl.target, std::move(self)); If the coroutine is re-entered here, it must be because there was no error (if there was an error, it would have been caught by the pre-reentry error check above). Since execution has resumed here in the completion handler of the async_handshake initiating function, we are guaranteed to be executing in the correct executor. Therefore we can simply call complete directly without needing to post to an executor. Note that the async_compose call which will encapsulate the use of this class embeds this object into a wrapper which provides the executor_type and get_executor() mechanism which asio uses to determine on which executor to invoke completion handlers. impl.tcp_layer().expires_never(); yield self.complete(ec); } #include &amp;lt;boost/asio/unyield.hpp&amp;gt; } std::unique_ptr&amp;lt; impl_data &amp;gt; impl_; }; The wss_connection class provides the bare bones required to connect a websocket and maintain the connection. It provides a protected interface so that derived classes can send text frames and it will call private virtual functions in order to notify the derived class of: transport up (websocket connection established). frame received. connection error (either during connection or operation). websocket close - the server has requested or agreed to a graceful shutdown. Connection errors will only be notified once, and once a connection error has been indicated, no other event will reach the derived class. One of the many areas that trips up asio/beast beginners is that care must be taken to ensure that only one async_write is in progress at a time on the WebSocket (or indeed any async io object). For this reason we implement a simple transmit queue state which can be considered to be an orthogonal region (parallel task) to the read state. // send_state - data to control sending data std::deque&amp;lt;std::string&amp;gt; send_queue_; enum send_state { not_sending, sending } send_state_ = not_sending; You will note that I have used a std::deque to hold the pending messages. Although a deque has theoretically better complexity when inserting or removing items at the ends than a vector, this is not the reason for choosing this data structure. The actual reason is that items in a deque are guaranteed to have a stable address, even when other items are added or removed. This is useful as it means we don’t have to move frames out of the transmit queue in order to send them. Remember that during an async_write, the data to which the supplied buffer sequence refers must have a stable address. Here are the functions that deal with the send state transitions. void wss_transport::send_text_frame(std::string frame) { if (state_ != connected) return; send_queue_.push_back(std::move(frame)); start_sending(); } void wss_transport::start_sending() { if (state_ == connected &amp;amp;&amp;amp; send_state_ == not_sending &amp;amp;&amp;amp; !send_queue_.empty()) { send_state_ = sending; websock_.async_write(net::buffer(send_queue_.front()), [this](error_code const &amp;amp;ec, std::size_t bt) { handle_send(ec, bt); }); } } void wss_transport::handle_send(const error_code &amp;amp;ec, std::size_t) { send_state_ = not_sending; send_queue_.pop_front(); if (ec) event_transport_error(ec); else start_sending(); } Finally, we can implement our specific exchange protocol on top of the wss_connection. In this case, FMex eschews the ping/pong built into websockets and requires a json ping/pong to be initiated by the client. void fmex_connection::ping_enter_state() { BOOST_ASSERT(ping_state_ == ping_not_started); ping_enter_wait(); } void fmex_connection::ping_enter_wait() { ping_state_ = ping_wait; ping_timer_.expires_after(5s); ping_timer_.async_wait([this](error_code const &amp;amp;ec) { if (!ec) ping_event_timeout(); }); } void fmex_connection::ping_event_timeout() { ping_state_ = ping_waiting_pong; auto frame = json::value(); auto &amp;amp;o = frame.emplace_object(); o[&quot;cmd&quot;] = &quot;ping&quot;; o[&quot;id&quot;] = &quot;my_ping_ident&quot;; o[&quot;args&quot;].emplace_array().push_back(timestamp()); send_text_frame(json::serialize(frame)); } void fmex_connection::ping_event_pong(json::value const &amp;amp;frame) { ping_enter_wait(); } Note that since we have implemented frame transmission in the base class in terms of a queue, the fmex class has no need to worry about ensuring the one-write-at-a-time rule. The base class handles it. This makes the application developer’s life easy. Finally, we implement on_text_frame and write a little message parser and switch. Note that this function may throw. The base class will catch any exceptions thrown here and ensure that the on_transport_error event will be called at the appropriate time. Thus again, the application developer’s life is improved as he doesn’t need to worry about handling exceptions in an asynchronous environment. void fmex_connection::on_text_frame(std::string_view frame) try { auto jframe = json::parse(json::string_view(frame.data(), frame.size())); // dispatch on frame type auto &amp;amp;type = jframe.as_object().at(&quot;type&quot;); if (type == &quot;hello&quot;) { on_hello(); } else if (type == &quot;ping&quot;) { ping_event_pong(jframe); } else if (type.as_string().starts_with(&quot;ticker.&quot;)) { fmt::print(stdout, &quot;fmex: tick {} : {}\n&quot;, type.as_string().subview(7), jframe.as_object().at(&quot;ticker&quot;)); } } catch (...) { fmt::print(stderr, &quot;text frame is not json : {}\n&quot;, frame); throw; } Compiling and running the program produces output similar to this: Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello fmex: tick btcusd_p : [1.0879E4,1.407E3,1.0879E4,2.28836E5,1.08795E4,1.13E2,1.0701E4,1.0939E4,1.0663E4,2.51888975E8,2.3378048830533768E4] fmex: tick btcusd_p : [1.08795E4,1E0,1.0879E4,3.79531E5,1.08795E4,3.518E3,1.0701E4,1.0939E4,1.0663E4,2.51888976E8,2.3378048922449758E4] fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.7747E5,1.08795E4,7.575E3,1.0701E4,1.0939E4,1.0663E4,2.51888978E8,2.3378049106290182E4] fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.77468E5,1.08795E4,9.229E3,1.0701E4,1.0939E4,1.0663E4,2.5188898E8,2.337804929013061E4] fmex: tick btcusd_p : [1.0879E4,1E0,1.0879E4,1.0039E4,1.08795E4,2.54203E5,1.0701E4,1.0939E4,1.0663E4,2.51888981E8,2.3378049382050827E4] Note however, that although pressing ctrl-c is noticed by the application, the fmex feed does not shut down in response. This is because we have not wired up a mechanism to communicate the stop() event to the implementation of the connection: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.8663E4,1.08595E4,4.1457E4,1.07125E4,1.0939E4,1.0667E4,2.58585817E8,2.3968266005011003E4] ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9942E4,1.08595E4,4.3727E4,1.07125E4,1.0939E4,1.0667E4,2.58585819E8,2.3968266189181537E4] ^CInterrupt confirmed. Shutting down fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9932E4,1.08595E4,4.0933E4,1.07125E4,1.0939E4,1.0667E4,2.58585821E8,2.396826637335208E4] fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.2722E4,1.08595E4,4.0943E4,1.07125E4,1.0939E4,1.0667E4,2.58585823E8,2.3968266557531104E4] fmex: tick btcusd_p : [1.08595E4,1.58E2,1.0859E4,6.2732E4,1.08595E4,3.7953E4,1.07125E4,1.0939E4,1.0667E4,2.58585981E8,2.3968281107003917E4] ^Z [1]+ Stopped ./blog_2020_09 $ kill %1 [1]+ Stopped ./blog_2020_09 $ [1]+ Terminated ./blog_2020_09 Step 3 - Re-Enabling Cancellation You will remember from step 1 that we created a little class called sigint_state which notices that the application has received a sigint and checks for a confirming sigint before taking action. We also added a slot to this to pass the signal to the fmex connection: fmex_connection_.start(); sigint_state_.add_slot([this]{ fmex_connection_.stop(); }); But we didn’t put any code in wss_transport::stop. Now all we have to do is provide a function object within wss_transport that we can adjust whenever the current state changes: // stop signal std::function&amp;lt;void()&amp;gt; stop_signal_; void wss_transport::stop() { net::dispatch(get_executor(), [this] { if (auto sig = boost::exchange(stop_signal_, nullptr)) sig(); }); } We will also need to provide a way for the connect operation to respond to the stop signal (the user might press ctrl-c while resolving for example). The way I have done this here is a simple approach, merely pass a reference to the wss_transport into the composed operation so that the operation can modify the function directly. There are other more scalable ways to do this, but this is good enough for now. The body of the coroutine then becomes: auto &amp;amp;impl = *impl_; if(ec) impl.error = ec; if (impl.error) return self.complete(impl.error); #include &amp;lt;boost/asio/yield.hpp&amp;gt; reenter(*this) { transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] { impl.resolver.cancel(); impl.error = net::error::operation_aborted; }; yield impl.resolver.async_resolve( impl.host, impl.port, std::move(self)); // transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] { impl.tcp_layer().cancel(); impl.error = net::error::operation_aborted; }; impl.tcp_layer().expires_after(15s); yield impl.tcp_layer().async_connect(impl.endpoints, std::move(self)); // if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(), impl.host.c_str())) return self.complete( error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()), net::error::get_ssl_category())); // impl.tcp_layer().expires_after(15s); yield impl.ssl_layer().async_handshake(ssl::stream_base::client, std::move(self)); // impl.tcp_layer().expires_after(15s); yield impl.ws.async_handshake( impl.host, impl.target, std::move(self)); // transport_-&amp;gt;stop_signal_ = nullptr; impl.tcp_layer().expires_never(); yield self.complete(impl.error); } #include &amp;lt;boost/asio/unyield.hpp&amp;gt; The final source code for step 3 is here. Stopping the program while connecting: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit ^CInterrupt confirmed. Shutting down fmex: transport error : system : 125 : Operation canceled And stopping the program while connected: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit fmex: tick btcusd_p : [1.0882E4,1E0,1.0882E4,3.75594E5,1.08825E4,5.103E3,1.07295E4,1.0939E4,1.06785E4,2.58278146E8,2.3907706652603207E4] ^CInterrupt confirmed. Shutting down closing websocket fmex: closed Future development Next month I’ll refactor the application to use C++20 coroutines and we can see whether this makes developing event based systems easier and/or more maintainable. Thanks for reading.</summary></entry><entry><title type="html">Krystian’s September Update</title><link href="http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate.html" rel="alternate" type="text/html" title="Krystian’s September Update" /><published>2020-09-29T00:00:00+00:00</published><updated>2020-09-29T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate.html">&lt;h1 id=&quot;reviewing-the-review&quot;&gt;Reviewing the review&lt;/h1&gt;

&lt;p&gt;The review period for Boost.JSON has come and gone, and we got some great feedback on the design of the library. Glancing over the results, it appears that the general mood was to accept the library. This doesn’t mean that there weren’t any problem areas – most notably the documentation, which often did contain the information people wanted, but it was difficult to find.&lt;/p&gt;

&lt;p&gt;Other points of contention were the use of a push parser as opposed to a pull parser, the use of &lt;code&gt;double&lt;/code&gt;, &lt;code&gt;uint64_t&lt;/code&gt;, and &lt;code&gt;int64_t&lt;/code&gt; without allowing for users to change them, and the value conversion interface. Overall some very good points were made, and I’d like to thank everyone for participating in the review.&lt;/p&gt;

&lt;h1 id=&quot;customizing-the-build&quot;&gt;Customizing the build&lt;/h1&gt;

&lt;p&gt;I put a bit of work into improving our CI matrix, as it had several redundant configurations and did not test newer compiler versions (e.g. GCC 10, clang 11), nor did we have any 32-bit jobs. The most difficult thing about working on the build matrix is balancing how exhaustive it is with the turnaround time – sure, we could add 60 configurations that test x86, x86-64, and ARM on every major compiler version released since 2011, but the turnaround would be abysmal.&lt;/p&gt;

&lt;p&gt;To alleviate this, I only added 32-bit jobs for the sanitizers that use a recent version of GCC. It’s a less common configuration in the days of 64-bit universality, and if 64 bit works then it’s highly likely that 32 bit will “just work” as well.&lt;/p&gt;

&lt;p&gt;Here’s a table of the new Travis configurations that will be added:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Compiler&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Library&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;C++ Standard&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Variant&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OS&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Architecture&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Job&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Documentation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 8.4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Coverage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 6.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Valgrind&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Address Sanitizer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;UB Sanitizer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;icc 2021.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Bionic)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 4.8.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 4.9.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 5.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 6.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 7.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 8.4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 9.3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 9.3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 10.2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 10.2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 3.8.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 4.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 5.0.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 6.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 7.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 9.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 9.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 10.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 10.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I think it strikes a good balance between exhaustiveness and turnaround time, and we now test the most recent compiler versions to make sure they won’t cause problems on the cutting edge.&lt;/p&gt;

&lt;h1 id=&quot;binary-size&quot;&gt;Binary size&lt;/h1&gt;

&lt;p&gt;It doesn’t matter how good a library is if it’s too big to use within your environment. As with all things in computer science, there is a trade-off between size and speed; seldom can you have both. We have been exploring options to reduce the size of the binary, and this mostly involved removing a lot of the pre-written tables we have (such as the ever-controversial jump table), since it allows the compiler to take into account the specific options it was past and optimize for those constraints (i.e. size and speed) rather than hard-coding in a set configuration as we did with the jump tables.&lt;/p&gt;

&lt;p&gt;Peter Dimov also helped out by transitioning our compile-time system of generating unique parse functions for each permutation of extensions to a runtime system, which drastically decreases the binary size without affecting performance too much.&lt;/p&gt;

&lt;p&gt;I must admit I’m not the biggest fan of these changes, but it’s important to support the use of Boost.JSON in embedded environments. As Peter has said time and time again: don’t overfit for a particular use-case or configuration.&lt;/p&gt;

&lt;p&gt;Another place with room for improvement is with string to float-point conversions. Right now we calculate a mantissa and base-10 exponent, then lookup the value in a massive table that contains pre-calculated powers of 10 from 1e-308 to 1e+308. As you can surmise, this takes up a substantial amount of space (8 bytes * 618 elements = 4.95 kb).&lt;/p&gt;

&lt;p&gt;Here is a boiled down version of how we currently perform the conversion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;double calculate_float(
    std::uint64_t mantissa, 
    std::uint32_t exponent, 
    bool sign)
{
    constexpr static double table[618] = 
    { 
        1e-308, 1e-307, 
        ..., 
        1e307, 1e308 
    };
    double power;
    if(exponent &amp;lt; -308 || exponent &amp;gt; 308)
        power = std::pow(10.0, exponent);
    else
        power = table[exponent + 308]
    double result = mantissa * power;
    return sign ? -result : result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To further reduce the size of the binary, Peter suggested that we instead calculate &lt;code&gt;power&lt;/code&gt; as &lt;code&gt;10^floor(exponent / 8) * 10^(exponent mod 8)&lt;/code&gt;. Yes, the division operations there might look expensive, but any decent optimizing compiler will transform &lt;code&gt;exponent / 8&lt;/code&gt; to &lt;code&gt;exponent &amp;gt;&amp;gt; 3&lt;/code&gt;, and &lt;code&gt;exponent mod 8&lt;/code&gt; to &lt;code&gt;exponent &amp;amp; 7&lt;/code&gt;. This does introduce another multiplication instruction, but at the same time, it makes our table 8 times smaller. In theory, the slight drop in performance is worth the significant reduction in binary size.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Reviewing the review The review period for Boost.JSON has come and gone, and we got some great feedback on the design of the library. Glancing over the results, it appears that the general mood was to accept the library. This doesn’t mean that there weren’t any problem areas – most notably the documentation, which often did contain the information people wanted, but it was difficult to find. Other points of contention were the use of a push parser as opposed to a pull parser, the use of double, uint64_t, and int64_t without allowing for users to change them, and the value conversion interface. Overall some very good points were made, and I’d like to thank everyone for participating in the review. Customizing the build I put a bit of work into improving our CI matrix, as it had several redundant configurations and did not test newer compiler versions (e.g. GCC 10, clang 11), nor did we have any 32-bit jobs. The most difficult thing about working on the build matrix is balancing how exhaustive it is with the turnaround time – sure, we could add 60 configurations that test x86, x86-64, and ARM on every major compiler version released since 2011, but the turnaround would be abysmal. To alleviate this, I only added 32-bit jobs for the sanitizers that use a recent version of GCC. It’s a less common configuration in the days of 64-bit universality, and if 64 bit works then it’s highly likely that 32 bit will “just work” as well. Here’s a table of the new Travis configurations that will be added: Compiler Library C++ Standard Variant OS Architecture Job — — — Boost Linux (Xenial) x86-64 Documentation gcc 8.4.0 libstdc++ 11 Boost Linux (Xenial) x86-64 Coverage clang 6.0.1 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 Valgrind clang 11.0.0 libstdc++ 17 Boost Linux (Xenial) x86-64 Address Sanitizer clang 11.0.0 libstdc++ 17 Boost Linux (Xenial) x86-64 UB Sanitizer msvc 14.1 MS STL 11, 14, 17 Boost Windows x86-64 — msvc 14.1 MS STL 17, 2a Standalone Windows x86-64 — msvc 14.2 MS STL 17, 2a Boost Windows x86-64 — msvc 14.2 MS STL 17, 2a Standalone Windows x86-64 — icc 2021.1 libstdc++ 11, 14, 17 Boost Linux (Bionic) x86-64 — gcc 4.8.5 libstdc++ 11 Boost Linux (Trusty) x86-64 — gcc 4.9.4 libstdc++ 11 Boost Linux (Trusty) x86-64 — gcc 5.5.0 libstdc++ 11 Boost Linux (Xenial) x86-64 — gcc 6.5.0 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — gcc 7.5.0 libstdc++ 14, 17 Boost Linux (Xenial) x86-64 — gcc 8.4.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — gcc 9.3.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — gcc 9.3.0 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — gcc 10.2.0 libstdc++ 17, 2a Boost Linux (Focal) x86-64 — gcc 10.2.0 libstdc++ 17, 2a Standalone Linux (Focal) x86-64 — gcc (trunk) libstdc++ 17, 2a Boost Linux (Focal) x86-64 — gcc (trunk) libstdc++ 17, 2a Standalone Linux (Focal) x86-64 — clang 3.8.0 libstdc++ 11 Boost Linux (Trusty) x86-64 — clang 4.0.0 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — clang 5.0.2 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — clang 6.0.1 libstdc++ 14, 17 Boost Linux (Xenial) x86-64 — clang 7.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 9.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 9.0.1 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang 10.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 10.0.1 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang 11.0.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 11.0.0 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang (trunk) libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang (trunk) libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — I think it strikes a good balance between exhaustiveness and turnaround time, and we now test the most recent compiler versions to make sure they won’t cause problems on the cutting edge. Binary size It doesn’t matter how good a library is if it’s too big to use within your environment. As with all things in computer science, there is a trade-off between size and speed; seldom can you have both. We have been exploring options to reduce the size of the binary, and this mostly involved removing a lot of the pre-written tables we have (such as the ever-controversial jump table), since it allows the compiler to take into account the specific options it was past and optimize for those constraints (i.e. size and speed) rather than hard-coding in a set configuration as we did with the jump tables. Peter Dimov also helped out by transitioning our compile-time system of generating unique parse functions for each permutation of extensions to a runtime system, which drastically decreases the binary size without affecting performance too much. I must admit I’m not the biggest fan of these changes, but it’s important to support the use of Boost.JSON in embedded environments. As Peter has said time and time again: don’t overfit for a particular use-case or configuration. Another place with room for improvement is with string to float-point conversions. Right now we calculate a mantissa and base-10 exponent, then lookup the value in a massive table that contains pre-calculated powers of 10 from 1e-308 to 1e+308. As you can surmise, this takes up a substantial amount of space (8 bytes * 618 elements = 4.95 kb). Here is a boiled down version of how we currently perform the conversion: double calculate_float( std::uint64_t mantissa, std::uint32_t exponent, bool sign) { constexpr static double table[618] = { 1e-308, 1e-307, ..., 1e307, 1e308 }; double power; if(exponent &amp;lt; -308 || exponent &amp;gt; 308) power = std::pow(10.0, exponent); else power = table[exponent + 308] double result = mantissa * power; return sign ? -result : result; } To further reduce the size of the binary, Peter suggested that we instead calculate power as 10^floor(exponent / 8) * 10^(exponent mod 8). Yes, the division operations there might look expensive, but any decent optimizing compiler will transform exponent / 8 to exponent &amp;gt;&amp;gt; 3, and exponent mod 8 to exponent &amp;amp; 7. This does introduce another multiplication instruction, but at the same time, it makes our table 8 times smaller. In theory, the slight drop in performance is worth the significant reduction in binary size.</summary></entry><entry><title type="html">Krystian’s August Update</title><link href="http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate.html" rel="alternate" type="text/html" title="Krystian’s August Update" /><published>2020-09-06T00:00:00+00:00</published><updated>2020-09-06T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate.html">&lt;h1 id=&quot;boostjson&quot;&gt;Boost.JSON&lt;/h1&gt;

&lt;p&gt;Boost.JSON is officially scheduled for review! It starts on September 14th, so there isn’t much time left to finish up polishing the library – but it looks like we will make the deadline.&lt;/p&gt;

&lt;h2 id=&quot;optimize-optimize-optimize&quot;&gt;Optimize, optimize, optimize&lt;/h2&gt;

&lt;p&gt;Boost.JSON’s performance has significantly increased in the past month. The change to the parsing functions where we pass and return &lt;code&gt;const char*&lt;/code&gt; instead of &lt;code&gt;result&lt;/code&gt; (detailed in my last post) was merged, bringing large gains across the board. After this, my work on optimizing &lt;code&gt;basic_parser&lt;/code&gt; was complete (for now…), save for a few more minor changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The handler is stored as the first data member as opposed to passing a reference to each parse function. This means that the &lt;code&gt;this&lt;/code&gt; pointer for &lt;code&gt;basic_parser&lt;/code&gt; is the &lt;code&gt;this&lt;/code&gt; pointer for the handler, which eliminates some register spills.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parser’s depth (i.e. nesting level of objects/arrays) is now tracked as &lt;code&gt;max_depth - actual_depth&lt;/code&gt;, meaning that we don’t have to read &lt;code&gt;max_depth&lt;/code&gt; from memory each time a structure is parsed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;parse_string&lt;/code&gt; was split into two functions: &lt;code&gt;parse_unescaped&lt;/code&gt; and &lt;code&gt;parse_escaped&lt;/code&gt;. The former is much cheaper to call as it doesn’t have to store the string within a local buffer, and since unescaped strings are vastly more common in JSON documents, this increases performance considerably.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-dom-parser&quot;&gt;The DOM parser&lt;/h3&gt;

&lt;p&gt;Our old implementation of &lt;code&gt;parser&lt;/code&gt; was pretty wasteful. It stored state information (such as whether we were parsing an object or array), keys, and values, all on one stack. This proved to be quite a pain when it came to unwinding it and also required us to align the stack when pushing arrays and objects.&lt;/p&gt;

&lt;p&gt;Several months ago, Vinnie and I tried to figure out how to make the homogeneous but came to a dead end. I decided to revisit the idea, and after some experimentation, it became apparent that there was a &lt;em&gt;lot&lt;/em&gt; of redundancy in the implementation. For example, &lt;code&gt;basic_parser&lt;/code&gt; already keeps track of the current object/array/string/key size, so there is no reason to so within &lt;code&gt;parser&lt;/code&gt;. The state information we were tracking was also not needed – &lt;code&gt;basic_parser&lt;/code&gt; already checks the syntactic correctness of the input. That left one more thing: strings and keys.&lt;/p&gt;

&lt;p&gt;My rudimentary implementation required two stacks: one for keys and strings, and the other for values. Other information, such as the sizes of objects and arrays, were obtained from &lt;code&gt;basic_parser&lt;/code&gt;. My implementation, though primitive, gave some promising results on the benchmarks: up to 10% for certain documents. After some brainstorming with Vinnie, he had the idea of storing object keys as values; the last piece of the puzzle we needed to make this thing work.&lt;/p&gt;

&lt;p&gt;His fleshed-out implementation was even faster. In just a week’s time, Boost.JSON’s performance increased by some 15%. I’m still working on the finishing touches, but the results are looking promising.&lt;/p&gt;

&lt;h2 id=&quot;more-utf-8-validation-malarkey&quot;&gt;More UTF-8 validation malarkey&lt;/h2&gt;

&lt;p&gt;Out of all the things I’ve worked on, nothing has proved as frustrating as UTF-8 validation. The validation itself is trivial; but making it work with an incremental parser is remarkably difficult. Shortly after merging the feature, &lt;a href=&quot;https://github.com/CPPAlliance/json/issues/162&quot;&gt;an issue was opened&lt;/a&gt;; while validation worked just fine when a document was parsed without suspending, I neglected to write tests for incremental parsing, and that’s precisely where the bug was. Turns out, if parsing suspended while validating a UTF-8 byte sequence, the handler just would not be called.&lt;/p&gt;

&lt;p&gt;This was… quite a problem to say the least, and required me to reimplement UTF-8 validation from scratch – but with a twist. We don’t want to pass partial UTF-8 sequences because it just transfers the burden of assembling incomplete sequences to the handler. This means that we need to store the sequences, append to them until we get a complete codepoint, and only then can we validate and send it off to the handler. Doing this in an efficient manner proved to be quite challenging, so I ended up with a “fix” that was 50% code and 50% &lt;code&gt;// KRYSTIAN TODO: this can be optimized&lt;/code&gt;. The tests provided in the issue finally passed, so the patch was merged.&lt;/p&gt;

&lt;p&gt;I thought my woes with validation were over, but I was wrong. Just over a week later, a new issue rolled in:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/CPPAlliance/json/issues/162&quot;&gt;Handler not invoked correctly in multi-byte UTF8 sequences, part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Luckily, fixing this didn’t require another rewrite. This taught me a fine lesson in exhaustive testing.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Boost.JSON Boost.JSON is officially scheduled for review! It starts on September 14th, so there isn’t much time left to finish up polishing the library – but it looks like we will make the deadline. Optimize, optimize, optimize Boost.JSON’s performance has significantly increased in the past month. The change to the parsing functions where we pass and return const char* instead of result (detailed in my last post) was merged, bringing large gains across the board. After this, my work on optimizing basic_parser was complete (for now…), save for a few more minor changes: The handler is stored as the first data member as opposed to passing a reference to each parse function. This means that the this pointer for basic_parser is the this pointer for the handler, which eliminates some register spills. The parser’s depth (i.e. nesting level of objects/arrays) is now tracked as max_depth - actual_depth, meaning that we don’t have to read max_depth from memory each time a structure is parsed. parse_string was split into two functions: parse_unescaped and parse_escaped. The former is much cheaper to call as it doesn’t have to store the string within a local buffer, and since unescaped strings are vastly more common in JSON documents, this increases performance considerably. The DOM parser Our old implementation of parser was pretty wasteful. It stored state information (such as whether we were parsing an object or array), keys, and values, all on one stack. This proved to be quite a pain when it came to unwinding it and also required us to align the stack when pushing arrays and objects. Several months ago, Vinnie and I tried to figure out how to make the homogeneous but came to a dead end. I decided to revisit the idea, and after some experimentation, it became apparent that there was a lot of redundancy in the implementation. For example, basic_parser already keeps track of the current object/array/string/key size, so there is no reason to so within parser. The state information we were tracking was also not needed – basic_parser already checks the syntactic correctness of the input. That left one more thing: strings and keys. My rudimentary implementation required two stacks: one for keys and strings, and the other for values. Other information, such as the sizes of objects and arrays, were obtained from basic_parser. My implementation, though primitive, gave some promising results on the benchmarks: up to 10% for certain documents. After some brainstorming with Vinnie, he had the idea of storing object keys as values; the last piece of the puzzle we needed to make this thing work. His fleshed-out implementation was even faster. In just a week’s time, Boost.JSON’s performance increased by some 15%. I’m still working on the finishing touches, but the results are looking promising. More UTF-8 validation malarkey Out of all the things I’ve worked on, nothing has proved as frustrating as UTF-8 validation. The validation itself is trivial; but making it work with an incremental parser is remarkably difficult. Shortly after merging the feature, an issue was opened; while validation worked just fine when a document was parsed without suspending, I neglected to write tests for incremental parsing, and that’s precisely where the bug was. Turns out, if parsing suspended while validating a UTF-8 byte sequence, the handler just would not be called. This was… quite a problem to say the least, and required me to reimplement UTF-8 validation from scratch – but with a twist. We don’t want to pass partial UTF-8 sequences because it just transfers the burden of assembling incomplete sequences to the handler. This means that we need to store the sequences, append to them until we get a complete codepoint, and only then can we validate and send it off to the handler. Doing this in an efficient manner proved to be quite challenging, so I ended up with a “fix” that was 50% code and 50% // KRYSTIAN TODO: this can be optimized. The tests provided in the issue finally passed, so the patch was merged. I thought my woes with validation were over, but I was wrong. Just over a week later, a new issue rolled in: Handler not invoked correctly in multi-byte UTF8 sequences, part 2 Luckily, fixing this didn’t require another rewrite. This taught me a fine lesson in exhaustive testing.</summary></entry><entry><title type="html">Richard’s August Update</title><link href="http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate.html" rel="alternate" type="text/html" title="Richard’s August Update" /><published>2020-09-01T00:00:00+00:00</published><updated>2020-09-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate.html">&lt;h1 id=&quot;new-debugging-feature-in-asio-and-beast&quot;&gt;New Debugging Feature in Asio and Beast&lt;/h1&gt;

&lt;p&gt;As covered previously, Boost 1.74 brought an implementation of the new unified executors model to Boost.Asio.&lt;/p&gt;

&lt;p&gt;Support for this is not the only thing that is new in Beast.&lt;/p&gt;

&lt;p&gt;Chris Kohlhoff recently submitted a &lt;a href=&quot;https://github.com/boostorg/beast/pull/2053&quot;&gt;PR&lt;/a&gt; to Beast’s repository 
demonstrating how to annotate source code with the &lt;code&gt;BOOST_ASIO_HANDLER_LOCATION&lt;/code&gt; macro. I have since followed up and 
annotated all asynchronous operations in Beast this way.&lt;/p&gt;

&lt;p&gt;In a normal build, there is no effect (and zero extra code generation). However, defining the preprocessor macro 
&lt;code&gt;BOOST_ASIO_ENABLE_HANDLER_TRACKING&lt;/code&gt; will cause these macros to generate code which will emit handler tracking
log data to stdout in a very specific format.&lt;/p&gt;

&lt;p&gt;The output is designed to describe the flow of asynchronous events in a format suitable for generating a visualisation
in linear terms. i.e. the asynchronous events are flattened and linked to show causality.&lt;/p&gt;

&lt;p&gt;Here is an example of the output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@asio|1597543084.233257|&amp;gt;33|
@asio|1597543084.233273|33|deadline_timer@0x7fa6cac25218.cancel
@asio|1597543084.233681|33^34|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321)
@asio|1597543084.233681|33^34|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331)
@asio|1597543084.233681|33^34|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201)
@asio|1597543084.233681|33^34|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64)
@asio|1597543084.233681|33^34|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223)
@asio|1597543084.233681|33^34|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277)
@asio|1597543084.233681|33*34|deadline_timer@0x7fa6cac25298.async_wait
@asio|1597543084.233801|33^35|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373)
@asio|1597543084.233801|33^35|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331)
@asio|1597543084.233801|33^35|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201)
@asio|1597543084.233801|33^35|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64)
@asio|1597543084.233801|33^35|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223)
@asio|1597543084.233801|33^35|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277)
@asio|1597543084.233801|33*35|socket@0x7fa6cac251c8.async_send
@asio|1597543084.233910|.35|non_blocking_send,ec=system:0,bytes_transferred=103
@asio|1597543084.233949|&amp;lt;33|
@asio|1597543084.233983|&amp;lt;31|
@asio|1597543084.234031|&amp;gt;30|ec=system:89
@asio|1597543084.234045|30*36|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234054|&amp;gt;36|
@asio|1597543084.234064|&amp;lt;36|
@asio|1597543084.234072|&amp;lt;30|
@asio|1597543084.234086|&amp;gt;35|ec=system:0,bytes_transferred=103
@asio|1597543084.234100|35*37|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234109|&amp;gt;37|
@asio|1597543084.234119|37|deadline_timer@0x7fa6cac25298.cancel
@asio|1597543084.234198|37^38|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321)
@asio|1597543084.234198|37^38|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168)
@asio|1597543084.234198|37^38|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212)
@asio|1597543084.234198|37^38|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297)
@asio|1597543084.234198|37^38|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101)
@asio|1597543084.234198|37*38|deadline_timer@0x7fa6cac25218.async_wait
@asio|1597543084.234288|37^39|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373)
@asio|1597543084.234288|37^39|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168)
@asio|1597543084.234288|37^39|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212)
@asio|1597543084.234288|37^39|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297)
@asio|1597543084.234288|37^39|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101)
@asio|1597543084.234288|37*39|socket@0x7fa6cac251c8.async_receive
@asio|1597543084.234334|.39|non_blocking_recv,ec=system:35,bytes_transferred=0
@asio|1597543084.234353|&amp;lt;37|
@asio|1597543084.234364|&amp;lt;35|
@asio|1597543084.234380|&amp;gt;34|ec=system:89
@asio|1597543084.234392|34*40|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234401|&amp;gt;40|
@asio|1597543084.234408|&amp;lt;40|
@asio|1597543084.234416|&amp;lt;34|
@asio|1597543084.427594|.39|non_blocking_recv,ec=system:0,bytes_transferred=534
@asio|1597543084.427680|&amp;gt;39|ec=system:0,bytes_transferred=534
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far, so good. But not very informative or friendly to the native eye.&lt;/p&gt;

&lt;p&gt;Fortunately as of Boost 1.74 there is a tool in the Asio source tree to convert this data into something consumable by the open source
tool dot, which can then output the resulting execution graph in one of a number of common graphical formats such as
PNG, BMP, SVG and many others.&lt;/p&gt;

&lt;p&gt;Here is an example of a visualisation of a simple execution graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-09-01-handler-tracking-example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The tool you need to do this is in the &lt;code&gt;asio&lt;/code&gt; subproject of the Boost repo. The full path is 
&lt;code&gt;libs/asio/tools/handlerviz.pl&lt;/code&gt;. The command is self-documenting but for clarity, the process would be like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compile and link your program with the compiler flag &lt;code&gt;-DBOOST_ASIO_ENABLE_HANDLER_TRACKING&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;run your program, capturing stdout to a file (say &lt;code&gt;mylog.txt&lt;/code&gt;) (or you can pipe it to the next step)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;handlerviz.pl &amp;lt; mylog.txt | dot -Tpng mygraph.png&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;You should now be able to view your graph in a web browser, editor or picture viewer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The documentation for dot is &lt;a href=&quot;https://linux.die.net/man/1/dot&quot;&gt;here&lt;/a&gt; dot is usually available in the graphviz package 
of your linux distro/brew cask. Windows users can download an executable suite 
&lt;a href=&quot;https://www.graphviz.org/download/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have written your own asynchronous operations to compliment Beast or Asio, or indeed you just wish you add your
handler locations to the graph output, you can do so by inserting the &lt;code&gt;BOOST_ASIO_HANDLER_LOCATION&lt;/code&gt; macro just before
each asynchronous suspension point (i.e. just before the call to &lt;code&gt;async_xxx&lt;/code&gt;). If you’re doing this in an Asio 
&lt;code&gt;coroutine&lt;/code&gt; (not to be confused with C++ coroutines) then be sure to place the macro in curly braces after the 
YIELD macro, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    ...

    // this marks a suspension point of the coroutine
    BOOST_ASIO_CORO_YIELD
    {
        // This macro creates scoped variables so must be in a private scope
        BOOST_ASIO_HANDLER_LOCATION((           // note: double open brackets
            __FILE__, __LINE__,                 // source location
            &quot;websocket::tcp::async_teardown&quot;    // name of the initiating function
        ));

        // this is the initiation of the next inner asynchronous operation
        s_.async_wait(
            net::socket_base::wait_read,
                beast::detail::bind_continuation(std::move(*this)));

        // there is an implied return statement here
    }

    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When writing applications, people historically have used Continuation Passing Style when calling asynchronous 
operations, capturing a shared_ptr to the connection implementation in each handler (continuation).&lt;/p&gt;

&lt;p&gt;When using this macro in user code with written in continuation passing style, you might do so like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void send_request(http::request&amp;lt;http::string_body&amp;gt; req)
{
    send_queue_.push_back(std::move(req));
    if (!sending_)
    {
        sending_ = true;
        maybe_initiate_send();
    }
}

void my_connection_impl::maybe_initiate_send()
{
    if (send_queue_.empty())
    {
        sending_ = false;
        return;
    }

    // assume request_queue_ is a std::deque so elements will have stable addresses
    auto&amp;amp; current_request = request_queue_.front(); 

    BOOST_ASIO_HANDLER_LOCATION((
        __FILE__, __LINE__,
        &quot;my_connection_impl::maybe_initiate_send&quot;
    ));

    // suspension point

    boost::beast::http::async_write(stream_, current_request_, 
        [self = this-&amp;gt;shared_from_this()](boost::beast::error_code ec, std::size_t)
        {
            // continuation

            if (!ec)
            {
                self-&amp;gt;request_queue_.pop_front();
                self-&amp;gt;maybe_initiate_send();
            }
            else
            {
                // handle error
            }
        });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you’re using c++ coroutines it becomes a little more complicated as you want the lifetime of the tracking
state to be destroyed after the asynchronous initiation function but before the coroutine continuation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;namespace net = boost::asio;
namespace http = boost::beast::http;

auto connect_and_send(
    boost::asio::ip::tcp::socket&amp;amp; stream, 
    std::string host, 
    std::string port, 
    http::request&amp;lt;http::string_body&amp;gt; req) 
-&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    namespace net = boost::asio;
    
    auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor);

    // suspension point coming up

    auto oresults = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::resolver::results_type&amp;gt;&amp;gt;();
    {
        BOOST_ASIO_HANDLER_LOCATION((
            __FILE__, __LINE__,
            &quot;my_connection_impl::connect_and_send&quot;
        ));
        oresults.emplace(resolver.async_resolve(host, port, net::use_awaitable));
    }
    auto results = co_await std::move(*oresults);

    auto oconnect = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::endpoint&amp;gt;&amp;gt;();
    {
        BOOST_ASIO_HANDLER_LOCATION((
            __FILE__, __LINE__,
            &quot;my_connection_impl::connect_and_send&quot;
        ));
        oconnect.emplace(net::async_connect(stream, results, net::use_awaitable));
    }
    auto ep = co_await *std::move(oconnect);

    // ... and so on ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which might look a little unwieldy compared to the unannotated code, which could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;auto connect_and_send(
    boost::asio::ip::tcp::socket&amp;amp; stream, 
    std::string host, 
    std::string port, 
    http::request&amp;lt;http::string_body&amp;gt; req) 
-&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    namespace net = boost::asio;
    
    auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor);

    auto ep = co_await net::async_connect(stream, 
                            co_await resolver.async_resolve(host, port, net::use_awaitable), 
                            net::use_awaitable);

    // ... and so on ...

}
&lt;/code&gt;&lt;/pre&gt;</content><author><name></name></author><category term="richard" /><summary type="html">New Debugging Feature in Asio and Beast As covered previously, Boost 1.74 brought an implementation of the new unified executors model to Boost.Asio. Support for this is not the only thing that is new in Beast. Chris Kohlhoff recently submitted a PR to Beast’s repository demonstrating how to annotate source code with the BOOST_ASIO_HANDLER_LOCATION macro. I have since followed up and annotated all asynchronous operations in Beast this way. In a normal build, there is no effect (and zero extra code generation). However, defining the preprocessor macro BOOST_ASIO_ENABLE_HANDLER_TRACKING will cause these macros to generate code which will emit handler tracking log data to stdout in a very specific format. The output is designed to describe the flow of asynchronous events in a format suitable for generating a visualisation in linear terms. i.e. the asynchronous events are flattened and linked to show causality. Here is an example of the output: @asio|1597543084.233257|&amp;gt;33| @asio|1597543084.233273|33|deadline_timer@0x7fa6cac25218.cancel @asio|1597543084.233681|33^34|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321) @asio|1597543084.233681|33^34|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331) @asio|1597543084.233681|33^34|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201) @asio|1597543084.233681|33^34|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64) @asio|1597543084.233681|33^34|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223) @asio|1597543084.233681|33^34|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277) @asio|1597543084.233681|33*34|deadline_timer@0x7fa6cac25298.async_wait @asio|1597543084.233801|33^35|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373) @asio|1597543084.233801|33^35|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331) @asio|1597543084.233801|33^35|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201) @asio|1597543084.233801|33^35|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64) @asio|1597543084.233801|33^35|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223) @asio|1597543084.233801|33^35|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277) @asio|1597543084.233801|33*35|socket@0x7fa6cac251c8.async_send @asio|1597543084.233910|.35|non_blocking_send,ec=system:0,bytes_transferred=103 @asio|1597543084.233949|&amp;lt;33| @asio|1597543084.233983|&amp;lt;31| @asio|1597543084.234031|&amp;gt;30|ec=system:89 @asio|1597543084.234045|30*36|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234054|&amp;gt;36| @asio|1597543084.234064|&amp;lt;36| @asio|1597543084.234072|&amp;lt;30| @asio|1597543084.234086|&amp;gt;35|ec=system:0,bytes_transferred=103 @asio|1597543084.234100|35*37|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234109|&amp;gt;37| @asio|1597543084.234119|37|deadline_timer@0x7fa6cac25298.cancel @asio|1597543084.234198|37^38|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321) @asio|1597543084.234198|37^38|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168) @asio|1597543084.234198|37^38|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212) @asio|1597543084.234198|37^38|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297) @asio|1597543084.234198|37^38|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101) @asio|1597543084.234198|37*38|deadline_timer@0x7fa6cac25218.async_wait @asio|1597543084.234288|37^39|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373) @asio|1597543084.234288|37^39|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168) @asio|1597543084.234288|37^39|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212) @asio|1597543084.234288|37^39|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297) @asio|1597543084.234288|37^39|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101) @asio|1597543084.234288|37*39|socket@0x7fa6cac251c8.async_receive @asio|1597543084.234334|.39|non_blocking_recv,ec=system:35,bytes_transferred=0 @asio|1597543084.234353|&amp;lt;37| @asio|1597543084.234364|&amp;lt;35| @asio|1597543084.234380|&amp;gt;34|ec=system:89 @asio|1597543084.234392|34*40|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234401|&amp;gt;40| @asio|1597543084.234408|&amp;lt;40| @asio|1597543084.234416|&amp;lt;34| @asio|1597543084.427594|.39|non_blocking_recv,ec=system:0,bytes_transferred=534 @asio|1597543084.427680|&amp;gt;39|ec=system:0,bytes_transferred=534 So far, so good. But not very informative or friendly to the native eye. Fortunately as of Boost 1.74 there is a tool in the Asio source tree to convert this data into something consumable by the open source tool dot, which can then output the resulting execution graph in one of a number of common graphical formats such as PNG, BMP, SVG and many others. Here is an example of a visualisation of a simple execution graph: The tool you need to do this is in the asio subproject of the Boost repo. The full path is libs/asio/tools/handlerviz.pl. The command is self-documenting but for clarity, the process would be like this: Compile and link your program with the compiler flag -DBOOST_ASIO_ENABLE_HANDLER_TRACKING run your program, capturing stdout to a file (say mylog.txt) (or you can pipe it to the next step) handlerviz.pl &amp;lt; mylog.txt | dot -Tpng mygraph.png You should now be able to view your graph in a web browser, editor or picture viewer. The documentation for dot is here dot is usually available in the graphviz package of your linux distro/brew cask. Windows users can download an executable suite here. If you have written your own asynchronous operations to compliment Beast or Asio, or indeed you just wish you add your handler locations to the graph output, you can do so by inserting the BOOST_ASIO_HANDLER_LOCATION macro just before each asynchronous suspension point (i.e. just before the call to async_xxx). If you’re doing this in an Asio coroutine (not to be confused with C++ coroutines) then be sure to place the macro in curly braces after the YIELD macro, for example: ... // this marks a suspension point of the coroutine BOOST_ASIO_CORO_YIELD { // This macro creates scoped variables so must be in a private scope BOOST_ASIO_HANDLER_LOCATION(( // note: double open brackets __FILE__, __LINE__, // source location &quot;websocket::tcp::async_teardown&quot; // name of the initiating function )); // this is the initiation of the next inner asynchronous operation s_.async_wait( net::socket_base::wait_read, beast::detail::bind_continuation(std::move(*this))); // there is an implied return statement here } ... When writing applications, people historically have used Continuation Passing Style when calling asynchronous operations, capturing a shared_ptr to the connection implementation in each handler (continuation). When using this macro in user code with written in continuation passing style, you might do so like this: void send_request(http::request&amp;lt;http::string_body&amp;gt; req) { send_queue_.push_back(std::move(req)); if (!sending_) { sending_ = true; maybe_initiate_send(); } } void my_connection_impl::maybe_initiate_send() { if (send_queue_.empty()) { sending_ = false; return; } // assume request_queue_ is a std::deque so elements will have stable addresses auto&amp;amp; current_request = request_queue_.front(); BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::maybe_initiate_send&quot; )); // suspension point boost::beast::http::async_write(stream_, current_request_, [self = this-&amp;gt;shared_from_this()](boost::beast::error_code ec, std::size_t) { // continuation if (!ec) { self-&amp;gt;request_queue_.pop_front(); self-&amp;gt;maybe_initiate_send(); } else { // handle error } }); } If you’re using c++ coroutines it becomes a little more complicated as you want the lifetime of the tracking state to be destroyed after the asynchronous initiation function but before the coroutine continuation: namespace net = boost::asio; namespace http = boost::beast::http; auto connect_and_send( boost::asio::ip::tcp::socket&amp;amp; stream, std::string host, std::string port, http::request&amp;lt;http::string_body&amp;gt; req) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { namespace net = boost::asio; auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor); // suspension point coming up auto oresults = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::resolver::results_type&amp;gt;&amp;gt;(); { BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::connect_and_send&quot; )); oresults.emplace(resolver.async_resolve(host, port, net::use_awaitable)); } auto results = co_await std::move(*oresults); auto oconnect = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::endpoint&amp;gt;&amp;gt;(); { BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::connect_and_send&quot; )); oconnect.emplace(net::async_connect(stream, results, net::use_awaitable)); } auto ep = co_await *std::move(oconnect); // ... and so on ... } Which might look a little unwieldy compared to the unannotated code, which could look like this: auto connect_and_send( boost::asio::ip::tcp::socket&amp;amp; stream, std::string host, std::string port, http::request&amp;lt;http::string_body&amp;gt; req) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { namespace net = boost::asio; auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor); auto ep = co_await net::async_connect(stream, co_await resolver.async_resolve(host, port, net::use_awaitable), net::use_awaitable); // ... and so on ... }</summary></entry><entry><title type="html">Krystian’s July Update</title><link href="http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate.html" rel="alternate" type="text/html" title="Krystian’s July Update" /><published>2020-08-01T00:00:00+00:00</published><updated>2020-08-01T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate.html">&lt;h1 id=&quot;what-ive-been-doing&quot;&gt;What I’ve been doing&lt;/h1&gt;

&lt;p&gt;I’ve been spending a &lt;em&gt;lot&lt;/em&gt; of time working on optimizing the parser; perhaps a bit too much. Nevertheless, it’s very enjoyable and in doing so I’ve learned more than I could hope to ever learn in school. In addition to the optimization, comment and trailing comma support finally got merged, and I implemented UTF-8 validation (enabled by default, but it can be disabled).&lt;/p&gt;

&lt;h2 id=&quot;utf-8-validation&quot;&gt;UTF-8 validation&lt;/h2&gt;

&lt;p&gt;Prior to implementing this extension (or rather, feature which can be disabled), the parser considers any character appearing within a string to be valid, so long as it wasn’t a control character or formed an illegal escape. While this is &lt;em&gt;fast&lt;/em&gt;, it technically does not conform to the JSON standard.&lt;/p&gt;

&lt;p&gt;As per Section 2 of the &lt;a href=&quot;http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf&quot;&gt;JSON Data Interchange Syntax Standard&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A conforming JSON text is a sequence of Unicode code points that strictly conforms to the JSON grammar defined by this specification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As with most standardese, this particular requirement for conformance is not outright stated, but rather implied. Anyways, that’s enough standardese talk for this post.&lt;/p&gt;

&lt;p&gt;After working on this parser so much, I’ve pretty much got the suspend/resume idiom we use nailed down, so integrating it with the string parsing function was trivial… the actual validation, not so much. I hadn’t the slightest clue about any of the terminology used in the Unicode standard, so it took a good couple of hours to point myself in the right direction. Anyways, a lot of Googling and a messy python script for generating valid and invalid byte sequences later, I had something functional.&lt;/p&gt;

&lt;p&gt;Then came my favorite part: optimization.&lt;/p&gt;

&lt;p&gt;The first byte within a UTF-8 byte sequence determines how many bytes will follow, as well as the valid ranges for these following bytes. Since this byte has such a large valid range, I settled on using a lookup table to check whether the first byte is valid.&lt;/p&gt;

&lt;p&gt;Luckily, the following bytes have ranges that can be trivially checked using a mask. For example, if the first byte is &lt;code&gt;0xE1&lt;/code&gt;, then the byte sequence will be composed of three bytes, the latter two having a valid range of &lt;code&gt;0x80&lt;/code&gt; to &lt;code&gt;0xBF&lt;/code&gt;. Thus, our fast-path routine to verify this sequence can be written as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;uint32_t v;
// this is reversed on big-endian
std::memcpy(&amp;amp;v, bytes, 4); // 4 bytes load

switch (lookup_table[v &amp;amp; 0x7F]) // mask out the most significant bit
{
...
case 3:
	if ((v &amp;amp; 0x00C0C000) == 0x00808000)
		return result::ok;
	return result::fail;
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works well for all but one byte sequence combination. For whatever reason, UTF-8 byte sequences that start with &lt;code&gt;0xF0&lt;/code&gt; can have a second byte between &lt;code&gt;0x90&lt;/code&gt; and &lt;code&gt;0xBF&lt;/code&gt; which requires the check to be done as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;(v &amp;amp; 0xC0C0FF00) + 0x7F7F7000 &amp;lt;= 0x00002F00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It’s a weird little outlier that I spent way too much time trying to figure out.&lt;/p&gt;

&lt;p&gt;Since our parser supports incremental parsing, we only take the fast path if the input stream has four or more bytes remaining. If this condition isn’t met, we have to check each byte individually. It’s slower, but shouldn’t happen often.&lt;/p&gt;

&lt;h2 id=&quot;other-optimizations&quot;&gt;Other optimizations&lt;/h2&gt;

&lt;p&gt;I’ve been trying out a number of different optimizations to squeeze all the performance we can get out of the parser. Most recently, I rewrote the parser functions to take a &lt;code&gt;const char*&lt;/code&gt; parameter indicating the start of the value, and return a pointer to the end of the value (if parsing succeeds) or &lt;code&gt;nullptr&lt;/code&gt; upon failure or partial parsing.&lt;/p&gt;

&lt;p&gt;Since I’m not great at explaining things, here’s the before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;result parse_array(const_stream&amp;amp;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and here’s the after:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;const char* parse_array(const char*);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This allows us to keep the pointer to the current position in the stream entirely within the registers when parsing a document. Since the value is local to the function, the compiler no longer needs to write it to the &lt;code&gt;const_stream&lt;/code&gt; object at the top of the call stack (created within &lt;code&gt;basic_parser::write_some&lt;/code&gt;), nor read it each time a nested value is parsed. This yields an &lt;em&gt;8%&lt;/em&gt; boost in performance across the board.&lt;/p&gt;

&lt;p&gt;More time was spent optimizing the SSE2 functions used for parsing unescaped strings and whitespace as well. Within &lt;code&gt;count_whitespace&lt;/code&gt;, we were able to get rid of a &lt;code&gt;_mm_cmpeq_epi8&lt;/code&gt; (&lt;code&gt;PCMPEQB&lt;/code&gt;) instruction by performing a bitwise or with 4 after testing for spaces, and then comparing the result with &lt;code&gt;'\r'&lt;/code&gt;, as the ASCII value of tab (&lt;code&gt;'\t'&lt;/code&gt;) only differs from that of the carriage return by the third least significant bit. This was something that clang was doing for us, but it’s nice to implement it for all other compilers.&lt;/p&gt;

&lt;p&gt;For &lt;code&gt;count_unescaped&lt;/code&gt; (used to parse unescaped strings), we were able to again reduce the length of the hot path, this time a bit more significantly. Instead of checking for control characters by means of relational comparison, we can instead check for quotes and backslash first, and once that’s done, the &lt;code&gt;_mm_min_epu8&lt;/code&gt; (&lt;code&gt;PMINUB&lt;/code&gt;) instruction can be used to set all control characters (0 - 31) to 31, and then test for equality. This brought our performance on the &lt;code&gt;strings.json&lt;/code&gt; benchmark past the 8 GB/s mark from around 7.7 GB/s. Combined with the optimization of how the stream pointer is passed around, we now hit just a hair under 8.5 GB/s on this benchmark.&lt;/p&gt;

&lt;h2 id=&quot;the-important-but-boring-stuff&quot;&gt;The important but boring stuff&lt;/h2&gt;

&lt;p&gt;After merging the parser extensions, there was a bunch of housekeeping to do such as improving coverage and writing documentation. Though these are far from being my favorite tasks, they are integral to writing a good library, so it must be done. My initial approach to writing tests for the parser extensions was to run each test on every parser configuration we have, but this soon proved to be a nonoptimal approach when the time taken to run the test suite quadrupled. I ended up doing the right thing by making the tests more surgical in nature, and in doing so we even got 100% coverage on the parser.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">What I’ve been doing I’ve been spending a lot of time working on optimizing the parser; perhaps a bit too much. Nevertheless, it’s very enjoyable and in doing so I’ve learned more than I could hope to ever learn in school. In addition to the optimization, comment and trailing comma support finally got merged, and I implemented UTF-8 validation (enabled by default, but it can be disabled). UTF-8 validation Prior to implementing this extension (or rather, feature which can be disabled), the parser considers any character appearing within a string to be valid, so long as it wasn’t a control character or formed an illegal escape. While this is fast, it technically does not conform to the JSON standard. As per Section 2 of the JSON Data Interchange Syntax Standard: A conforming JSON text is a sequence of Unicode code points that strictly conforms to the JSON grammar defined by this specification. As with most standardese, this particular requirement for conformance is not outright stated, but rather implied. Anyways, that’s enough standardese talk for this post. After working on this parser so much, I’ve pretty much got the suspend/resume idiom we use nailed down, so integrating it with the string parsing function was trivial… the actual validation, not so much. I hadn’t the slightest clue about any of the terminology used in the Unicode standard, so it took a good couple of hours to point myself in the right direction. Anyways, a lot of Googling and a messy python script for generating valid and invalid byte sequences later, I had something functional. Then came my favorite part: optimization. The first byte within a UTF-8 byte sequence determines how many bytes will follow, as well as the valid ranges for these following bytes. Since this byte has such a large valid range, I settled on using a lookup table to check whether the first byte is valid. Luckily, the following bytes have ranges that can be trivially checked using a mask. For example, if the first byte is 0xE1, then the byte sequence will be composed of three bytes, the latter two having a valid range of 0x80 to 0xBF. Thus, our fast-path routine to verify this sequence can be written as: uint32_t v; // this is reversed on big-endian std::memcpy(&amp;amp;v, bytes, 4); // 4 bytes load switch (lookup_table[v &amp;amp; 0x7F]) // mask out the most significant bit { ... case 3: if ((v &amp;amp; 0x00C0C000) == 0x00808000) return result::ok; return result::fail; ... } This works well for all but one byte sequence combination. For whatever reason, UTF-8 byte sequences that start with 0xF0 can have a second byte between 0x90 and 0xBF which requires the check to be done as: (v &amp;amp; 0xC0C0FF00) + 0x7F7F7000 &amp;lt;= 0x00002F00 It’s a weird little outlier that I spent way too much time trying to figure out. Since our parser supports incremental parsing, we only take the fast path if the input stream has four or more bytes remaining. If this condition isn’t met, we have to check each byte individually. It’s slower, but shouldn’t happen often. Other optimizations I’ve been trying out a number of different optimizations to squeeze all the performance we can get out of the parser. Most recently, I rewrote the parser functions to take a const char* parameter indicating the start of the value, and return a pointer to the end of the value (if parsing succeeds) or nullptr upon failure or partial parsing. Since I’m not great at explaining things, here’s the before: result parse_array(const_stream&amp;amp;); and here’s the after: const char* parse_array(const char*); This allows us to keep the pointer to the current position in the stream entirely within the registers when parsing a document. Since the value is local to the function, the compiler no longer needs to write it to the const_stream object at the top of the call stack (created within basic_parser::write_some), nor read it each time a nested value is parsed. This yields an 8% boost in performance across the board. More time was spent optimizing the SSE2 functions used for parsing unescaped strings and whitespace as well. Within count_whitespace, we were able to get rid of a _mm_cmpeq_epi8 (PCMPEQB) instruction by performing a bitwise or with 4 after testing for spaces, and then comparing the result with '\r', as the ASCII value of tab ('\t') only differs from that of the carriage return by the third least significant bit. This was something that clang was doing for us, but it’s nice to implement it for all other compilers. For count_unescaped (used to parse unescaped strings), we were able to again reduce the length of the hot path, this time a bit more significantly. Instead of checking for control characters by means of relational comparison, we can instead check for quotes and backslash first, and once that’s done, the _mm_min_epu8 (PMINUB) instruction can be used to set all control characters (0 - 31) to 31, and then test for equality. This brought our performance on the strings.json benchmark past the 8 GB/s mark from around 7.7 GB/s. Combined with the optimization of how the stream pointer is passed around, we now hit just a hair under 8.5 GB/s on this benchmark. The important but boring stuff After merging the parser extensions, there was a bunch of housekeeping to do such as improving coverage and writing documentation. Though these are far from being my favorite tasks, they are integral to writing a good library, so it must be done. My initial approach to writing tests for the parser extensions was to run each test on every parser configuration we have, but this soon proved to be a nonoptimal approach when the time taken to run the test suite quadrupled. I ended up doing the right thing by making the tests more surgical in nature, and in doing so we even got 100% coverage on the parser.</summary></entry><entry><title type="html">Richard’s July Update</title><link href="http://cppalliance.org/richard/2020/08/01/RichardsJulyUpdate.html" rel="alternate" type="text/html" title="Richard’s July Update" /><published>2020-08-01T00:00:00+00:00</published><updated>2020-08-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/08/01/RichardsJulyUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/08/01/RichardsJulyUpdate.html">&lt;h1 id=&quot;boost-174---progress-update&quot;&gt;Boost 1.74 - Progress Update&lt;/h1&gt;

&lt;p&gt;Boost 1.74 beta release has been published and the various maintainers are applying last-minute bug fixes to their 
libraries in readiness for the final release on  12th August.&lt;/p&gt;

&lt;p&gt;For us in the Beast team, a fair amount of attention has been spent monitoring last minutes changes to Asio, as Chris
makes the final tweaks after the Unified Executors update I mentioned in last month’s blog.&lt;/p&gt;

&lt;h2 id=&quot;comprehensive-testing&quot;&gt;Comprehensive Testing&lt;/h2&gt;

&lt;p&gt;Last month I &lt;a href=&quot;https://github.com/boostorg/beast/commit/b84d8ad3d48d173bd78ed6dc2ed8d26d84762af3&quot;&gt;committed&lt;/a&gt; what I hoped 
would be the first of a suite of Dockerfiles which help the mass testing of Beast. The upstream changes to Asio
were a lesson in just how many compilers, hosts and target environments we have to support in order that our user base
is not surprised or impeded as a result of compiler selection or imposition.&lt;/p&gt;

&lt;p&gt;I am not expert is Docker matters. I mean, I can read the manual and follow basic instructions like anyone else,
but I was hoping that someone would come along to help flesh out the suite a little. Particularly for the Windows
builds, since I have no experience in installing software from the command line in Windows, and the greatest respect
for those individuals who have mastered the art.&lt;/p&gt;

&lt;p&gt;Fortunately for me, we’ve had a new addition to the team. Sam Darwin, who has submitted a number of commits which
increase Docker coverage. Of these I was most pleased to see the submission of the 
&lt;a href=&quot;https://github.com/boostorg/beast/commit/3486e9cb18aa39b392e07031a33e65b1792fbccf&quot;&gt;Windows&lt;/a&gt; build matrix which has 
been of enormous value. I think it would be fair to say that Microsoft Visual Studio is nothing short of notorious
for its subtle deviations from the standard. As if it were not difficult enough already to create useful and coherent 
template libraries, supporting (particularly older) versions of MSVC requires extra care and workarounds.&lt;/p&gt;

&lt;p&gt;Hopefully, now that two-phase lookup has been firmly 
&lt;a href=&quot;https://devblogs.microsoft.com/cppblog/two-phase-name-lookup-support-comes-to-msvc/&quot;&gt;adopted&lt;/a&gt; by Microsoft (some two 
decades after its standardisation), this kind of issue will become less of a concern as time moves forward and support 
for older compilers is gradually dropped.&lt;/p&gt;

&lt;p&gt;To be fair to Microsoft, if my memory serves, they were pioneers of bringing the C++ language to the masses back in the 
days of Visual Studio 97 and prior to that, the separate product Visual C++ (which we used to have to pay for!).&lt;/p&gt;

&lt;p&gt;In hindsight a number of errors were made in terms of implementation that had lasting effects on a generation of 
developers and their projects. But arguably, had Microsoft not championed this effort, it is likely that C++ may not
have achieved the penetration and exposure that it did.&lt;/p&gt;

&lt;h1 id=&quot;a-bug-in-asio-resolver-surely-not&quot;&gt;A Bug In Asio Resolver? Surely Not?&lt;/h1&gt;

&lt;p&gt;One of the examples in the Beast repository is a simple 
&lt;a href=&quot;https://github.com/boostorg/beast/tree/develop/example/http/client/crawl&quot;&gt;web crawler&lt;/a&gt;. If you have taken sufficient
interest to read the code, you will have noticed that it follows the model of “multiple threads, &lt;code&gt;one io_context&lt;/code&gt; per 
thread.”&lt;/p&gt;

&lt;p&gt;This may seem an odd decision, since a web crawler spends most of its time idle waiting for asynchronous IO to complete.
However, there is an unfortunate implementation detail in the *nix version of &lt;code&gt;asio::ip::tcp::resolver&lt;/code&gt; which is due to
a limitation of the &lt;a href=&quot;https://linux.die.net/man/3/getaddrinfo&quot;&gt;&lt;code&gt;getaddrinfo&lt;/code&gt;&lt;/a&gt; API upon which it depends.&lt;/p&gt;

&lt;p&gt;For background, &lt;code&gt;getaddrinfo&lt;/code&gt; is a thread-safe, blocking call. This means that Asio has to spawn a background thread
in order to perform the actual name resolution as part of the implementation of &lt;code&gt;async_resolve&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So with that out of the way, why am I writing this?&lt;/p&gt;

&lt;p&gt;One of the bugs I tackled this month was that this demo, when run with multiple (say 500) threads, can be reliably made 
to hang indefinitely on my Fedora 32 system. At first, I assumed that either we or Asio had introduced a race condition.
However, after digging into the lockup it turned out to almost always lock up while resolving the FQDN &lt;code&gt;secureupload.eu&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Investigating further, it turns out that the nameserver response for this FQDN is too long to fit into a UDP packet.
This means that the DNS client on the Linux host is forced to revert to a TCP connection in order to receive the entire
record. This can be evidenced by using &lt;code&gt;nslookup&lt;/code&gt; on the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nslookup secureupload.eu
Server: 192.168.0.1                    &amp;lt;&amp;lt;-- address of my local nameserver
Address: 192.168.0.1#53

Non-authoritative answer:
Name: secureupload.eu
Address: 45.87.161.67

... many others ...

Name: secureupload.eu
Address: 45.76.235.58
;; Truncated, retrying in TCP mode.    &amp;lt;&amp;lt;-- indication that nslookup is switching to TCP
Name: secureupload.eu
Address: 2a04:5b82:3:209::2

... many others

Name: secureupload.eu
Address: 2a04:5b82:3:203::2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Furthermore, whenever I checked the call stack, the thread in question was always stuck in the glibc function 
&lt;a href=&quot;https://code.woboq.org/userspace/glibc/resolv/res_send.c.html#send_vc&quot;&gt;&lt;code&gt;send_vc()&lt;/code&gt;&lt;/a&gt;, which is called by &lt;code&gt;getaddrinfo&lt;/code&gt; 
in response to the condition of a truncated UDP response.&lt;/p&gt;

&lt;p&gt;So despite my initial assumption that there must be a race in user code, the evidence was starting to point to something
interesting about this particular FQDN. Now I’ve been writing software for over three decades on and off and I’ve seen
a lot of bugs in code that the authors were adamant they they had not put there. We are as a rule, our own worst 
critics. So I was reluctant to believe that there could be a data-driven bug in glibc.&lt;/p&gt;

&lt;p&gt;Nevertheless, a scan of the redhat bug tracker by Chris turned up 
&lt;a href=&quot;https://bugzilla.redhat.com/show_bug.cgi?id=1429442&quot;&gt;this little nugget&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It turns out that what was happening was that the TCP connection to the upstream name server had gone quiet or had 
dropped packets - presumably courtesy my cheap Huawei Home Gateway router which was being swamped by 500 simultaneous
 requests by the 500 threads I had assigned to the crawler. Because the glibc implementation does not implement
a timeout on the request, the failed reception of a response by the nameserver caused the call to &lt;code&gt;gethostinfo&lt;/code&gt; to hang
whenever this FQDN was being resolved.&lt;/p&gt;

&lt;p&gt;So it turns out that there is indeed a bug in glibc, which can affect any *nix (or cygwin) program that performs DNS
address resolution when the requested domain’s response is too long to fit in a UDP response message.&lt;/p&gt;

&lt;p&gt;Until this bug is fixed, I have learned a lesson and you have been warned.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Boost 1.74 - Progress Update Boost 1.74 beta release has been published and the various maintainers are applying last-minute bug fixes to their libraries in readiness for the final release on 12th August. For us in the Beast team, a fair amount of attention has been spent monitoring last minutes changes to Asio, as Chris makes the final tweaks after the Unified Executors update I mentioned in last month’s blog. Comprehensive Testing Last month I committed what I hoped would be the first of a suite of Dockerfiles which help the mass testing of Beast. The upstream changes to Asio were a lesson in just how many compilers, hosts and target environments we have to support in order that our user base is not surprised or impeded as a result of compiler selection or imposition. I am not expert is Docker matters. I mean, I can read the manual and follow basic instructions like anyone else, but I was hoping that someone would come along to help flesh out the suite a little. Particularly for the Windows builds, since I have no experience in installing software from the command line in Windows, and the greatest respect for those individuals who have mastered the art. Fortunately for me, we’ve had a new addition to the team. Sam Darwin, who has submitted a number of commits which increase Docker coverage. Of these I was most pleased to see the submission of the Windows build matrix which has been of enormous value. I think it would be fair to say that Microsoft Visual Studio is nothing short of notorious for its subtle deviations from the standard. As if it were not difficult enough already to create useful and coherent template libraries, supporting (particularly older) versions of MSVC requires extra care and workarounds. Hopefully, now that two-phase lookup has been firmly adopted by Microsoft (some two decades after its standardisation), this kind of issue will become less of a concern as time moves forward and support for older compilers is gradually dropped. To be fair to Microsoft, if my memory serves, they were pioneers of bringing the C++ language to the masses back in the days of Visual Studio 97 and prior to that, the separate product Visual C++ (which we used to have to pay for!). In hindsight a number of errors were made in terms of implementation that had lasting effects on a generation of developers and their projects. But arguably, had Microsoft not championed this effort, it is likely that C++ may not have achieved the penetration and exposure that it did. A Bug In Asio Resolver? Surely Not? One of the examples in the Beast repository is a simple web crawler. If you have taken sufficient interest to read the code, you will have noticed that it follows the model of “multiple threads, one io_context per thread.” This may seem an odd decision, since a web crawler spends most of its time idle waiting for asynchronous IO to complete. However, there is an unfortunate implementation detail in the *nix version of asio::ip::tcp::resolver which is due to a limitation of the getaddrinfo API upon which it depends. For background, getaddrinfo is a thread-safe, blocking call. This means that Asio has to spawn a background thread in order to perform the actual name resolution as part of the implementation of async_resolve. So with that out of the way, why am I writing this? One of the bugs I tackled this month was that this demo, when run with multiple (say 500) threads, can be reliably made to hang indefinitely on my Fedora 32 system. At first, I assumed that either we or Asio had introduced a race condition. However, after digging into the lockup it turned out to almost always lock up while resolving the FQDN secureupload.eu. Investigating further, it turns out that the nameserver response for this FQDN is too long to fit into a UDP packet. This means that the DNS client on the Linux host is forced to revert to a TCP connection in order to receive the entire record. This can be evidenced by using nslookup on the command line: $ nslookup secureupload.eu Server: 192.168.0.1 &amp;lt;&amp;lt;-- address of my local nameserver Address: 192.168.0.1#53 Non-authoritative answer: Name: secureupload.eu Address: 45.87.161.67 ... many others ... Name: secureupload.eu Address: 45.76.235.58 ;; Truncated, retrying in TCP mode. &amp;lt;&amp;lt;-- indication that nslookup is switching to TCP Name: secureupload.eu Address: 2a04:5b82:3:209::2 ... many others Name: secureupload.eu Address: 2a04:5b82:3:203::2 Furthermore, whenever I checked the call stack, the thread in question was always stuck in the glibc function send_vc(), which is called by getaddrinfo in response to the condition of a truncated UDP response. So despite my initial assumption that there must be a race in user code, the evidence was starting to point to something interesting about this particular FQDN. Now I’ve been writing software for over three decades on and off and I’ve seen a lot of bugs in code that the authors were adamant they they had not put there. We are as a rule, our own worst critics. So I was reluctant to believe that there could be a data-driven bug in glibc. Nevertheless, a scan of the redhat bug tracker by Chris turned up this little nugget. It turns out that what was happening was that the TCP connection to the upstream name server had gone quiet or had dropped packets - presumably courtesy my cheap Huawei Home Gateway router which was being swamped by 500 simultaneous requests by the 500 threads I had assigned to the crawler. Because the glibc implementation does not implement a timeout on the request, the failed reception of a response by the nameserver caused the call to gethostinfo to hang whenever this FQDN was being resolved. So it turns out that there is indeed a bug in glibc, which can affect any *nix (or cygwin) program that performs DNS address resolution when the requested domain’s response is too long to fit in a UDP response message. Until this bug is fixed, I have learned a lesson and you have been warned.</summary></entry><entry><title type="html">Richard’s May/June Update</title><link href="http://cppalliance.org/richard/2020/07/01/RichardsJuneUpdate.html" rel="alternate" type="text/html" title="Richard’s May/June Update" /><published>2020-07-01T00:00:00+00:00</published><updated>2020-07-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/07/01/RichardsJuneUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/07/01/RichardsJuneUpdate.html">&lt;h1 id=&quot;boost-174---interesting-developments-in-asio&quot;&gt;Boost 1.74 - Interesting Developments in Asio&lt;/h1&gt;

&lt;p&gt;We’re currently beta-testing Boost 1.74, the lead-up to which has seen a flurry of activity in Asio, which has
impacted Beast.&lt;/p&gt;

&lt;p&gt;Recent versions of Asio have moved away from the idea of sequencing completion handlers directly on an &lt;code&gt;io_context&lt;/code&gt;
(which used to be called an &lt;code&gt;io_service&lt;/code&gt;) towards the execution of completion handlers by an Executor.&lt;/p&gt;

&lt;p&gt;The basic idea being that the executor is a lightweight handle to some execution context, which did what the &lt;code&gt;io_context&lt;/code&gt;
always used to do - schedule the execution of completion handlers.&lt;/p&gt;

&lt;p&gt;The changes to Asio have been tracking 
&lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/n4771.pdf&quot;&gt;The Networking TS&lt;/a&gt; which describes a concept
of Executor relevant to asynchronous IO.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0443r11.html&quot;&gt;Unified Executors&lt;/a&gt; proposal unifies the 
concepts of io execution and the general concept of “a place to execute work” - a somewhat more generic idea than merely
an IO loop or thread pool. Work has been ongoing by the members of WG21 to produce an execution model that 
serves all parties’ needs.&lt;/p&gt;

&lt;p&gt;Courtesy of an incredible effort by Chris Kohlhoff, Latest Asio and Boost.Asio 1.74 has been updated to accommodate both 
models of executors, with the Unified Executors model being the default. It’s important to note that most users won’t 
notice the change in API this time around since by default the Asio in 1.74 also includes the 1.73 interface.&lt;/p&gt;

&lt;p&gt;There are a number of preprocessor macros that can be defined to change this default behaviour:&lt;/p&gt;

&lt;h2 id=&quot;boost_asio_no_ts_executors&quot;&gt;&lt;code&gt;BOOST_ASIO_NO_TS_EXECUTORS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Defining this macro disables the Networking TS executor model. The most immediate thing you’ll notice if you define this
macro is that for some executor &lt;code&gt;e&lt;/code&gt;, the expression &lt;code&gt;e.context()&lt;/code&gt; becomes invalid.&lt;/p&gt;

&lt;p&gt;In the Unified Executors world, this operation is expressed as a query against the executor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;auto&amp;amp; ctx = asio::query(e, asio::execution::context);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea being that the execution context is a &lt;em&gt;property&lt;/em&gt; of an executor that can be &lt;em&gt;queried&lt;/em&gt; for.&lt;/p&gt;

&lt;p&gt;Another change which users are likely to notice when this macro is defined is that the &lt;code&gt;asio::executor_work_guard&amp;lt;&amp;gt;&lt;/code&gt; 
template corresponding &lt;code&gt;asio::make_work_guard&lt;/code&gt; function is no longer defined.&lt;/p&gt;

&lt;p&gt;You may well ask then, how we would prevent an underlying execution context from running out of work?&lt;/p&gt;

&lt;p&gt;In the Unified Executors world, we can think of Executors as an unbounded set of types with various properties
enabled or disabled. The idea is that the state of the properties define the behaviour of the interaction between the 
executor and its underlying context.&lt;/p&gt;

&lt;p&gt;In the new world, we don’t explicitly create a work guard which references the executor. We ‘simply’ create a new
executor which happens to have the property of ‘tracking work’ (i.e. this executor will in some way ensure that the 
underlying context has outstanding work until the executor’s lifetime ends).&lt;/p&gt;

&lt;p&gt;Again, given that &lt;code&gt;e&lt;/code&gt; is some executor, here’s how we spell this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;auto tracked = asio::require(e, asio::execution::outstanding_work.tracked);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After executing this statement, there are now two executors in play. The first, &lt;code&gt;e&lt;/code&gt; may or may not be “tracking work”
(ensuring that the underlying context does not stop), but &lt;code&gt;tracked&lt;/code&gt; certainly is.&lt;/p&gt;

&lt;p&gt;There is another way to spell this, more useful in a generic programming environment.&lt;/p&gt;

&lt;p&gt;Suppose you were writing generic code and you don’t know the type of the executor presented to you, or even what kind
of execution context it is associated with. However, you do know that &lt;em&gt;if&lt;/em&gt; the underlying context can stop if it runs
out of work, then we want to prevent it from doing so for the duration of some operation.&lt;/p&gt;

&lt;p&gt;In this case, we can’t use &lt;code&gt;require&lt;/code&gt; because this will fail to compile if the given executor does not support the 
&lt;code&gt;outstanding_work::tracked&lt;/code&gt; property. Therefore we would request (or more correctly, &lt;em&gt;prefer&lt;/em&gt;) the capability rather 
than require it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;auto maybe_tracked = asio::prefer(e, asio::execution::outstanding_work.tracked);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now use &lt;code&gt;maybe_tracked&lt;/code&gt; as the executor for our operation, and it will “do the right thing” regarding the tracking
of work whatever the underlying type of execution context. It is important to note that it &lt;em&gt;is&lt;/em&gt; an executor, not merely
a guard object that contains an executor.&lt;/p&gt;

&lt;h3 id=&quot;post-dispatch-and-defer&quot;&gt;post, dispatch and defer&lt;/h3&gt;

&lt;p&gt;Another notable change in the Asio API when this macro is defined is that models of the Executor concept lose their
&lt;code&gt;post&lt;/code&gt;, &lt;code&gt;dispatch&lt;/code&gt; and &lt;code&gt;defer&lt;/code&gt; member functions.&lt;/p&gt;

&lt;p&gt;The free function versions still remain, so if you have code like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;e.dispatch([]{ /* something */ });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you will need to rewrite it as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;asio::dispatch(e, []{ /* something */ });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or you can be more creative with the underlying property system:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;asio::execution::execute(
    asio::prefer(
        e, 
        asio::execution::blocking.possibly), 
    []{ /* something */ });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which is more-or-less what the implementation of &lt;code&gt;dispatch&lt;/code&gt; does under the covers. It’s actually a little more involved
than that since the completion token’s associated allocator has to be taken into account. There is a 
property for that too: &lt;code&gt;asio::execution::allocator&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In summary, all previous Asio and Networking TS execution/completion scenarios are now handled by executing a handler
in some executor supporting a set of relevant properties.&lt;/p&gt;

&lt;h2 id=&quot;boost_asio_no_deprecated&quot;&gt;BOOST_ASIO_NO_DEPRECATED&lt;/h2&gt;

&lt;p&gt;Defining this macro will ensure that old asio-style invocation and allocation completion handler customisation
functions will no longer be used. The newer paradigm is to explicitly query or require execution properties at the 
time of scheduling a completion handler for invocation. If you don’t know what any of that means, you’d be in the 
majority and don’t need to worry about it.&lt;/p&gt;

&lt;h2 id=&quot;boost_asio_use_ts_executor_as_default&quot;&gt;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&lt;/h2&gt;

&lt;p&gt;As of Boost 1.74, Asio IO objects will be associated with the new &lt;code&gt;asio::any_io_executor&lt;/code&gt; rather than the previous
polymorphic &lt;code&gt;asio::executor&lt;/code&gt;. Defining this macro, undoes this change. It may be useful to you if you have written code
that depends on the use of &lt;code&gt;asio::executor&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;other-observations&quot;&gt;Other observations&lt;/h2&gt;

&lt;h3 id=&quot;strands-are-still-a-thing&quot;&gt;Strands are Still a Thing&lt;/h3&gt;

&lt;p&gt;Asio &lt;code&gt;strand&lt;/code&gt; objects still seem to occupy a twilight zone between executors and something other than executors.&lt;/p&gt;

&lt;p&gt;To be honest, when I first saw the property mechanism, I assumed that a strand would be “just another executor” with 
some “sequential execution” property enabled. This turns out not to be the case. A strand has its own distinct execution
context which manages the sequencing of completion handler invocations within it. The strand keeps a copy of the inner
executor, which is the one where the strand’s completion handlers will be invoked in turn.&lt;/p&gt;

&lt;p&gt;However, a strand models the Executor concept, so it also &lt;em&gt;is an&lt;/em&gt; executor.&lt;/p&gt;

&lt;h3 id=&quot;execute-looks-set-to-become-the-new-call&quot;&gt;execute() looks set to become the new call().&lt;/h3&gt;

&lt;p&gt;Reading the Unified Executors paper is an interesting, exciting or horrifying experience - depending on your view of
what you’d like C++ to be.&lt;/p&gt;

&lt;p&gt;My take from the paper, fleshed out a little with the experience of touching the implementation in Asio, is that in the 
new world, the programming thought process will go something like this,
imagine the following situation:&lt;/p&gt;

&lt;p&gt;“I need to execute this set of tasks,&lt;/p&gt;

&lt;p&gt;Ideally I’d like them to execute in parallel,&lt;/p&gt;

&lt;p&gt;I’d like to wait for them to be done”&lt;/p&gt;

&lt;p&gt;As I understand things, the idea behind unified executors is that I will be able to express these desires and mandates
by executing my work function(s) in some executor yielded by a series of calls to &lt;code&gt;prefer&lt;/code&gt; and &lt;code&gt;require&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;    auto eparallel = prefer(e, bulk_guarantee.unsequenced); // prefer parallel execution
    auto eblock = require(eparallel, blocking.always);      // require blocking
    execute(eblock, task1, task2, task3, task...);          // blocking call which will execute in parallel if possible
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Proponents will no doubt think,&lt;/p&gt;

&lt;p&gt;“Great! Programming by expression of intent”.&lt;/p&gt;

&lt;p&gt;Detractors might say,&lt;/p&gt;

&lt;p&gt;“Ugh! Nondeterministic programs. How do I debug this when it goes wrong?”&lt;/p&gt;

&lt;p&gt;To be honest that this stage, I find myself in both camps. No doubt time will tell.&lt;/p&gt;

&lt;h1 id=&quot;adventures-in-b2-boost-build&quot;&gt;Adventures in B2 (Boost Build)&lt;/h1&gt;

&lt;p&gt;Because of the pressure of testing Beast with the new multi-faceted Asio, I wanted a way to bulk compile and test many
different variants of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compilers&lt;/li&gt;
  &lt;li&gt;Preprocessor macro definitions&lt;/li&gt;
  &lt;li&gt;C++ standards&lt;/li&gt;
  &lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I was dimly aware that the Boost build tool, B2, was capable of doing this from one command-line invocation.&lt;/p&gt;

&lt;p&gt;It’s worth mentioning at this point that I have fairly recently discovered just how powerful B2 is. It’s a shame that
it has never been offered to the world in a neat package with some friendly conversation-style documentation, which
seems to be the norm these days.&lt;/p&gt;

&lt;p&gt;It can actually do anything CMake can do and more. For example, all of the above.&lt;/p&gt;

&lt;p&gt;My thanks to Peter Dimov for teaching me about the existence of B2 &lt;em&gt;features&lt;/em&gt; and how to use them.&lt;/p&gt;

&lt;p&gt;It turns out to be a simple 2-step process:&lt;/p&gt;

&lt;p&gt;First defined a &lt;code&gt;user-config.jam&lt;/code&gt; file to describe the feature and its settings:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-jam&quot;&gt;import feature ;

feature.feature asio.mode : dflt nodep nots ts nodep-nots nodep-ts : propagated composite ;
feature.compose &amp;lt;asio.mode&amp;gt;nodep : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;nodep-nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;nodep-ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ;

using clang :   : clang++ : &amp;lt;stdlib&amp;gt;&quot;libc++&quot; &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ;
using gcc :   : g++ : &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then ask b2 to do the rest:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./b2 --user-config=./user-config.jam \
  toolset=clang,gcc \
  asio.mode=dflt,nodep,nots,ts,nodep-nots,nodep-ts \
  variant=release \
  cxxstd=2a,17,14,11 \
  -j`grep processor /proc/cpuinfo | wc -l` \
  libs/beast/test libs/beast/example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will compile all examples and run all tests in beast on a linux platform for the cross-product of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;clang and gcc&lt;/li&gt;
  &lt;li&gt;all 6 of the legal combinations of the preprocessor macros BOOST_ASIO_NO_DEPRECATED, BOOST_ASIO_NO_TS_EXECUTORS and 
BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&lt;/li&gt;
  &lt;li&gt;C++ standards 2a, 17, 14 and 11&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So that’s 48 separate scenarios.&lt;/p&gt;

&lt;p&gt;It will also:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build any dependencies.&lt;/li&gt;
  &lt;li&gt;Build each scenario into its own separately named path in the bin.v2 directory.&lt;/li&gt;
  &lt;li&gt;Understand which tests passed and failed so that passing tests are not re-run on subsequent calls to b2 unless a
dependent file has changed.&lt;/li&gt;
  &lt;li&gt;Use as many CPUs as are available on the host (in my case, fortunately that’s 48, otherwise this would take a long time
to run)&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Boost 1.74 - Interesting Developments in Asio We’re currently beta-testing Boost 1.74, the lead-up to which has seen a flurry of activity in Asio, which has impacted Beast. Recent versions of Asio have moved away from the idea of sequencing completion handlers directly on an io_context (which used to be called an io_service) towards the execution of completion handlers by an Executor. The basic idea being that the executor is a lightweight handle to some execution context, which did what the io_context always used to do - schedule the execution of completion handlers. The changes to Asio have been tracking The Networking TS which describes a concept of Executor relevant to asynchronous IO. The Unified Executors proposal unifies the concepts of io execution and the general concept of “a place to execute work” - a somewhat more generic idea than merely an IO loop or thread pool. Work has been ongoing by the members of WG21 to produce an execution model that serves all parties’ needs. Courtesy of an incredible effort by Chris Kohlhoff, Latest Asio and Boost.Asio 1.74 has been updated to accommodate both models of executors, with the Unified Executors model being the default. It’s important to note that most users won’t notice the change in API this time around since by default the Asio in 1.74 also includes the 1.73 interface. There are a number of preprocessor macros that can be defined to change this default behaviour: BOOST_ASIO_NO_TS_EXECUTORS Defining this macro disables the Networking TS executor model. The most immediate thing you’ll notice if you define this macro is that for some executor e, the expression e.context() becomes invalid. In the Unified Executors world, this operation is expressed as a query against the executor: auto&amp;amp; ctx = asio::query(e, asio::execution::context); The idea being that the execution context is a property of an executor that can be queried for. Another change which users are likely to notice when this macro is defined is that the asio::executor_work_guard&amp;lt;&amp;gt; template corresponding asio::make_work_guard function is no longer defined. You may well ask then, how we would prevent an underlying execution context from running out of work? In the Unified Executors world, we can think of Executors as an unbounded set of types with various properties enabled or disabled. The idea is that the state of the properties define the behaviour of the interaction between the executor and its underlying context. In the new world, we don’t explicitly create a work guard which references the executor. We ‘simply’ create a new executor which happens to have the property of ‘tracking work’ (i.e. this executor will in some way ensure that the underlying context has outstanding work until the executor’s lifetime ends). Again, given that e is some executor, here’s how we spell this: auto tracked = asio::require(e, asio::execution::outstanding_work.tracked); After executing this statement, there are now two executors in play. The first, e may or may not be “tracking work” (ensuring that the underlying context does not stop), but tracked certainly is. There is another way to spell this, more useful in a generic programming environment. Suppose you were writing generic code and you don’t know the type of the executor presented to you, or even what kind of execution context it is associated with. However, you do know that if the underlying context can stop if it runs out of work, then we want to prevent it from doing so for the duration of some operation. In this case, we can’t use require because this will fail to compile if the given executor does not support the outstanding_work::tracked property. Therefore we would request (or more correctly, prefer) the capability rather than require it: auto maybe_tracked = asio::prefer(e, asio::execution::outstanding_work.tracked); We can now use maybe_tracked as the executor for our operation, and it will “do the right thing” regarding the tracking of work whatever the underlying type of execution context. It is important to note that it is an executor, not merely a guard object that contains an executor. post, dispatch and defer Another notable change in the Asio API when this macro is defined is that models of the Executor concept lose their post, dispatch and defer member functions. The free function versions still remain, so if you have code like this: e.dispatch([]{ /* something */ }); you will need to rewrite it as: asio::dispatch(e, []{ /* something */ }); or you can be more creative with the underlying property system: asio::execution::execute( asio::prefer( e, asio::execution::blocking.possibly), []{ /* something */ }); Which is more-or-less what the implementation of dispatch does under the covers. It’s actually a little more involved than that since the completion token’s associated allocator has to be taken into account. There is a property for that too: asio::execution::allocator. In summary, all previous Asio and Networking TS execution/completion scenarios are now handled by executing a handler in some executor supporting a set of relevant properties. BOOST_ASIO_NO_DEPRECATED Defining this macro will ensure that old asio-style invocation and allocation completion handler customisation functions will no longer be used. The newer paradigm is to explicitly query or require execution properties at the time of scheduling a completion handler for invocation. If you don’t know what any of that means, you’d be in the majority and don’t need to worry about it. BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT As of Boost 1.74, Asio IO objects will be associated with the new asio::any_io_executor rather than the previous polymorphic asio::executor. Defining this macro, undoes this change. It may be useful to you if you have written code that depends on the use of asio::executor. Other observations Strands are Still a Thing Asio strand objects still seem to occupy a twilight zone between executors and something other than executors. To be honest, when I first saw the property mechanism, I assumed that a strand would be “just another executor” with some “sequential execution” property enabled. This turns out not to be the case. A strand has its own distinct execution context which manages the sequencing of completion handler invocations within it. The strand keeps a copy of the inner executor, which is the one where the strand’s completion handlers will be invoked in turn. However, a strand models the Executor concept, so it also is an executor. execute() looks set to become the new call(). Reading the Unified Executors paper is an interesting, exciting or horrifying experience - depending on your view of what you’d like C++ to be. My take from the paper, fleshed out a little with the experience of touching the implementation in Asio, is that in the new world, the programming thought process will go something like this, imagine the following situation: “I need to execute this set of tasks, Ideally I’d like them to execute in parallel, I’d like to wait for them to be done” As I understand things, the idea behind unified executors is that I will be able to express these desires and mandates by executing my work function(s) in some executor yielded by a series of calls to prefer and require. Something like: auto eparallel = prefer(e, bulk_guarantee.unsequenced); // prefer parallel execution auto eblock = require(eparallel, blocking.always); // require blocking execute(eblock, task1, task2, task3, task...); // blocking call which will execute in parallel if possible Proponents will no doubt think, “Great! Programming by expression of intent”. Detractors might say, “Ugh! Nondeterministic programs. How do I debug this when it goes wrong?” To be honest that this stage, I find myself in both camps. No doubt time will tell. Adventures in B2 (Boost Build) Because of the pressure of testing Beast with the new multi-faceted Asio, I wanted a way to bulk compile and test many different variants of: Compilers Preprocessor macro definitions C++ standards etc. I was dimly aware that the Boost build tool, B2, was capable of doing this from one command-line invocation. It’s worth mentioning at this point that I have fairly recently discovered just how powerful B2 is. It’s a shame that it has never been offered to the world in a neat package with some friendly conversation-style documentation, which seems to be the norm these days. It can actually do anything CMake can do and more. For example, all of the above. My thanks to Peter Dimov for teaching me about the existence of B2 features and how to use them. It turns out to be a simple 2-step process: First defined a user-config.jam file to describe the feature and its settings: import feature ; feature.feature asio.mode : dflt nodep nots ts nodep-nots nodep-ts : propagated composite ; feature.compose &amp;lt;asio.mode&amp;gt;nodep : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;nodep-nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;nodep-ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ; using clang : : clang++ : &amp;lt;stdlib&amp;gt;&quot;libc++&quot; &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ; using gcc : : g++ : &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ; Then ask b2 to do the rest: ./b2 --user-config=./user-config.jam \ toolset=clang,gcc \ asio.mode=dflt,nodep,nots,ts,nodep-nots,nodep-ts \ variant=release \ cxxstd=2a,17,14,11 \ -j`grep processor /proc/cpuinfo | wc -l` \ libs/beast/test libs/beast/example This will compile all examples and run all tests in beast on a linux platform for the cross-product of: clang and gcc all 6 of the legal combinations of the preprocessor macros BOOST_ASIO_NO_DEPRECATED, BOOST_ASIO_NO_TS_EXECUTORS and BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT C++ standards 2a, 17, 14 and 11 So that’s 48 separate scenarios. It will also: Build any dependencies. Build each scenario into its own separately named path in the bin.v2 directory. Understand which tests passed and failed so that passing tests are not re-run on subsequent calls to b2 unless a dependent file has changed. Use as many CPUs as are available on the host (in my case, fortunately that’s 48, otherwise this would take a long time to run)</summary></entry><entry><title type="html">Krystian’s May &amp;amp; June Update</title><link href="http://cppalliance.org/krystian/2020/07/01/KrystiansMayJuneUpdate.html" rel="alternate" type="text/html" title="Krystian’s May &amp;amp; June Update" /><published>2020-07-01T00:00:00+00:00</published><updated>2020-07-01T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/07/01/KrystiansMayJuneUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/07/01/KrystiansMayJuneUpdate.html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;I’ve been very busy these last two months getting Boost.JSON ready for release, hence the combined blog post. Now that things are winding down, I hopefully can get back the normal blog release schedule.&lt;/p&gt;

&lt;h1 id=&quot;boostjson&quot;&gt;Boost.JSON&lt;/h1&gt;

&lt;p&gt;Aside from a couple of personal projects, the vast majority of my time was spent getting Boost.JSON set for release. Breaking it down, this consisted of three main tasks: a &lt;code&gt;tag_invoke&lt;/code&gt; based &lt;code&gt;value&lt;/code&gt; conversion interface, parser optimizations, and support for extended JSON syntax.&lt;/p&gt;

&lt;h2 id=&quot;value-conversion&quot;&gt;Value Conversion&lt;/h2&gt;

&lt;p&gt;Our previous interface that allowed users to specify their own conversions to and from &lt;code&gt;value&lt;/code&gt; proved unsatisfactory, as it required too much boiler-plate when specifying conversions to and from non-class types (e.g. enumeration types). To remedy this, I was tasked with implementing an ADL solution based on &lt;code&gt;tag_invoke&lt;/code&gt; which greatly reduces the amount of boiler-plate and provides a single, straightforward way to implement a custom conversion. For example, consider the following class type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct customer
{
	std::string name;
	std::size_t balance;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To convert an object of type &lt;code&gt;customer&lt;/code&gt; to &lt;code&gt;value&lt;/code&gt;, all you need is to write an overload of &lt;code&gt;tag_invoke&lt;/code&gt;. This can be implemented as an inline &lt;code&gt;friend&lt;/code&gt; function within the class definition (thus making it visible to ADL but not unqualified lookup; see [[basic.lookup.argdep] p4.3]](http://eel.is/c++draft/basic.lookup.argdep#4.3)), or as a free function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;void tag_invoke(value_from_tag, value&amp;amp; jv, const customer&amp;amp; c)
{
	object&amp;amp; obj = jv.emplace_object();
	obj[&quot;name&quot;] = c.name;
	obj[&quot;balance&quot;] = c.balance;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that a reference to &lt;code&gt;value&lt;/code&gt; is passed to the function performing the conversion. This ensures that the &lt;code&gt;storage_ptr&lt;/code&gt; passed to the calling function (i.e. &lt;code&gt;value_from(T&amp;amp;&amp;amp;, storage_ptr)&lt;/code&gt;) is correctly propagated to the result.&lt;/p&gt;

&lt;p&gt;Conversions from &lt;code&gt;value&lt;/code&gt; to a type &lt;code&gt;T&lt;/code&gt; are specified in a similar fashion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;customer tag_invoke(value_to_tag&amp;lt;customer&amp;gt;, const value&amp;amp; jv)
{
	return customer{
		value_to&amp;lt;std::string&amp;gt;(jv.at(&quot;name&quot;])), 
		jv.at(&quot;balance&quot;).as_uint64() 
	};
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to user-provided &lt;code&gt;tag_invoke&lt;/code&gt; overloads, generic conversions are provided for container-like, map-like, and string-like types, with obvious results. In general, if your container works with a range-based for loop, it will work with &lt;code&gt;value_from&lt;/code&gt; and &lt;code&gt;value_to&lt;/code&gt; without you having to write anything.&lt;/p&gt;

&lt;h2 id=&quot;parser-optimizations&quot;&gt;Parser Optimizations&lt;/h2&gt;

&lt;p&gt;Optimizing the parser was a side-project turned obsession for me. While it’s often a painfully tedious process of trying an idea, running benchmarks, and being disappointed with the results, the few times that you get a performance increase makes it all worth it.&lt;/p&gt;

&lt;p&gt;To preface, Boost.JSON is unique in that it can parse incrementally (no other C++ libraries implement this). However, incremental parsing is considerably slower than parsing a JSON document in its entirety, as a stack must be maintained to track which function the parser should resume to once more data is available. In addition to this, the use cases for incremental parsing will often involve bottlenecks much more significant than the speed of the parser. With this in mind, Boost.JSON’s parser is optimized for non-incremental parsing of a valid JSON document. The remainder of this post will be written without consideration for incremental parsing.&lt;/p&gt;

&lt;p&gt;Most of the optimizations were branch eliminations, such as removing branches based on call site preconditions. These yield small performance gains, but once compounded we saw a performance increase of up to 7% on certain benchmarks. The biggest gain in this category came from removing a large switch statement in &lt;code&gt;parse_value&lt;/code&gt; in favor of a manually written jump table. Making this function branchless significantly increases performance as it’s the most called function when parsing. This also makes the function very compact, meaning it can be inlined almost everywhere.&lt;/p&gt;

&lt;p&gt;In addition to benchmark driven optimization, I also optimized based on codegen. Going into it I really had no idea what I was doing, but after staring at it for a long time and watching some videos I got the hang of it. I used this method to optimize &lt;code&gt;parse_array&lt;/code&gt; and &lt;code&gt;parse_object&lt;/code&gt;, aiming to get the most linear hot path possible, with the fewest number of jumps. It took a few hours, but I was able to reach my target. This was done by moving some branches around, removing the &lt;code&gt;local_const_stream&lt;/code&gt; variable, and adding some optimization hints to various branches. In addition to this, the &lt;code&gt;std::size_t&lt;/code&gt; parameter (representing the number of elements) was removed from the &lt;code&gt;on_array_end&lt;/code&gt; and &lt;code&gt;on_object_end&lt;/code&gt; handlers as it didn’t provide any useful information and is not used by &lt;code&gt;parser&lt;/code&gt;. This yielded a performance increase of up to 4% in certain cases.&lt;/p&gt;

&lt;p&gt;The last major optimization was &lt;a href=&quot;https://github.com/CPPAlliance/json/issues/115&quot;&gt;suggested&lt;/a&gt; by &lt;a href=&quot;https://github.com/joaquintides&quot;&gt;Joaquín M López Muñoz&lt;/a&gt;. In essence, integer division is a slow operation, so compilers have all sorts of ways to avoid it; one of which is doing multiplication instead. When dividing by a constant divisor, the compiler is able to convert this to multiplication by the reciprocal of the divisor, which can be up to 20 times faster. Where this is applicable in Boost.JSON is in the calculation used to get the index of the bucket for a &lt;code&gt;object&lt;/code&gt; key. The implementation was pretty straightforward, and it yielded up to a 10% increase in performance for &lt;code&gt;object&lt;/code&gt; heavy benchmarks – a remarkable gain from such a small change. Thank you Joaquín :)&lt;/p&gt;

&lt;h2 id=&quot;parser-extensions&quot;&gt;Parser Extensions&lt;/h2&gt;

&lt;p&gt;The last major thing I worked on for Boost.JSON was implementing support for extended JSON syntaxes. The two supported extensions are: - allowing C and C++ style comments to appear within whitespace, and&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;allowing trailing commas to appear after the last element of an array or object. 
This post isn’t quite in chronological order, but comment support was my introduction into working on the parser (a trial by fire). After a few naive attempts at implementation, the result was comment parsing that did not affect performance at all when not enabled (as it should) and only has a minor impact on performance when enabled. This was done by building off existing branches within &lt;code&gt;parse_array&lt;/code&gt; and &lt;code&gt;parse_object&lt;/code&gt; instead of checking for comments every time whitespace is being parsed. Allowing for trailing commas was done in much the same way. The larger takeaway from implementing these extensions was getting to know the internals of the parser much better, allowing me to implement the aforementioned optimizations, as well as more complex extensions in the future.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to get in touch with me, you can message me on the &lt;a href=&quot;http://slack.cpp.al/&quot;&gt;Cpplang slack&lt;/a&gt;, or &lt;a href=&quot;mailto:sdkrystian@gmail.com&quot;&gt;shoot me an email&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Overview I’ve been very busy these last two months getting Boost.JSON ready for release, hence the combined blog post. Now that things are winding down, I hopefully can get back the normal blog release schedule. Boost.JSON Aside from a couple of personal projects, the vast majority of my time was spent getting Boost.JSON set for release. Breaking it down, this consisted of three main tasks: a tag_invoke based value conversion interface, parser optimizations, and support for extended JSON syntax. Value Conversion Our previous interface that allowed users to specify their own conversions to and from value proved unsatisfactory, as it required too much boiler-plate when specifying conversions to and from non-class types (e.g. enumeration types). To remedy this, I was tasked with implementing an ADL solution based on tag_invoke which greatly reduces the amount of boiler-plate and provides a single, straightforward way to implement a custom conversion. For example, consider the following class type: struct customer { std::string name; std::size_t balance; }; To convert an object of type customer to value, all you need is to write an overload of tag_invoke. This can be implemented as an inline friend function within the class definition (thus making it visible to ADL but not unqualified lookup; see [[basic.lookup.argdep] p4.3]](http://eel.is/c++draft/basic.lookup.argdep#4.3)), or as a free function: void tag_invoke(value_from_tag, value&amp;amp; jv, const customer&amp;amp; c) { object&amp;amp; obj = jv.emplace_object(); obj[&quot;name&quot;] = c.name; obj[&quot;balance&quot;] = c.balance; } Note that a reference to value is passed to the function performing the conversion. This ensures that the storage_ptr passed to the calling function (i.e. value_from(T&amp;amp;&amp;amp;, storage_ptr)) is correctly propagated to the result. Conversions from value to a type T are specified in a similar fashion: customer tag_invoke(value_to_tag&amp;lt;customer&amp;gt;, const value&amp;amp; jv) { return customer{ value_to&amp;lt;std::string&amp;gt;(jv.at(&quot;name&quot;])), jv.at(&quot;balance&quot;).as_uint64() }; } In addition to user-provided tag_invoke overloads, generic conversions are provided for container-like, map-like, and string-like types, with obvious results. In general, if your container works with a range-based for loop, it will work with value_from and value_to without you having to write anything. Parser Optimizations Optimizing the parser was a side-project turned obsession for me. While it’s often a painfully tedious process of trying an idea, running benchmarks, and being disappointed with the results, the few times that you get a performance increase makes it all worth it. To preface, Boost.JSON is unique in that it can parse incrementally (no other C++ libraries implement this). However, incremental parsing is considerably slower than parsing a JSON document in its entirety, as a stack must be maintained to track which function the parser should resume to once more data is available. In addition to this, the use cases for incremental parsing will often involve bottlenecks much more significant than the speed of the parser. With this in mind, Boost.JSON’s parser is optimized for non-incremental parsing of a valid JSON document. The remainder of this post will be written without consideration for incremental parsing. Most of the optimizations were branch eliminations, such as removing branches based on call site preconditions. These yield small performance gains, but once compounded we saw a performance increase of up to 7% on certain benchmarks. The biggest gain in this category came from removing a large switch statement in parse_value in favor of a manually written jump table. Making this function branchless significantly increases performance as it’s the most called function when parsing. This also makes the function very compact, meaning it can be inlined almost everywhere. In addition to benchmark driven optimization, I also optimized based on codegen. Going into it I really had no idea what I was doing, but after staring at it for a long time and watching some videos I got the hang of it. I used this method to optimize parse_array and parse_object, aiming to get the most linear hot path possible, with the fewest number of jumps. It took a few hours, but I was able to reach my target. This was done by moving some branches around, removing the local_const_stream variable, and adding some optimization hints to various branches. In addition to this, the std::size_t parameter (representing the number of elements) was removed from the on_array_end and on_object_end handlers as it didn’t provide any useful information and is not used by parser. This yielded a performance increase of up to 4% in certain cases. The last major optimization was suggested by Joaquín M López Muñoz. In essence, integer division is a slow operation, so compilers have all sorts of ways to avoid it; one of which is doing multiplication instead. When dividing by a constant divisor, the compiler is able to convert this to multiplication by the reciprocal of the divisor, which can be up to 20 times faster. Where this is applicable in Boost.JSON is in the calculation used to get the index of the bucket for a object key. The implementation was pretty straightforward, and it yielded up to a 10% increase in performance for object heavy benchmarks – a remarkable gain from such a small change. Thank you Joaquín :) Parser Extensions The last major thing I worked on for Boost.JSON was implementing support for extended JSON syntaxes. The two supported extensions are: - allowing C and C++ style comments to appear within whitespace, and allowing trailing commas to appear after the last element of an array or object. This post isn’t quite in chronological order, but comment support was my introduction into working on the parser (a trial by fire). After a few naive attempts at implementation, the result was comment parsing that did not affect performance at all when not enabled (as it should) and only has a minor impact on performance when enabled. This was done by building off existing branches within parse_array and parse_object instead of checking for comments every time whitespace is being parsed. Allowing for trailing commas was done in much the same way. The larger takeaway from implementing these extensions was getting to know the internals of the parser much better, allowing me to implement the aforementioned optimizations, as well as more complex extensions in the future. If you want to get in touch with me, you can message me on the Cpplang slack, or shoot me an email.</summary></entry><entry><title type="html">Automated Documentation Previews</title><link href="http://cppalliance.org/sam/2020/06/04/WebsitePreviews.html" rel="alternate" type="text/html" title="Automated Documentation Previews" /><published>2020-06-04T00:00:00+00:00</published><updated>2020-06-04T00:00:00+00:00</updated><id>http://cppalliance.org/sam/2020/06/04/WebsitePreviews</id><content type="html" xml:base="http://cppalliance.org/sam/2020/06/04/WebsitePreviews.html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Greetings, and welcome to my first blog post at The C++ Alliance.&lt;/p&gt;

&lt;p&gt;I’ve recently begun working on an interesting project for the Alliance which might also have more widespread applicability. The same requirement could possibly apply to your organization as well.&lt;/p&gt;

&lt;p&gt;Consider an open-source project that has multiple contributors who are submitting changes via pull-requests in Github. You’d like to have assurances that a pull-request passes all tests before being merged. That is done with continuous integration solutions such as Travis or Circle-CI, which are quite popular and well-known. Similarly, if the submission is &lt;em&gt;documentation&lt;/em&gt;, you would like to be able to view the formatted output in it’s final published format so you can review the layout, the colors, and so on. What would be the best way to build and publish documentation from pull requests?&lt;/p&gt;

&lt;p&gt;Perhaps the first thought would be to include the functionality in Travis or Circle-CI. And that is certainly possible. However, in some cases there may be sensitive passwords, ssh keys, or other tokens in the configuration. Is it safe to allow random pull requests, from conceivably anyone on the whole internet, to trigger a Circle-CI build that contains authentication information? Let’s explore that question, and then present a possible alternative that should be more secure.&lt;/p&gt;

&lt;h1 id=&quot;security&quot;&gt;Security&lt;/h1&gt;

&lt;p&gt;In Circle-CI, you can choose to enable or disable jobs for Pull Requests. It’s clearly safer to leave them disabled, but if the goal is to run automatic tests, this feature must be turned on. Next, you may choose to enable or disable access to sensitive keys for Pull Requests. This sounds like a great feature that will allow the jobs to be run safely. You could build Pull Requests with limited authorization. But what if you’d like to include secret keys in the build, that are needed to publish the documentation to an external server which is going to host the resulting content. After building the docs, they must be transferred to wherever they will be hosted. That means you must either include the secret keys in plain text, or toggle the setting to enable sensitive keys in Circle-CI.&lt;/p&gt;

&lt;p&gt;Let’s briefly think about the latter option. If secret keys are enabled in Circle-CI, they are not outright published or visible to the end-user. The build system obfuscates them. The obfuscation is a good first step. Unfortunately, there’s a file called .circleci/config.yml in the project, which contains all the commands to be run by the build system. A pull request could modify that file so that it prints the secrets in clear text.&lt;/p&gt;

&lt;p&gt;What can be done?&lt;/p&gt;

&lt;p&gt;The answer - which is not overly difficult if you already have some experience - is to run an in-house build server such as Jenkins. This adds multiple layers of security:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optionally, does &lt;em&gt;not&lt;/em&gt; publicly print the build output.&lt;/li&gt;
  &lt;li&gt;Optionally, does &lt;em&gt;not&lt;/em&gt; run based on a .circleci file or Jenkinsfile, so modifying the configuration file is not an avenue for external attacks.&lt;/li&gt;
  &lt;li&gt;For each build job, it will only include the minimal number of secret keys required for the current task, and nothing more.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the new system may not be impregnable, it’s a major improvement compared to the security issues with Circle-CI for this specific requirement.&lt;/p&gt;

&lt;h1 id=&quot;design&quot;&gt;Design&lt;/h1&gt;

&lt;p&gt;Here is a high level overview of how the system operates, before getting into further details.&lt;/p&gt;

&lt;p&gt;A jenkins server is installed.&lt;/p&gt;

&lt;p&gt;It builds the documentation jobs, and then copies the resulting files to AWS S3.&lt;/p&gt;

&lt;p&gt;The job posts a message in the GitHub pull request conversation with a hyperlink to the new docs.&lt;/p&gt;

&lt;p&gt;Each pull request will get it’s own separate “website”. There could be hundreds of versions being simultaneously hosted.&lt;/p&gt;

&lt;p&gt;An nginx proxy server which sits in front of S3 serves the documents with a consistent URL format, and allows multiple repositories to share the same S3 bucket.&lt;/p&gt;

&lt;p&gt;The resulting functionality can be seen in action. On this pull request &lt;a href=&quot;https://github.com/boostorg/beast/pull/1973&quot;&gt;https://github.com/boostorg/beast/pull/1973&lt;/a&gt; a message appears:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;An automated preview of the documentation is available at &lt;a href=&quot;http://1973.beastdocs.prtest.cppalliance.org/libs/beast/doc/html/index.html&quot;&gt;http://1973.beastdocs.prtest.cppalliance.org/libs/beast/doc/html/index.html&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The link takes you to the preview, and will be updated with each new commit to the pull request.&lt;/p&gt;

&lt;h1 id=&quot;more-details&quot;&gt;More Details&lt;/h1&gt;

&lt;p&gt;The Jenkins server polls each configured repository at a 5 minute interval, to see if a new pull request has been added. Alternatively, instead of polling, you may add a webhook in Github.&lt;/p&gt;

&lt;p&gt;Each repository corresponds to a separate Jenkins “project” on the server. A job checks out a copy of the submitted code, runs the specific steps necessary for that codebase, and uploads the resulting website to an AWS S3 bucket.&lt;/p&gt;

&lt;p&gt;The configuration leverages a few Jenkins plugins:&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“GitHub Pull Request Builder” to launch jobs based on the existence of a new pull request.&lt;/li&gt;
  &lt;li&gt;“S3 Publisher Plugin” for copying files to S3.&lt;/li&gt;
  &lt;li&gt;“CloudBees Docker Custom Build Environment Plugin” to run the build inside an isolated docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One previews bucket is created in S3 such as s3://example-previews&lt;/p&gt;

&lt;p&gt;The file path in the S3 bucket is formatted to be “repository”/”PR #”. For example, the filepath of pull request #60 for the repo called “website” is s3://example-previews/website/60&lt;/p&gt;

&lt;p&gt;The web URL is generated by inverting this path, so “website/60” becomes “60.website”. The full URL has the format “60.website.prtest.example.com”. This translation is accomplished with an nginx reverse proxy, hosted on the same Jenkins server.&lt;/p&gt;

&lt;p&gt;nginx rule:&lt;br /&gt;
rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break;&lt;/p&gt;

&lt;p&gt;A wildcard DNS entry sends the preview visitors to nginx:&lt;br /&gt;
*.prtest.example.com -&amp;gt; jenkins.example.com&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;

&lt;p&gt;In this section, we will go over all the steps in detail, as a tutorial.&lt;/p&gt;

&lt;p&gt;In the following code sections,&lt;br /&gt;
Replace “example.com” with your domain.&lt;br /&gt;
Replace “website” with your repository name.&lt;br /&gt;
Replace “example-previews” with your S3 bucket name.&lt;/p&gt;

&lt;h3 id=&quot;general-server-setup&quot;&gt;General Server Setup&lt;/h3&gt;

&lt;p&gt;Install Jenkins - https://www.jenkins.io/doc/book/installing/&lt;/p&gt;

&lt;p&gt;Install SSL certificate for Jenkins (jenkins.example.com):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apt install certbot
certbot certonly
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install nginx.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt install nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a website, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server {
    listen 80;
    listen [::]:80;
    server_name jenkins.example.com;
    location '/.well-known/acme-challenge' {
        default_type &quot;text/plain&quot;;
        root /var/www/letsencrypt;
    }
    location / {
         return 301 https://jenkins.example.com:8443$request_uri;
    }
}

server {
listen 8443 ssl default_server;
listen [::]:8443 ssl default_server;
ssl_certificate /etc/letsencrypt/live/jenkins.example.com/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/jenkins.example.com/privkey.pem;
#include snippets/snakeoil.conf;
location / {
include /etc/nginx/proxy_params;
proxy_pass http://localhost:8080;
proxy_read_timeout 90s;
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set the URL inside of Jenkins-&amp;gt;Manage Jenkins-&amp;gt;Configure System to be https://&lt;em&gt;url&lt;/em&gt; , replacing &lt;em&gt;url&lt;/em&gt; with the hostname such as jenkins.example.com.&lt;/p&gt;

&lt;p&gt;Install the plugin “GitHub pull requests builder”
Go to &lt;code&gt;Manage Jenkins&lt;/code&gt; -&amp;gt; &lt;code&gt;Configure System&lt;/code&gt; -&amp;gt; &lt;code&gt;GitHub pull requests builder&lt;/code&gt; section.&lt;/p&gt;

&lt;p&gt;Click “Create API Token”. Log into github.&lt;/p&gt;

&lt;p&gt;Update “Commit Status Build Triggered”, “Commit Status Build Start” to –none–
Create all three types of “Commit Status Build Result” with –none–&lt;/p&gt;

&lt;p&gt;On the server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt install git build-essential
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install the plugin “CloudBees Docker Custom Build Environment”&lt;/p&gt;

&lt;p&gt;add Jenkins to docker group&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;usermod -a -G docker jenkins
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Restart jenkins.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl restart jenkins
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install the “S3 publisher plugin”&lt;/p&gt;

&lt;p&gt;In Manage Jenkins-&amp;gt;Configure System, go to S3 Profiles, create profile. Assuming the IAM user in AWS is called “example-bot”, then create example-bot-profile with the AWS creds. The necessary IAM permissions are covered a bit further down in this document.&lt;/p&gt;

&lt;p&gt;Install the “Post Build Task plugin”&lt;/p&gt;

&lt;h3 id=&quot;nginx-setup&quot;&gt;Nginx Setup&lt;/h3&gt;

&lt;p&gt;Create a wildcard DNS entry at your DNS hosting provider:
*.prtest.website.example.com CNAME to jenkins.example.com&lt;/p&gt;

&lt;p&gt;Create an nginx site for previews:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server {
    # Listen on port 80 for all IPs associated with your machine
    listen 80 default_server;

    # Catch all other server names
    server_name _;

    if ($host ~* ([0-9]+)\.(.*?)\.(.*)) {
        set $pullrequest $1;
        set $repo $2;
    }

    location / {
        set $backendserver 'http://example-previews.s3-website-us-east-1.amazonaws.com';

        #CUSTOMIZATIONS
        if ($repo = &quot;example&quot; ) {
          rewrite ^(.*)/something$ $1/something.html ;
        }

        #FINAL REWRITE
        rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break;

        # The rewritten request is passed to S3
        proxy_pass http://example-previews.s3-website-us-east-1.amazonaws.com;
        #proxy_pass $backendserver;
        include /etc/nginx/proxy_params;
        proxy_redirect /$repo/$pullrequest / ;
    }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;aws-setup&quot;&gt;AWS Setup&lt;/h3&gt;

&lt;p&gt;Turn on static web hosting on the bucket.
Endpoint is http://example-previews.s3-website-us-east-1.amazonaws.com&lt;/p&gt;

&lt;p&gt;Add bucket policy&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;PublicReadGetObject&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: &quot;*&quot;,
            &quot;Action&quot;: &quot;s3:GetObject&quot;,
            &quot;Resource&quot;: &quot;arn:aws:s3:::example-previews/*&quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create an IAM user and add these permissions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:GetBucketLocation&quot;,
                &quot;s3:ListAllMyBuckets&quot;
            ],
            &quot;Resource&quot;: &quot;*&quot;
        },
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:ListBucket&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::example-previews&quot;
            ]
        },
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:PutObject&quot;,
                &quot;s3:GetObject&quot;,
                &quot;s3:DeleteObject&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::example-previews/*&quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;jenkins-freestyle-projects&quot;&gt;JENKINS FREESTYLE PROJECTS&lt;/h3&gt;

&lt;p&gt;Create a new Freestyle Project&lt;/p&gt;

&lt;p&gt;Github Project (checked)
Project URL: https://github.com/yourorg/website/&lt;/p&gt;

&lt;p&gt;Source Code Management
Git (checked)
Repositories: https://github.com/yourorg/website
Credentials: github-example-bot (you should add a credential here, that successfully connects to github)
Advanced:
Refspec: +refs/pull/&lt;em&gt;:refs/remotes/origin/pr/&lt;/em&gt;
Branch Specifier: ${ghprbActualCommit}&lt;/p&gt;

&lt;p&gt;Build Triggers
GitHub Pull Request Builder (checked)
GitHub API Credentials: mybot&lt;/p&gt;

&lt;p&gt;#Consider whether to enable the following setting.
#It is optional. You may also approve each PR.
Advanced:
Build every pull request automatically without asking.&lt;/p&gt;

&lt;p&gt;Trigger Setup:
Build Status Message:
&lt;code&gt;An automated preview of this PR is available at [http://$ghprbPullId.website.prtest.example.com](http://$ghprbPullId.website.prtest.example.com)&lt;/code&gt;
Update Commit Message during build:
Commit Status Build Triggered: –none–
Commit Status Build Started: –none–
Commit Status Build Result: create all types of result, with message –none–&lt;/p&gt;

&lt;p&gt;Build Environment:
Build inside a Docker container (checked)
#Note: choose a Docker image that is appropriate for your project
Pull docker image from repository: circleci/ruby:2.4-node-browsers-legacy&lt;/p&gt;

&lt;p&gt;Build:
Execute Shell:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Note: whichever build steps your site requires.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Post-build Actions
Publish artifacts to S3
S3 Profile: example-bot-profile&lt;/p&gt;

&lt;p&gt;Source: _site/** (set this value as necessary for your code)
Destination:  example-previews/example/${ghprbPullId}
Bucket Region: us-east-1
No upload on build failure (checked)&lt;/p&gt;

&lt;p&gt;#The following part is optional. It will post an alert into a Slack channel.
Add Post Build Tasks&lt;/p&gt;

&lt;p&gt;Log Text: GitHub&lt;/p&gt;

&lt;p&gt;Script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
PREVIEWMESSAGE=&quot;A preview of the example website is available at http://$ghprbPullId.example.prtest.example.com&quot;
curl -X POST -H 'Content-type: application/json' --data &quot;{\&quot;text\&quot;:\&quot;$PREVIEWMESSAGE\&quot;}&quot;  https://hooks.slack.com/services/T21Q22/B0141JT/aPF___
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check box “Run script only if all previous steps were successful”&lt;/p&gt;

&lt;p&gt;In Slack administration, (not in jenkins), create a Slack app. Create a “webhook” for your channel. That webhook goes into the curl command.&lt;/p&gt;</content><author><name></name></author><category term="sam" /><summary type="html">Overview Greetings, and welcome to my first blog post at The C++ Alliance. I’ve recently begun working on an interesting project for the Alliance which might also have more widespread applicability. The same requirement could possibly apply to your organization as well. Consider an open-source project that has multiple contributors who are submitting changes via pull-requests in Github. You’d like to have assurances that a pull-request passes all tests before being merged. That is done with continuous integration solutions such as Travis or Circle-CI, which are quite popular and well-known. Similarly, if the submission is documentation, you would like to be able to view the formatted output in it’s final published format so you can review the layout, the colors, and so on. What would be the best way to build and publish documentation from pull requests? Perhaps the first thought would be to include the functionality in Travis or Circle-CI. And that is certainly possible. However, in some cases there may be sensitive passwords, ssh keys, or other tokens in the configuration. Is it safe to allow random pull requests, from conceivably anyone on the whole internet, to trigger a Circle-CI build that contains authentication information? Let’s explore that question, and then present a possible alternative that should be more secure. Security In Circle-CI, you can choose to enable or disable jobs for Pull Requests. It’s clearly safer to leave them disabled, but if the goal is to run automatic tests, this feature must be turned on. Next, you may choose to enable or disable access to sensitive keys for Pull Requests. This sounds like a great feature that will allow the jobs to be run safely. You could build Pull Requests with limited authorization. But what if you’d like to include secret keys in the build, that are needed to publish the documentation to an external server which is going to host the resulting content. After building the docs, they must be transferred to wherever they will be hosted. That means you must either include the secret keys in plain text, or toggle the setting to enable sensitive keys in Circle-CI. Let’s briefly think about the latter option. If secret keys are enabled in Circle-CI, they are not outright published or visible to the end-user. The build system obfuscates them. The obfuscation is a good first step. Unfortunately, there’s a file called .circleci/config.yml in the project, which contains all the commands to be run by the build system. A pull request could modify that file so that it prints the secrets in clear text. What can be done? The answer - which is not overly difficult if you already have some experience - is to run an in-house build server such as Jenkins. This adds multiple layers of security: Optionally, does not publicly print the build output. Optionally, does not run based on a .circleci file or Jenkinsfile, so modifying the configuration file is not an avenue for external attacks. For each build job, it will only include the minimal number of secret keys required for the current task, and nothing more. While the new system may not be impregnable, it’s a major improvement compared to the security issues with Circle-CI for this specific requirement. Design Here is a high level overview of how the system operates, before getting into further details. A jenkins server is installed. It builds the documentation jobs, and then copies the resulting files to AWS S3. The job posts a message in the GitHub pull request conversation with a hyperlink to the new docs. Each pull request will get it’s own separate “website”. There could be hundreds of versions being simultaneously hosted. An nginx proxy server which sits in front of S3 serves the documents with a consistent URL format, and allows multiple repositories to share the same S3 bucket. The resulting functionality can be seen in action. On this pull request https://github.com/boostorg/beast/pull/1973 a message appears: An automated preview of the documentation is available at http://1973.beastdocs.prtest.cppalliance.org/libs/beast/doc/html/index.html The link takes you to the preview, and will be updated with each new commit to the pull request. More Details The Jenkins server polls each configured repository at a 5 minute interval, to see if a new pull request has been added. Alternatively, instead of polling, you may add a webhook in Github. Each repository corresponds to a separate Jenkins “project” on the server. A job checks out a copy of the submitted code, runs the specific steps necessary for that codebase, and uploads the resulting website to an AWS S3 bucket. The configuration leverages a few Jenkins plugins: “GitHub Pull Request Builder” to launch jobs based on the existence of a new pull request. “S3 Publisher Plugin” for copying files to S3. “CloudBees Docker Custom Build Environment Plugin” to run the build inside an isolated docker container. One previews bucket is created in S3 such as s3://example-previews The file path in the S3 bucket is formatted to be “repository”/”PR #”. For example, the filepath of pull request #60 for the repo called “website” is s3://example-previews/website/60 The web URL is generated by inverting this path, so “website/60” becomes “60.website”. The full URL has the format “60.website.prtest.example.com”. This translation is accomplished with an nginx reverse proxy, hosted on the same Jenkins server. nginx rule: rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break; A wildcard DNS entry sends the preview visitors to nginx: *.prtest.example.com -&amp;gt; jenkins.example.com Implementation In this section, we will go over all the steps in detail, as a tutorial. In the following code sections, Replace “example.com” with your domain. Replace “website” with your repository name. Replace “example-previews” with your S3 bucket name. General Server Setup Install Jenkins - https://www.jenkins.io/doc/book/installing/ Install SSL certificate for Jenkins (jenkins.example.com): apt install certbot certbot certonly Install nginx. apt install nginx Create a website, as follows: server { listen 80; listen [::]:80; server_name jenkins.example.com; location '/.well-known/acme-challenge' { default_type &quot;text/plain&quot;; root /var/www/letsencrypt; } location / { return 301 https://jenkins.example.com:8443$request_uri; } } server { listen 8443 ssl default_server; listen [::]:8443 ssl default_server; ssl_certificate /etc/letsencrypt/live/jenkins.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/jenkins.example.com/privkey.pem; #include snippets/snakeoil.conf; location / { include /etc/nginx/proxy_params; proxy_pass http://localhost:8080; proxy_read_timeout 90s; } } Set the URL inside of Jenkins-&amp;gt;Manage Jenkins-&amp;gt;Configure System to be https://url , replacing url with the hostname such as jenkins.example.com. Install the plugin “GitHub pull requests builder” Go to Manage Jenkins -&amp;gt; Configure System -&amp;gt; GitHub pull requests builder section. Click “Create API Token”. Log into github. Update “Commit Status Build Triggered”, “Commit Status Build Start” to –none– Create all three types of “Commit Status Build Result” with –none– On the server: apt install git build-essential Install the plugin “CloudBees Docker Custom Build Environment” add Jenkins to docker group usermod -a -G docker jenkins Restart jenkins. systemctl restart jenkins Install the “S3 publisher plugin” In Manage Jenkins-&amp;gt;Configure System, go to S3 Profiles, create profile. Assuming the IAM user in AWS is called “example-bot”, then create example-bot-profile with the AWS creds. The necessary IAM permissions are covered a bit further down in this document. Install the “Post Build Task plugin” Nginx Setup Create a wildcard DNS entry at your DNS hosting provider: *.prtest.website.example.com CNAME to jenkins.example.com Create an nginx site for previews: server { # Listen on port 80 for all IPs associated with your machine listen 80 default_server; # Catch all other server names server_name _; if ($host ~* ([0-9]+)\.(.*?)\.(.*)) { set $pullrequest $1; set $repo $2; } location / { set $backendserver 'http://example-previews.s3-website-us-east-1.amazonaws.com'; #CUSTOMIZATIONS if ($repo = &quot;example&quot; ) { rewrite ^(.*)/something$ $1/something.html ; } #FINAL REWRITE rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break; # The rewritten request is passed to S3 proxy_pass http://example-previews.s3-website-us-east-1.amazonaws.com; #proxy_pass $backendserver; include /etc/nginx/proxy_params; proxy_redirect /$repo/$pullrequest / ; } } AWS Setup Turn on static web hosting on the bucket. Endpoint is http://example-previews.s3-website-us-east-1.amazonaws.com Add bucket policy { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;PublicReadGetObject&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:GetObject&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::example-previews/*&quot; } ] } Create an IAM user and add these permissions &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:GetBucketLocation&quot;, &quot;s3:ListAllMyBuckets&quot; ], &quot;Resource&quot;: &quot;*&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:ListBucket&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::example-previews&quot; ] }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:PutObject&quot;, &quot;s3:GetObject&quot;, &quot;s3:DeleteObject&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::example-previews/*&quot; ] } ] } JENKINS FREESTYLE PROJECTS Create a new Freestyle Project Github Project (checked) Project URL: https://github.com/yourorg/website/ Source Code Management Git (checked) Repositories: https://github.com/yourorg/website Credentials: github-example-bot (you should add a credential here, that successfully connects to github) Advanced: Refspec: +refs/pull/:refs/remotes/origin/pr/ Branch Specifier: ${ghprbActualCommit} Build Triggers GitHub Pull Request Builder (checked) GitHub API Credentials: mybot #Consider whether to enable the following setting. #It is optional. You may also approve each PR. Advanced: Build every pull request automatically without asking. Trigger Setup: Build Status Message: An automated preview of this PR is available at [http://$ghprbPullId.website.prtest.example.com](http://$ghprbPullId.website.prtest.example.com) Update Commit Message during build: Commit Status Build Triggered: –none– Commit Status Build Started: –none– Commit Status Build Result: create all types of result, with message –none– Build Environment: Build inside a Docker container (checked) #Note: choose a Docker image that is appropriate for your project Pull docker image from repository: circleci/ruby:2.4-node-browsers-legacy Build: Execute Shell: #Note: whichever build steps your site requires. Post-build Actions Publish artifacts to S3 S3 Profile: example-bot-profile Source: _site/** (set this value as necessary for your code) Destination: example-previews/example/${ghprbPullId} Bucket Region: us-east-1 No upload on build failure (checked) #The following part is optional. It will post an alert into a Slack channel. Add Post Build Tasks Log Text: GitHub Script: #!/bin/bash PREVIEWMESSAGE=&quot;A preview of the example website is available at http://$ghprbPullId.example.prtest.example.com&quot; curl -X POST -H 'Content-type: application/json' --data &quot;{\&quot;text\&quot;:\&quot;$PREVIEWMESSAGE\&quot;}&quot; https://hooks.slack.com/services/T21Q22/B0141JT/aPF___ Check box “Run script only if all previous steps were successful” In Slack administration, (not in jenkins), create a Slack app. Create a “webhook” for your channel. That webhook goes into the curl command.</summary></entry><entry><title type="html">Krystian’s April Update</title><link href="http://cppalliance.org/krystian/2020/05/08/KrystiansAprilUpdate.html" rel="alternate" type="text/html" title="Krystian’s April Update" /><published>2020-05-08T00:00:00+00:00</published><updated>2020-05-08T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/05/08/KrystiansAprilUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/05/08/KrystiansAprilUpdate.html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Boost 1.73.0 has been released! Save for some minor documentation issues, Boost.StaticString enjoyed a bug-free release, so most of this month was spent working on Boost.JSON getting it ready for review. Unfortunately, I could not spend too much time working due to school and final exams, but now that those have passed I’ll be able to put in significantly more time working on projects such as Boost.JSON.&lt;/p&gt;

&lt;h1 id=&quot;boostjson&quot;&gt;Boost.JSON&lt;/h1&gt;

&lt;p&gt;A good portion of my work on Boost.JSON was spent updating the documentation to reflect the replacement of the &lt;code&gt;storage&lt;/code&gt; allocator model with &lt;code&gt;boost::container::pmr::memory_resource&lt;/code&gt; (or &lt;code&gt;std::pmr::memory_resource&lt;/code&gt; in standalone). The old model wasn’t necessarily bad, but using &lt;code&gt;memory_resource&lt;/code&gt; permits the use of existing allocators found in Boost.Container/the standard library, eliminating the need for writing proprietary allocators that only work with Boost.JSON.&lt;/p&gt;

&lt;p&gt;Even though &lt;code&gt;storage&lt;/code&gt; will be going away, &lt;code&gt;storage_ptr&lt;/code&gt; will remain to support shared ownership of a &lt;code&gt;memory_resource&lt;/code&gt; – something that &lt;code&gt;polymorphic_allocator&lt;/code&gt; lacks. As with &lt;code&gt;polymorphic_allocator&lt;/code&gt;, &lt;code&gt;storage_ptr&lt;/code&gt; will still support non-owning reference semantics in contexts where the lifetime of a &lt;code&gt;memory_resource&lt;/code&gt; is bound to a scope, giving users more flexibility.&lt;/p&gt;

&lt;p&gt;I also worked on &lt;code&gt;monotonic_resource&lt;/code&gt;, the &lt;code&gt;memory_resource&lt;/code&gt; counterpart to &lt;code&gt;pool&lt;/code&gt;. This allocator has one goal: to be &lt;em&gt;fast&lt;/em&gt;. I ended up adding the following features to facilitate this (mostly from &lt;code&gt;monotonic_buffer_resource&lt;/code&gt;):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Construction from an initial buffer,&lt;/li&gt;
  &lt;li&gt;The ability to reset the allocator without releasing memory, and&lt;/li&gt;
  &lt;li&gt;The ability to set a limit on the number of bytes that can be dynamically allocated.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The implementations of these features are pretty trivial, but they provide significant opportunities to cut down on dynamic allocations. For example, when parsing a large number of JSON documents, a single &lt;code&gt;monotonic_resource&lt;/code&gt; can be used and reset in between the parsing of each document without releasing any dynamically allocated storage. While care should be taken to destroy objects that occupy the storage before the allocator is reset, this can substantially reduce the number of allocations required and thus result in non-trivial performance gains.&lt;/p&gt;

&lt;p&gt;The other major thing I worked on was fixing an overload resolution bug on clang-cl involving &lt;code&gt;json::value&lt;/code&gt;. This was originally brought to my attention by Vinnie when the CI build for clang-cl started reporting that overload resolution for &lt;code&gt;value({false, 1, &quot;2&quot;})&lt;/code&gt; was ambiguous. After a few hours of investigating, I found that &lt;code&gt;false&lt;/code&gt; was being treated as a null pointer constant – something that was certainly annoying, but it also didn’t fully explain why this error was happening.&lt;/p&gt;

&lt;p&gt;After this unfortunate discovery, I tried again with &lt;code&gt;value({0, 1, &quot;2&quot;})&lt;/code&gt;, this time on clang, and it turns out this was a problem here as well. After &lt;em&gt;many&lt;/em&gt; hours of testing, I found that the constructor in &lt;code&gt;storage_ptr&lt;/code&gt; taking a parameter of type &lt;code&gt;memory_resource&lt;/code&gt; had a small problem: its constraint was missing &lt;code&gt;::type&lt;/code&gt; after the &lt;code&gt;enable_if&lt;/code&gt;, allowing &lt;code&gt;storage_ptr&lt;/code&gt; to be constructed from any pointer type, including &lt;code&gt;const char*&lt;/code&gt;. This somewhat helped to alleviate the problem, but &lt;code&gt;value({false, false, false})&lt;/code&gt; was still failing. After many more hours of groking the standard and trying to reproduce the error, I finally came upon the following &lt;code&gt;json::string&lt;/code&gt; constructors:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;string(string const&amp;amp; other, std::size_t pos, std::size_t count = npos, storage_ptr sp = {})

string(string_view other, std::size_t pos, std::size_t count = npos, storage_ptr sp = {})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the problem here? Since the first parameter of both constructors can be constructed from null pointer constants, overload resolution for &lt;code&gt;string(0, 0, 0)&lt;/code&gt; would be ambiguous. However, this isn’t the full story. Consider the following constructors for &lt;code&gt;value&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;value(std::initializer_list&amp;lt;value_ref&amp;gt; init)

value(string str)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the initialization of &lt;code&gt;value({0, 0, 0})&lt;/code&gt; the implicit conversion sequence to &lt;code&gt;str&lt;/code&gt; would be ambiguous, but the one to &lt;code&gt;value_ref&lt;/code&gt; can be formed. There is a special rule for overload resolution (separate from two-stage overload resolution during list-initialization) that considers any list-initialization sequence that converts to &lt;code&gt;std::initializer_list&lt;/code&gt; to be a better conversion sequence than one that does not, with the exception to this rule being that it only applies when the two conversion sequences are otherwise identical.&lt;/p&gt;

&lt;p&gt;This rule &lt;em&gt;should&lt;/em&gt; apply here, however, I found that clang has a small bug that prevents this rule from going into effect if any of the candidates have an ambiguous conversion sequence for the same parameter. We solve this pretty trivially by removing some of the redundant constructor overloads in &lt;code&gt;json::string&lt;/code&gt; and all was well. It was a fun little puzzle to solve (the explanation was a bit of an oversimplification; if you have questions please let me know).&lt;/p&gt;

&lt;p&gt;If you want to get in touch with me, you can message me on the &lt;a href=&quot;http://slack.cpp.al/&quot;&gt;Cpplang slack&lt;/a&gt;, or &lt;a href=&quot;mailto:sdkrystian@gmail.com&quot;&gt;shoot me an email&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Overview Boost 1.73.0 has been released! Save for some minor documentation issues, Boost.StaticString enjoyed a bug-free release, so most of this month was spent working on Boost.JSON getting it ready for review. Unfortunately, I could not spend too much time working due to school and final exams, but now that those have passed I’ll be able to put in significantly more time working on projects such as Boost.JSON. Boost.JSON A good portion of my work on Boost.JSON was spent updating the documentation to reflect the replacement of the storage allocator model with boost::container::pmr::memory_resource (or std::pmr::memory_resource in standalone). The old model wasn’t necessarily bad, but using memory_resource permits the use of existing allocators found in Boost.Container/the standard library, eliminating the need for writing proprietary allocators that only work with Boost.JSON. Even though storage will be going away, storage_ptr will remain to support shared ownership of a memory_resource – something that polymorphic_allocator lacks. As with polymorphic_allocator, storage_ptr will still support non-owning reference semantics in contexts where the lifetime of a memory_resource is bound to a scope, giving users more flexibility. I also worked on monotonic_resource, the memory_resource counterpart to pool. This allocator has one goal: to be fast. I ended up adding the following features to facilitate this (mostly from monotonic_buffer_resource): Construction from an initial buffer, The ability to reset the allocator without releasing memory, and The ability to set a limit on the number of bytes that can be dynamically allocated. The implementations of these features are pretty trivial, but they provide significant opportunities to cut down on dynamic allocations. For example, when parsing a large number of JSON documents, a single monotonic_resource can be used and reset in between the parsing of each document without releasing any dynamically allocated storage. While care should be taken to destroy objects that occupy the storage before the allocator is reset, this can substantially reduce the number of allocations required and thus result in non-trivial performance gains. The other major thing I worked on was fixing an overload resolution bug on clang-cl involving json::value. This was originally brought to my attention by Vinnie when the CI build for clang-cl started reporting that overload resolution for value({false, 1, &quot;2&quot;}) was ambiguous. After a few hours of investigating, I found that false was being treated as a null pointer constant – something that was certainly annoying, but it also didn’t fully explain why this error was happening. After this unfortunate discovery, I tried again with value({0, 1, &quot;2&quot;}), this time on clang, and it turns out this was a problem here as well. After many hours of testing, I found that the constructor in storage_ptr taking a parameter of type memory_resource had a small problem: its constraint was missing ::type after the enable_if, allowing storage_ptr to be constructed from any pointer type, including const char*. This somewhat helped to alleviate the problem, but value({false, false, false}) was still failing. After many more hours of groking the standard and trying to reproduce the error, I finally came upon the following json::string constructors: string(string const&amp;amp; other, std::size_t pos, std::size_t count = npos, storage_ptr sp = {}) string(string_view other, std::size_t pos, std::size_t count = npos, storage_ptr sp = {}) See the problem here? Since the first parameter of both constructors can be constructed from null pointer constants, overload resolution for string(0, 0, 0) would be ambiguous. However, this isn’t the full story. Consider the following constructors for value: value(std::initializer_list&amp;lt;value_ref&amp;gt; init) value(string str) For the initialization of value({0, 0, 0}) the implicit conversion sequence to str would be ambiguous, but the one to value_ref can be formed. There is a special rule for overload resolution (separate from two-stage overload resolution during list-initialization) that considers any list-initialization sequence that converts to std::initializer_list to be a better conversion sequence than one that does not, with the exception to this rule being that it only applies when the two conversion sequences are otherwise identical. This rule should apply here, however, I found that clang has a small bug that prevents this rule from going into effect if any of the candidates have an ambiguous conversion sequence for the same parameter. We solve this pretty trivially by removing some of the redundant constructor overloads in json::string and all was well. It was a fun little puzzle to solve (the explanation was a bit of an oversimplification; if you have questions please let me know). If you want to get in touch with me, you can message me on the Cpplang slack, or shoot me an email.</summary></entry><entry><title type="html">Richard’s April Update</title><link href="http://cppalliance.org/richard/2020/04/30/RichardsAprilUpdate.html" rel="alternate" type="text/html" title="Richard’s April Update" /><published>2020-04-30T00:00:00+00:00</published><updated>2020-04-30T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/04/30/RichardsAprilUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/04/30/RichardsAprilUpdate.html">&lt;h1 id=&quot;boost-173-released-and-other-matters&quot;&gt;Boost 1.73 Released and other Matters&lt;/h1&gt;

&lt;p&gt;The 1.73.0 release of Boost took up more attention than I had anticipated, but in the end all seemed to go well.&lt;/p&gt;

&lt;p&gt;Since then I’ve been working through the issues list on GitHub and am now starting to make some headway.&lt;/p&gt;

&lt;p&gt;I cam across a few other interesting (to me) topics this month.&lt;/p&gt;

&lt;h1 id=&quot;possibly-interesting-asio-things&quot;&gt;(Possibly) Interesting Asio Things&lt;/h1&gt;

&lt;p&gt;Last month I asked the question, “Is it possible to write an asynchronous composed operation entirely as a lambda?”.&lt;/p&gt;

&lt;p&gt;This month I went a little further with two items that interested me.&lt;/p&gt;

&lt;p&gt;The first is whether asio’s &lt;code&gt;async_compose&lt;/code&gt; can be adapted so that we can implement a complex composed operation involving
more than one IO object easily using the asio faux &lt;code&gt;coroutine&lt;/code&gt; mechanism.&lt;/p&gt;

&lt;p&gt;The second was whether is was possible to easily implement an async future in Asio.&lt;/p&gt;

&lt;h2 id=&quot;async-asio-future&quot;&gt;Async Asio Future&lt;/h2&gt;

&lt;p&gt;Here is my motivating use case:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    auto p = async::promise&amp;lt;std::string&amp;gt;();
    auto f = p.get_future();

    // long-running process starts which will yield a string
    start_something(std::move(p));

    // wait on the future
    f.async_wait([](some_result_type x) {
      // use the x
    });

    // or
    auto str = co_await f.async_wait(net::use_awaitable);

   // or shorthand
   auto str = co_await f();

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The salient points here are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;no matter on which thread the promise is fulfilled, the future will complete on the associated executor of the handler
passed to &lt;code&gt;async_wait&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Ideally the promise/future should not make use of mutexes un-necessarily.&lt;/li&gt;
  &lt;li&gt;(problematic for ASIO) It must work with objects that are not default-constructable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the end, I didn’t achieve the second goal as this was not a priority project, but I would be interested to see
if anyone can improve on the design.&lt;/p&gt;

&lt;p&gt;The source code is &lt;a href=&quot;https://github.com/madmongo1/webclient/blob/develop/include/boost/webclient/async/future.hpp&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I tried a couple of ways around the non-default-constructable requirement. My first was to require the CompletionToken
to the async_wait initiating function to be compatible with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;void (error_code, std::optional&amp;lt;T&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But I felt this was unwieldy.&lt;/p&gt;

&lt;p&gt;Then I remembered Boost.Outcome. I have been looking for a use for this library for some time.
It turns out that you can legally write an ASIO composed operation who’s handler takes a single
argument of any type, and this will translate cleanly when used with &lt;code&gt;net::use_future&lt;/code&gt;, &lt;code&gt;net::use_awaitable&lt;/code&gt; etc.&lt;/p&gt;

&lt;p&gt;A default Boost.Outcome object almost fits the bill, except that its exception_ptr type is boost rather than standard.&lt;/p&gt;

&lt;p&gt;This is easily solved with a typedef:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template&amp;lt;class T&amp;gt; using myoutcome = boost::outcome2::basic_outcome&amp;lt;T, error:code, std::exception_ptr&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I was feeling please with myself for figuring this out, until I came to test code code under C++11… and realised
that Boost.Outcome is only compatible with C++14 or higher.&lt;/p&gt;

&lt;p&gt;So in the end, I cobbled together a ‘good enough’ version of outcome using a variant:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template &amp;lt; class T &amp;gt;
struct outcome
{
    outcome(T arg) : var_(std::move(arg)) {}
    outcome(error_code const&amp;amp; arg) : var_(arg) {}
    outcome(std::exception_ptr const&amp;amp; arg) : var_(arg) {}

    auto has_value() const -&amp;gt; bool { return polyfill::holds_alternative&amp;lt; T &amp;gt;(var_); }
    auto has_error() const -&amp;gt; bool { return polyfill::holds_alternative&amp;lt; error_code &amp;gt;(var_); }
    auto has_exception() const -&amp;gt; bool { return polyfill::holds_alternative&amp;lt; std::exception_ptr &amp;gt;(var_); }

    auto value() &amp;amp; -&amp;gt; T &amp;amp;;
    auto value() &amp;amp;&amp;amp; -&amp;gt; T &amp;amp;&amp;amp;;
    auto value() const &amp;amp; -&amp;gt; T const &amp;amp;;

    auto error() const -&amp;gt; error_code const &amp;amp;;

    using variant_type = polyfill::variant&amp;lt; T, error_code, std::exception_ptr &amp;gt;;
    variant_type var_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code for this is &lt;a href=&quot;https://github.com/madmongo1/webclient/blob/develop/include/boost/webclient/polyfill/outcome.hpp&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally this allowed me to express intent at the call site like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    auto f = p.get_future();

    f.async_wait([](outcome&amp;lt;std::string&amp;gt; os){
        if (os.has_value())
            // use the value
        else if (os.has_error())
            // use the error
        else
            // deal with the exception
    });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The coroutine interface can be made cleaner:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    try {
        auto str = co_await f();
        // use the string
    }
    catch(system_error&amp;amp; ec) {
        // use the error code in ec.code()
    }
    catch(...) {
        // probably catastrophic
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the above code to compile we’d have to add the following trivial transform:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template &amp;lt; class T &amp;gt;
    auto future&amp;lt; T &amp;gt;::operator()() -&amp;gt; net::awaitable&amp;lt; T &amp;gt;
    {
        auto r = co_await async_wait(net::use_awaitable);
        if (r.has_value())
            co_return std::move(r).assume_value();
        else if (r.has_error())
            throw system_error(r.assume_error());
        else
            throw r.exception();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;easy-complex-coroutines-with-async_compose&quot;&gt;Easy Complex Coroutines with async_compose&lt;/h2&gt;

&lt;p&gt;When your composed operation’s intermediate completion handlers are invoked,
the underlying &lt;code&gt;detail::composed_op&lt;/code&gt; provides a mutable reference to itself. A typical completion handler looks like
this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template&amp;lt;class Self&amp;gt;
    void operator()(Self&amp;amp; self, error_code ec = {} , std::size_t bytes_transferred = 0)
    {
        reenter(this) {
            // yields and operations on Self
            yield async_write(sock, buf, std::move(self));  // note that self is moved
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What I wanted was a composed operation where the following is legal:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template&amp;lt;class Self&amp;gt;
    void operator()(Self self /* note copy */, error_code ec = {} , std::size_t bytes_transferred = 0)
    {
        reenter(this) {
            // yields and operations on Self
            yield
            {
                async_write(sock, buf, self);
                timer.async_wait(self);
                writing = true;
                sending = true;
            }

            while(writing || sending)
                yield
                    // something needs to happen here to reset the flags and handle errors and cancellation.
                ;
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which I think looks reasonably clear and easy to follow.&lt;/p&gt;

&lt;p&gt;In this work I had to overcome two problems - writing the framework to allow it, and thinking of a maintainable way to
express intent in the interrelationships between the asynchronous operations on the timer and the socket.&lt;/p&gt;

&lt;p&gt;Solving the copyable composed_op problem was easy. I did what I always do in situations like this. I cheated.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;asio::async_compose&lt;/code&gt; produces a specialisation of a &lt;code&gt;detail::composed_op&amp;lt;&amp;gt;&lt;/code&gt; template. Substituting a disregard of the
rules for knowledge and skill, I simply reached into the guts of asio and produced a copyable wrapper to this class.
I also cut/pasted some ancillary free functions in order to make asio work nicely with my new class:&lt;/p&gt;

&lt;p&gt;Here’s the code… it’s not pretty:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template &amp;lt; class Impl, class Work, class Handler, class Signature &amp;gt;
struct shared_composed_op
{
    using composed_op_type = boost::asio::detail::composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt;;

    using allocator_type = typename net::associated_allocator&amp;lt; composed_op_type &amp;gt;::type;
    using executor_type  = typename net::associated_executor&amp;lt; composed_op_type &amp;gt;::type;

    shared_composed_op(composed_op_type &amp;amp;&amp;amp;op)
    : impl_(std::make_shared&amp;lt; composed_op_type &amp;gt;(std::move(op)))
    {
    }

    shared_composed_op(std::shared_ptr&amp;lt; composed_op_type &amp;gt; op)
    : impl_(std::move(op))
    {
    }

    void initial_resume() { impl_-&amp;gt;impl_(*this); }

    template &amp;lt; class... Args &amp;gt;
    void operator()(Args &amp;amp;&amp;amp;... args)
    {
        if (impl_-&amp;gt;invocations_ &amp;lt; ~unsigned(0))
        {
            ++impl_-&amp;gt;invocations_;
            impl_-&amp;gt;impl_(*this, std::forward&amp;lt; Args &amp;gt;(args)...);
        }
    }

    template &amp;lt; class... Args &amp;gt;
    void complete(Args &amp;amp;&amp;amp;... args)
    {
        impl_-&amp;gt;complete(std::forward&amp;lt; Args &amp;gt;(args)...);
    }

    auto get_allocator() const -&amp;gt; allocator_type { return impl_-&amp;gt;get_allocator(); }
    auto get_executor() const -&amp;gt; executor_type { return impl_-&amp;gt;get_executor(); }

    std::shared_ptr&amp;lt; composed_op_type &amp;gt; impl_;
};

template &amp;lt; class Impl, class Work, class Handler, class Signature &amp;gt;
auto share(boost::asio::detail::composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; &amp;amp;composed_op)
    -&amp;gt; shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt;
{
    auto op = shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt;(std::move(composed_op));
    op.initial_resume();
    return op;
}

template &amp;lt; class Impl, class Work, class Handler, class Signature &amp;gt;
auto share(shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; shared_thing)
    -&amp;gt; shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt;
{
    return shared_thing;
}

template &amp;lt; typename Impl, typename Work, typename Handler, typename Signature &amp;gt;
inline void *asio_handler_allocate(std::size_t size, shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler)
{
    return boost_asio_handler_alloc_helpers::allocate(size, this_handler-&amp;gt;impl_-&amp;gt;handler_);
}

template &amp;lt; typename Impl, typename Work, typename Handler, typename Signature &amp;gt;
inline void asio_handler_deallocate(void *                                                pointer,
                                    std::size_t                                           size,
                                    shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler)
{
    boost_asio_handler_alloc_helpers::deallocate(pointer, size, this_handler-&amp;gt;impl_-&amp;gt;handler_);
}

template &amp;lt; typename Impl, typename Work, typename Handler, typename Signature &amp;gt;
inline bool asio_handler_is_continuation(shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler)
{
    return asio_handler_is_continuation(this_handler-&amp;gt;impl_.get());
}

template &amp;lt; typename Function, typename Impl, typename Work, typename Handler, typename Signature &amp;gt;
inline void asio_handler_invoke(Function &amp;amp;function, shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler)
{
    boost_asio_handler_invoke_helpers::invoke(function, this_handler-&amp;gt;impl_-&amp;gt;handler_);
}

template &amp;lt; typename Function, typename Impl, typename Work, typename Handler, typename Signature &amp;gt;
inline void asio_handler_invoke(const Function &amp;amp;                                      function,
                                shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler)
{
    boost_asio_handler_invoke_helpers::invoke(function, this_handler-&amp;gt;impl_-&amp;gt;handler_);
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that in hand, and with a little more &lt;em&gt;jiggery pokery&lt;/em&gt;, I was able to express intent thus:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template &amp;lt; class Self &amp;gt;
    void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t bytes_transferred = 0)
    {
...
        auto &amp;amp;state = *state_;

        reenter(this)
        {
            ...

            // here's the interesting bit - self becomes a copyable handle to itself
            yield share(self);

            // deduce the port
            yield
            {
                this-&amp;gt;initiate_resolve(share(self), state.uri.hostname(), deduce_http_service(state.uri));
                this-&amp;gt;initiate_timout(share(self), state.session_.resolve_timeout());
            }

            while (this-&amp;gt;resolving() || this-&amp;gt;timeout_outstanding())
                yield;

            if (this-&amp;gt;error)
                goto finish;

            // connect the socket

            state.current_resolve_result = this-&amp;gt;resolved_endpoints().begin();
            while (state.current_resolve_result != this-&amp;gt;resolved_endpoints().end())
            {
                state.tcp_stream().expires_after(state.session_.connect_timeout());
                yield state.tcp_stream().async_connect(state.current_resolve_result-&amp;gt;endpoint(), share(self));
                log(&quot;Connect to: &quot;, state.current_resolve_result-&amp;gt;endpoint(), &quot; result: &quot;, ec);
                // if the connect is successful, we can exit the loop early.
                if (!ec)
                    goto connected;
                ++state.current_resolve_result;
            }
            // if we leave the loop, make sure there is an error of some kind
            this-&amp;gt;set_error(ec);
            goto finish;

        connected:

            ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The full code can be seen &lt;a href=&quot;https://github.com/madmongo1/webclient/blob/develop/include/boost/webclient/asio/get_op.hpp&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There are a couple of interesting things to note:&lt;/p&gt;

&lt;p&gt;If you start two or more async operations that will complete on the same object, they must all be allowed to complete.
This is why we yield and wait for both the socket and the timeout:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;            while (this-&amp;gt;resolving() || this-&amp;gt;timeout_outstanding())
                yield;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This leads directly to the problem of managing the error_code. Two error_codes will be produced - one for the timer
(which we hope to cancel before it times out) and one for the resolve operation.
This means we have to store the first relevant error code somewhere:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;/// @brief a mixin to manage overall operation error state
struct has_error_code
{
    auto set_error(error_code const &amp;amp;ec) -&amp;gt; error_code &amp;amp;
    {
        if (!error)
        {
            if (ec &amp;amp;&amp;amp; ec != net::error::operation_aborted)
                error = ec;
        }
        return error;
    }

    error_code error;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we need a means of allowing communication between the timeout timer and the resolver:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template &amp;lt; class Self &amp;gt;
    void initiate_resolve(Self self, std::string const &amp;amp;host, std::string const &amp;amp;service)
    {
        results_.reset();
        resolver_.async_resolve(host, service, std::move(self));
    }

    template &amp;lt; class Self &amp;gt;
    void operator()(Self &amp;amp;self, error_code ec, resolver_type::results_type results)
    {
        results_.emplace(std::move(results));

        auto &amp;amp;this_ = *static_cast&amp;lt; Derived * &amp;gt;(this);
        this_.on_resolved(ec);

        auto &amp;amp;has_err = static_cast&amp;lt; has_error_code &amp;amp; &amp;gt;(this_);
        this_(self, has_err.set_error(ec));
    }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One cancels the other….&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void on_timeout()
    {
        this-&amp;gt;cancel_resolver();
        log(&quot;Timeout&quot;);
    }

    void on_resolved(error_code const &amp;amp;ec)
    {
        this-&amp;gt;cancel_timeout();
        log(&quot;Resolve complete: &quot;, ec);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    auto resolving() const -&amp;gt; bool { return !results_.has_value(); }

    auto cancel_resolver() -&amp;gt; void { resolver_.cancel(); }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end I was unsure how much is gained, other than pretty code (which does have value in itself).&lt;/p&gt;

&lt;h1 id=&quot;unified-webclient&quot;&gt;Unified WebClient&lt;/h1&gt;

&lt;p&gt;Exploratory work started on the unified web client. After some discussion, Vinnie and I agreed on the following design
decisions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Interface to model closely the very popular Python Requests module.&lt;/li&gt;
  &lt;li&gt;Sync and Async modes available.&lt;/li&gt;
  &lt;li&gt;Homogenous (mostly non-template) interface, behind which system-specific implementations can reside.&lt;/li&gt;
  &lt;li&gt;Where native library support is available, that will be used,&lt;/li&gt;
  &lt;li&gt;Where not, internally the library will be implemented in Asio/Beast.&lt;/li&gt;
  &lt;li&gt;Coroutine friendly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once more progress has been made on the Boost.Beast issue tracker, I will be focusing attention here.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Boost 1.73 Released and other Matters The 1.73.0 release of Boost took up more attention than I had anticipated, but in the end all seemed to go well. Since then I’ve been working through the issues list on GitHub and am now starting to make some headway. I cam across a few other interesting (to me) topics this month. (Possibly) Interesting Asio Things Last month I asked the question, “Is it possible to write an asynchronous composed operation entirely as a lambda?”. This month I went a little further with two items that interested me. The first is whether asio’s async_compose can be adapted so that we can implement a complex composed operation involving more than one IO object easily using the asio faux coroutine mechanism. The second was whether is was possible to easily implement an async future in Asio. Async Asio Future Here is my motivating use case: auto p = async::promise&amp;lt;std::string&amp;gt;(); auto f = p.get_future(); // long-running process starts which will yield a string start_something(std::move(p)); // wait on the future f.async_wait([](some_result_type x) { // use the x }); // or auto str = co_await f.async_wait(net::use_awaitable); // or shorthand auto str = co_await f(); The salient points here are: no matter on which thread the promise is fulfilled, the future will complete on the associated executor of the handler passed to async_wait Ideally the promise/future should not make use of mutexes un-necessarily. (problematic for ASIO) It must work with objects that are not default-constructable. In the end, I didn’t achieve the second goal as this was not a priority project, but I would be interested to see if anyone can improve on the design. The source code is here I tried a couple of ways around the non-default-constructable requirement. My first was to require the CompletionToken to the async_wait initiating function to be compatible with: void (error_code, std::optional&amp;lt;T&amp;gt;) But I felt this was unwieldy. Then I remembered Boost.Outcome. I have been looking for a use for this library for some time. It turns out that you can legally write an ASIO composed operation who’s handler takes a single argument of any type, and this will translate cleanly when used with net::use_future, net::use_awaitable etc. A default Boost.Outcome object almost fits the bill, except that its exception_ptr type is boost rather than standard. This is easily solved with a typedef: template&amp;lt;class T&amp;gt; using myoutcome = boost::outcome2::basic_outcome&amp;lt;T, error:code, std::exception_ptr&amp;gt;; I was feeling please with myself for figuring this out, until I came to test code code under C++11… and realised that Boost.Outcome is only compatible with C++14 or higher. So in the end, I cobbled together a ‘good enough’ version of outcome using a variant: template &amp;lt; class T &amp;gt; struct outcome { outcome(T arg) : var_(std::move(arg)) {} outcome(error_code const&amp;amp; arg) : var_(arg) {} outcome(std::exception_ptr const&amp;amp; arg) : var_(arg) {} auto has_value() const -&amp;gt; bool { return polyfill::holds_alternative&amp;lt; T &amp;gt;(var_); } auto has_error() const -&amp;gt; bool { return polyfill::holds_alternative&amp;lt; error_code &amp;gt;(var_); } auto has_exception() const -&amp;gt; bool { return polyfill::holds_alternative&amp;lt; std::exception_ptr &amp;gt;(var_); } auto value() &amp;amp; -&amp;gt; T &amp;amp;; auto value() &amp;amp;&amp;amp; -&amp;gt; T &amp;amp;&amp;amp;; auto value() const &amp;amp; -&amp;gt; T const &amp;amp;; auto error() const -&amp;gt; error_code const &amp;amp;; using variant_type = polyfill::variant&amp;lt; T, error_code, std::exception_ptr &amp;gt;; variant_type var_; }; The code for this is here Finally this allowed me to express intent at the call site like so: auto f = p.get_future(); f.async_wait([](outcome&amp;lt;std::string&amp;gt; os){ if (os.has_value()) // use the value else if (os.has_error()) // use the error else // deal with the exception }); The coroutine interface can be made cleaner: try { auto str = co_await f(); // use the string } catch(system_error&amp;amp; ec) { // use the error code in ec.code() } catch(...) { // probably catastrophic } For the above code to compile we’d have to add the following trivial transform: template &amp;lt; class T &amp;gt; auto future&amp;lt; T &amp;gt;::operator()() -&amp;gt; net::awaitable&amp;lt; T &amp;gt; { auto r = co_await async_wait(net::use_awaitable); if (r.has_value()) co_return std::move(r).assume_value(); else if (r.has_error()) throw system_error(r.assume_error()); else throw r.exception(); } Easy Complex Coroutines with async_compose When your composed operation’s intermediate completion handlers are invoked, the underlying detail::composed_op provides a mutable reference to itself. A typical completion handler looks like this: template&amp;lt;class Self&amp;gt; void operator()(Self&amp;amp; self, error_code ec = {} , std::size_t bytes_transferred = 0) { reenter(this) { // yields and operations on Self yield async_write(sock, buf, std::move(self)); // note that self is moved } } What I wanted was a composed operation where the following is legal: template&amp;lt;class Self&amp;gt; void operator()(Self self /* note copy */, error_code ec = {} , std::size_t bytes_transferred = 0) { reenter(this) { // yields and operations on Self yield { async_write(sock, buf, self); timer.async_wait(self); writing = true; sending = true; } while(writing || sending) yield // something needs to happen here to reset the flags and handle errors and cancellation. ; } } Which I think looks reasonably clear and easy to follow. In this work I had to overcome two problems - writing the framework to allow it, and thinking of a maintainable way to express intent in the interrelationships between the asynchronous operations on the timer and the socket. Solving the copyable composed_op problem was easy. I did what I always do in situations like this. I cheated. asio::async_compose produces a specialisation of a detail::composed_op&amp;lt;&amp;gt; template. Substituting a disregard of the rules for knowledge and skill, I simply reached into the guts of asio and produced a copyable wrapper to this class. I also cut/pasted some ancillary free functions in order to make asio work nicely with my new class: Here’s the code… it’s not pretty: template &amp;lt; class Impl, class Work, class Handler, class Signature &amp;gt; struct shared_composed_op { using composed_op_type = boost::asio::detail::composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt;; using allocator_type = typename net::associated_allocator&amp;lt; composed_op_type &amp;gt;::type; using executor_type = typename net::associated_executor&amp;lt; composed_op_type &amp;gt;::type; shared_composed_op(composed_op_type &amp;amp;&amp;amp;op) : impl_(std::make_shared&amp;lt; composed_op_type &amp;gt;(std::move(op))) { } shared_composed_op(std::shared_ptr&amp;lt; composed_op_type &amp;gt; op) : impl_(std::move(op)) { } void initial_resume() { impl_-&amp;gt;impl_(*this); } template &amp;lt; class... Args &amp;gt; void operator()(Args &amp;amp;&amp;amp;... args) { if (impl_-&amp;gt;invocations_ &amp;lt; ~unsigned(0)) { ++impl_-&amp;gt;invocations_; impl_-&amp;gt;impl_(*this, std::forward&amp;lt; Args &amp;gt;(args)...); } } template &amp;lt; class... Args &amp;gt; void complete(Args &amp;amp;&amp;amp;... args) { impl_-&amp;gt;complete(std::forward&amp;lt; Args &amp;gt;(args)...); } auto get_allocator() const -&amp;gt; allocator_type { return impl_-&amp;gt;get_allocator(); } auto get_executor() const -&amp;gt; executor_type { return impl_-&amp;gt;get_executor(); } std::shared_ptr&amp;lt; composed_op_type &amp;gt; impl_; }; template &amp;lt; class Impl, class Work, class Handler, class Signature &amp;gt; auto share(boost::asio::detail::composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; &amp;amp;composed_op) -&amp;gt; shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; { auto op = shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt;(std::move(composed_op)); op.initial_resume(); return op; } template &amp;lt; class Impl, class Work, class Handler, class Signature &amp;gt; auto share(shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; shared_thing) -&amp;gt; shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; { return shared_thing; } template &amp;lt; typename Impl, typename Work, typename Handler, typename Signature &amp;gt; inline void *asio_handler_allocate(std::size_t size, shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler) { return boost_asio_handler_alloc_helpers::allocate(size, this_handler-&amp;gt;impl_-&amp;gt;handler_); } template &amp;lt; typename Impl, typename Work, typename Handler, typename Signature &amp;gt; inline void asio_handler_deallocate(void * pointer, std::size_t size, shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler) { boost_asio_handler_alloc_helpers::deallocate(pointer, size, this_handler-&amp;gt;impl_-&amp;gt;handler_); } template &amp;lt; typename Impl, typename Work, typename Handler, typename Signature &amp;gt; inline bool asio_handler_is_continuation(shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler) { return asio_handler_is_continuation(this_handler-&amp;gt;impl_.get()); } template &amp;lt; typename Function, typename Impl, typename Work, typename Handler, typename Signature &amp;gt; inline void asio_handler_invoke(Function &amp;amp;function, shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler) { boost_asio_handler_invoke_helpers::invoke(function, this_handler-&amp;gt;impl_-&amp;gt;handler_); } template &amp;lt; typename Function, typename Impl, typename Work, typename Handler, typename Signature &amp;gt; inline void asio_handler_invoke(const Function &amp;amp; function, shared_composed_op&amp;lt; Impl, Work, Handler, Signature &amp;gt; *this_handler) { boost_asio_handler_invoke_helpers::invoke(function, this_handler-&amp;gt;impl_-&amp;gt;handler_); } With that in hand, and with a little more jiggery pokery, I was able to express intent thus: template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t bytes_transferred = 0) { ... auto &amp;amp;state = *state_; reenter(this) { ... // here's the interesting bit - self becomes a copyable handle to itself yield share(self); // deduce the port yield { this-&amp;gt;initiate_resolve(share(self), state.uri.hostname(), deduce_http_service(state.uri)); this-&amp;gt;initiate_timout(share(self), state.session_.resolve_timeout()); } while (this-&amp;gt;resolving() || this-&amp;gt;timeout_outstanding()) yield; if (this-&amp;gt;error) goto finish; // connect the socket state.current_resolve_result = this-&amp;gt;resolved_endpoints().begin(); while (state.current_resolve_result != this-&amp;gt;resolved_endpoints().end()) { state.tcp_stream().expires_after(state.session_.connect_timeout()); yield state.tcp_stream().async_connect(state.current_resolve_result-&amp;gt;endpoint(), share(self)); log(&quot;Connect to: &quot;, state.current_resolve_result-&amp;gt;endpoint(), &quot; result: &quot;, ec); // if the connect is successful, we can exit the loop early. if (!ec) goto connected; ++state.current_resolve_result; } // if we leave the loop, make sure there is an error of some kind this-&amp;gt;set_error(ec); goto finish; connected: ... The full code can be seen here There are a couple of interesting things to note: If you start two or more async operations that will complete on the same object, they must all be allowed to complete. This is why we yield and wait for both the socket and the timeout: while (this-&amp;gt;resolving() || this-&amp;gt;timeout_outstanding()) yield; This leads directly to the problem of managing the error_code. Two error_codes will be produced - one for the timer (which we hope to cancel before it times out) and one for the resolve operation. This means we have to store the first relevant error code somewhere: /// @brief a mixin to manage overall operation error state struct has_error_code { auto set_error(error_code const &amp;amp;ec) -&amp;gt; error_code &amp;amp; { if (!error) { if (ec &amp;amp;&amp;amp; ec != net::error::operation_aborted) error = ec; } return error; } error_code error; }; And we need a means of allowing communication between the timeout timer and the resolver: template &amp;lt; class Self &amp;gt; void initiate_resolve(Self self, std::string const &amp;amp;host, std::string const &amp;amp;service) { results_.reset(); resolver_.async_resolve(host, service, std::move(self)); } template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec, resolver_type::results_type results) { results_.emplace(std::move(results)); auto &amp;amp;this_ = *static_cast&amp;lt; Derived * &amp;gt;(this); this_.on_resolved(ec); auto &amp;amp;has_err = static_cast&amp;lt; has_error_code &amp;amp; &amp;gt;(this_); this_(self, has_err.set_error(ec)); } One cancels the other…. void on_timeout() { this-&amp;gt;cancel_resolver(); log(&quot;Timeout&quot;); } void on_resolved(error_code const &amp;amp;ec) { this-&amp;gt;cancel_timeout(); log(&quot;Resolve complete: &quot;, ec); } auto resolving() const -&amp;gt; bool { return !results_.has_value(); } auto cancel_resolver() -&amp;gt; void { resolver_.cancel(); } In the end I was unsure how much is gained, other than pretty code (which does have value in itself). Unified WebClient Exploratory work started on the unified web client. After some discussion, Vinnie and I agreed on the following design decisions: Interface to model closely the very popular Python Requests module. Sync and Async modes available. Homogenous (mostly non-template) interface, behind which system-specific implementations can reside. Where native library support is available, that will be used, Where not, internally the library will be implemented in Asio/Beast. Coroutine friendly. Once more progress has been made on the Boost.Beast issue tracker, I will be focusing attention here.</summary></entry><entry><title type="html">New Boost Release</title><link href="http://cppalliance.org/company/2020/04/28/New-Boost-Release.html" rel="alternate" type="text/html" title="New Boost Release" /><published>2020-04-28T00:00:00+00:00</published><updated>2020-04-28T00:00:00+00:00</updated><id>http://cppalliance.org/company/2020/04/28/New-Boost-Release</id><content type="html" xml:base="http://cppalliance.org/company/2020/04/28/New-Boost-Release.html">&lt;h1 id=&quot;boost-173&quot;&gt;Boost 1.73&lt;/h1&gt;

&lt;h2 id=&quot;new-library&quot;&gt;New Library&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.boost.org/doc/libs/1_73_0/libs/static_string/doc/html/index.html&quot;&gt;StaticString:&lt;/a&gt;
A dynamically resizable string of characters with compile-time fixed capacity and contiguous embedded storage, from Krystian Stasiowski and Vinnie Falco.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.boost.org/users/history/version_1_73_0.html&quot;&gt;Boost Release Notes&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="company" /><summary type="html">Boost 1.73 New Library StaticString: A dynamically resizable string of characters with compile-time fixed capacity and contiguous embedded storage, from Krystian Stasiowski and Vinnie Falco. Boost Release Notes</summary></entry><entry><title type="html">Krystian’s March Update</title><link href="http://cppalliance.org/krystian/2020/04/07/KrystiansMarchUpdate.html" rel="alternate" type="text/html" title="Krystian’s March Update" /><published>2020-04-07T00:00:00+00:00</published><updated>2020-04-07T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/04/07/KrystiansMarchUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/04/07/KrystiansMarchUpdate.html">&lt;h1 id=&quot;the-rundown&quot;&gt;The Rundown&lt;/h1&gt;

&lt;p&gt;Due to the COVID-19 pandemic, my classes have been moved online. It’s certainly an interesting way to teach, but admittedly I can’t say it’s enjoyable or effective. However, it has given me a lot of time to work on various projects, which is a reasonable trade-off (at least in my opinion). I got quite a bit done this month due to the substantial increase in leisure time, and was able to work on several projects that previously didn’t fit into my schedule.&lt;/p&gt;

&lt;h1 id=&quot;booststaticstring&quot;&gt;Boost.StaticString&lt;/h1&gt;

&lt;p&gt;I spent the first few days of March putting the finishing touches on Boost.StaticString in preparation for the release of Boost 1.73.0, mostly consisting of housekeeping tasks, but also some bug fixes for certain compiler configurations. In particular, a problem arose with GCC 5 regarding its &lt;code&gt;constexpr&lt;/code&gt; support, two of which impede using &lt;code&gt;basic_static_string&lt;/code&gt; during constant evaluation: &lt;code&gt;throw&lt;/code&gt; expressions, and non-static member functions whose return type is the class they are a member of. With respect to the former, consider the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr int throw_not_evaluated(bool flag)
{
	if (flag)
		throw 1;
	return 0;
}

constexpr int const_eval = throw_not_evaluated(false);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://godbolt.org/z/CEuEvr&quot;&gt;View this on Godbolt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is helpful to first establish what the standard has to say regarding the above example. Looking at &lt;a href=&quot;https://timsong-cpp.github.io/cppwp/n4140/dcl.constexpr#3&quot;&gt;[dcl.constexpr] p3&lt;/a&gt;, we see that &lt;code&gt;throw_not_evaluated&lt;/code&gt; contains no constructs that are explicitly prohibited from appearing within a &lt;code&gt;constexpr&lt;/code&gt; function in all contexts. Now taking a look at &lt;a href=&quot;https://timsong-cpp.github.io/cppwp/n4140/expr.const#2&quot;&gt;[expr.const] p2&lt;/a&gt; we see:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A &lt;em&gt;conditional-expression&lt;/em&gt; &lt;code&gt;e&lt;/code&gt; is a core constant expression unless the evaluation of &lt;code&gt;e&lt;/code&gt;, following the rules of the abstract machine, would evaluate one of the following expressions:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;[…] a &lt;em&gt;throw-expression&lt;/em&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Boiling down the standardese, this effectively says that &lt;code&gt;throw_not_evaluated(false)&lt;/code&gt; is a constant expression unless, when evaluated, it would evaluate&lt;code&gt;throw 1&lt;/code&gt;. This would not occur, meaning that  &lt;code&gt;throw_not_evaluated(false)&lt;/code&gt; is indeed a constant expression, and we can use it to initialize &lt;code&gt;const_eval&lt;/code&gt;. Clang, later versions of GCC, and MSVC all agree on this and compile it without any complaints. However, GCC 5 with the &lt;code&gt;-std=c++14&lt;/code&gt; flag fails to do so, citing:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;error: expression ‘&amp;lt;&lt;strong&gt;throw-expression&lt;/strong&gt;&amp;gt;’ is not a constant-expression&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sounds excusable to me! GCC 5.1 was released in 2015 after all, so you can’t expect every feature to be implemented less than a year after C++14 was finalized, right? It all sounded sane to me, but before going ahead and disabling &lt;code&gt;constexpr&lt;/code&gt; for functions that were potentially-throwing, I decided to try a small variation of the original:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct magic
{
    constexpr magic(bool flag)
    {
        if (flag)
            throw 1;
        return;
    }

    constexpr operator int() { return 1; }
};

constexpr int magic_eval = magic(false);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://godbolt.org/z/fXQxDT&quot;&gt;View this on Godbolt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It miraculously works. Further, if we construct a &lt;code&gt;magic&lt;/code&gt; object within a constexpr function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr int throw_not_evaluated(bool flag)
{
	return magic(flag);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://godbolt.org/z/qMAkiQ&quot;&gt;View this on Godbolt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gathering the remnants of my sanity, I lifted the &lt;code&gt;BOOST_STATIC_STRING_THROW_IF&lt;/code&gt; macros out of all the potentially-throwing functions and replaced them with a class template (for GCC 5) or function template (for all other compilers) that can be constructed/called with the &lt;code&gt;throw_exception&amp;lt;ExceptionType&amp;gt;(message)&lt;/code&gt; syntax. I also considered adding a &lt;code&gt;throw_exception_if&lt;/code&gt; variation to hide the &lt;code&gt;if&lt;/code&gt; statement within shorter functions, but on advisement from Vinnie Falco and Peter Dimov, I didn’t end up doing this to allow for better optimization.&lt;/p&gt;

&lt;p&gt;Moving on to a simpler GCC 5 issue (but significantly more annoying to diagnose), I discovered that &lt;code&gt;substr&lt;/code&gt; was causes an ICE during constant evaluation. Took some time to get to the bottom of it, but I eventually figured out that this was because the return type of &lt;code&gt;substr&lt;/code&gt; is &lt;code&gt;basic_static_string&lt;/code&gt;. Unfortunately, the only remedy for this was to disable &lt;code&gt;constexpr&lt;/code&gt; for the function when GCC 5 is used.&lt;/p&gt;

&lt;p&gt;Save for these two issues, the rest of the work that had to be done for StaticString was smooth sailing, mostly improvements to the coverage of the &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;replace&lt;/code&gt; overloads that take input iterators. In the future I plan to do an overhaul of the documentation, but as of now it’s ready for release, so I’m excited to finally get this project out into the wild.&lt;/p&gt;

&lt;h1 id=&quot;boostjson&quot;&gt;Boost.JSON&lt;/h1&gt;

&lt;p&gt;Most of the time I spent on Boost.JSON in the past month was spent learning the interface and internals of the library, as I will be writing documentation on many of the components in the future. My primary focus was on the &lt;code&gt;value_ref&lt;/code&gt; class, which is used as the value type of initializer lists used to represent JSON documents within source code. The reason that this is used instead of &lt;code&gt;value&lt;/code&gt; is because &lt;code&gt;std::initializer_list&lt;/code&gt; suffers from one fatal flaw: the underlying array that it refers to is &lt;code&gt;const&lt;/code&gt;, which means you cannot move its elements. Copying a large JSON document is not trivial, so &lt;code&gt;value_ref&lt;/code&gt; is used as a proxy referring to an underlying object that can be moved by an operation using an initializer list. This is achieved by storing a pointer to a function that will appropriately copy/move construct a &lt;code&gt;value&lt;/code&gt; when called requested by the target.&lt;/p&gt;

&lt;p&gt;While looking at how &lt;code&gt;value_ref&lt;/code&gt; works, I went ahead and added support for moving from a &lt;code&gt;json::string&lt;/code&gt;, since up to that point all strings were stored as &lt;code&gt;string_view&lt;/code&gt; internally, thus precluding the ability to move from a &lt;code&gt;value_ref&lt;/code&gt; constructed from a &lt;code&gt;json::string&lt;/code&gt;. There also was a bug caused by how &lt;code&gt;value_ref&lt;/code&gt; handles construction from rvalues, in part due to the unintuitive nature of type deduction. Consider the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct value_ref
{
	template&amp;lt;typename T&amp;gt;
	value_ref(const T&amp;amp;) { ... }

	template&amp;lt;typename T&amp;gt;
	value_ref(T&amp;amp;&amp;amp;) { ... }
};

json::value jv;
const json::value const_jv;
value_ref rvalue = std::move(jv); // #1
value_ref const_rvalue = std::move(const_jv); // #2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both &lt;code&gt;#1&lt;/code&gt; and &lt;code&gt;#2&lt;/code&gt;, the constructor &lt;code&gt;value_ref(T&amp;amp;&amp;amp;)&lt;/code&gt; is called. This certainly makes sense once you consider the deduction that is performed, however, by glancing at just the declarations themselves, it isn’t obvious, as we’ve all been taught that references to non-const types will not bind to a const object. Where this becomes a problem for &lt;code&gt;value_ref&lt;/code&gt; is that the constructor taking a &lt;code&gt;T&amp;amp;&amp;amp;&lt;/code&gt; parameter expects to be able to move from the parameter, so it internally stores a non-const &lt;code&gt;void*&lt;/code&gt;. Converting a “pointer to &lt;code&gt;const&lt;/code&gt; &lt;code&gt;T&lt;/code&gt;” to &lt;code&gt;void*&lt;/code&gt; isn’t permitted, so you get a hard error. The fix for this was fairly trivial.&lt;/p&gt;

&lt;h1 id=&quot;standardization&quot;&gt;Standardization&lt;/h1&gt;

&lt;p&gt;Most of my time this month was spent working on standardization related activities, which can be broken up into two somewhat separate categories:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fixing existing wording&lt;/li&gt;
  &lt;li&gt;Working on papers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;editorial-issues&quot;&gt;Editorial issues&lt;/h2&gt;

&lt;p&gt;I submitted a fair few pull requests to the &lt;a href=&quot;https://github.com/cplusplus/draft&quot;&gt;draft repository&lt;/a&gt;, most of which were general wording cleanups and improvements to the consistency of the wording. With respect to wording consistency, I targeted instances of incorrect cv-qualification notation, definitions of terms within notes, cross-references that name the wrong subclause or are self-referential, and redundant normative wording. These kinds of issues are all over the standard, but I generally stay away from the library wording unless it’s absolutely necessary since it has a subtitle difference in the wording style compared to that of the core language. I ended up writing atool to make grepping for these issues a little easier, which is certainly an improvement over manually inspecting each TeX source.&lt;/p&gt;

&lt;p&gt;The wording cleanups are the most time consuming, but also are the ones I find the most enjoyable. They all follow the same principle of rephrasing an idea with more accurate wording while not changing the actual meaning – something that often proves to be a challenge. These generally start off as small issues I notice in the text, but then snowball into complete rewrites to make the whole thing consistent. Anyways, if it sparks your interest you can find the various editorial fixes I worked on &lt;a href=&quot;https://github.com/cplusplus/draft/pulls?q=is%3Apr+author%3Asdkrystian&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;p1997&quot;&gt;P1997&lt;/h2&gt;

&lt;p&gt;Of my papers, &lt;a href=&quot;http://wg21.link/p1997&quot;&gt;P1997&lt;/a&gt; is the one that I’m putting the most time into. In short, it proposes to make arrays more “regular”, allowing assignment, initialization from other arrays, array placeholder types, and many more features we bestow upon scalar and class types. The holy grail of changes (not proposed in the paper) would be to allow the passing of arrays by value &lt;em&gt;without&lt;/em&gt; decaying to a pointer: fully cementing arrays as first-class types. Unlike the other proposed changes this isn’t a pure extension, so to make it remotely feasible existing declarations of parameters with array type (for the sake of brevity we will abbreviate them as PAT) would have to be deprecated in C++23, removed in C++26, and then reinstated in C++29. For this reason, we are undertaking this as an entirely different paper to allow for the pure extensions to be added in C++23, leaving the removal and reinstatement on the backburner.&lt;/p&gt;

&lt;p&gt;In order to bring convincing evidence that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The current semantics for PAT are unintuitive and merely syntactic sugar.&lt;/li&gt;
  &lt;li&gt;The deprecation of PAT would not be significantly disruptive to existing codebases.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Ted (the co-author) and I decided to see exactly how often PAT appear within normal C++ code. While &lt;a href=&quot;https://codesearch.isocpp.org/&quot;&gt;codesearch.isocpp.org&lt;/a&gt; by Andrew Tomazos is a fantastic tool, the search we wanted to conduct was simply not possible with his tool, so we set out to create our own. We needed two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A dataset&lt;/li&gt;
  &lt;li&gt;Some tool to parse all the source files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the dataset, I wrote a tool to clone the top 4000 C++ Github repositories, and clean out all files that weren’t C++ source files (.cpp, .cxx, .cc, .h, .hpp, .hxx, .ipp, .tpp). Github has a nice API to search for repositories, but it unfortunately limits the results for a single search query to 1000 results, but since I was sorting them by start count, I was able to use it as an index to begin a new search query. After accidentally sending my Github credentials to Ted once and waiting 10 hours for all the repositories to be cloned, we had our dataset: 2.7 million source files, totaling around 32GB.&lt;/p&gt;

&lt;p&gt;To parse all these files, we opted to use the Clang frontend. Its AST matcher was perfectly suited to the task, so it was just a matter of forming the correct matchers for the three contexts a PAT can appear in (function parameter, non-type template parameter, and &lt;code&gt;catch&lt;/code&gt; clause parameter). All the files were parsed in single file mode, since opening and expanding &lt;code&gt;#include&lt;/code&gt; directives would make the processing of the files take exponentially longer. Forming the correct match pattern proved to be the most difficult part, as the syntax is not entirely intuitive and often times they simply didn’t find every PAT in our test cases.&lt;/p&gt;

&lt;p&gt;Doing a single file parse along with wanting to find every PAT possible &lt;em&gt;and&lt;/em&gt; suppressing all diagnostics landed us in the land of gotchas. To start, the &lt;code&gt;arrayType&lt;/code&gt; matcher only would match declarations that had an array declarator present, i.e. &lt;code&gt;int a[]&lt;/code&gt; would be found but &lt;code&gt;T a&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; names an array type would not. Eventually I found &lt;code&gt;decayedType&lt;/code&gt;, which did exactly what we wanted, so long as we filtered out every result that was a function pointer. This worked for function parameters and non-type template parameters, but not &lt;code&gt;catch&lt;/code&gt; clause parameters. In the Clang AST, &lt;code&gt;catch&lt;/code&gt; is categorized as a statement that encloses a variable declaration whose type is not considered to be decayed (as far as I could see) so we could only match parameters that used an array declarator. I don’t expect anyone to actually declare PATs in a &lt;code&gt;catch&lt;/code&gt; clause, and after running the tool on the dataset exactly zero instances were found, so this is most likely a non-issue.&lt;/p&gt;

&lt;p&gt;Single file parsing introduced a number of issues all stemming from the fact that none of the &lt;code&gt;#include&lt;/code&gt; directives were processed, meaning that there was a large number of types that were unresolved. Consider the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;not_processed.h&amp;gt;

using array = unresolved[2];
using array_ref = unk_array&amp;amp;;
array_ref ref = {unresolved(), unresolved()};

void f(decltype(ref) param);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For reasons that I don’t know, since &lt;code&gt;unresolved&lt;/code&gt; has no visible declaration Clang reports that &lt;code&gt;param&lt;/code&gt; has a decayed type. I suspect this is because diagnostics are suppressed and some recursive procedure used in the determination of the type named by  &lt;code&gt;array_ref&lt;/code&gt; returns upon encountering an error at the declaration of &lt;code&gt;array&lt;/code&gt; and simply returns &lt;code&gt;unresolved[2]&lt;/code&gt; as the type. If you know why this happens, don’t hesitate to ping me! I ended up tracking the number of PAT declarations that use an array declarator separately since I suspect that this number may end up being more accurate.&lt;/p&gt;

&lt;p&gt;Once the tool was ready to go and we started to run it on the dataset, we encountered issues of the worst kind: assertion failures. I suppose such errors could be expected when abusing a compiler to the extent that we did, but they weren’t particularly enjoyable to fix. I should mention that tool itself is meant to be run on a directory, so once an assertion failed, it would end the entire run on that directory. My initial solution to this was changing the problematic asserts to throw an exception, but the number of failures was ever-growing. Creating a new &lt;code&gt;CompilerInstance&lt;/code&gt; for each file did somewhat remedy the situation, but didn’t fix it all. Eventually, we called it good enough and let it run over the entire dataset. Clang itself was in our dataset, with a nasty little surprise taking the form of infinitely recursive template instantiations and debug pragmas that would crash the compiler. Clang relies on the diagnostics to signal when the recursive instantiation limit is reached, but since those were disabled the thread would never terminate. Evil.&lt;/p&gt;

&lt;p&gt;Once the paper is finished, I’ll report the results in a future post.&lt;/p&gt;

&lt;h2 id=&quot;implicit-destruction&quot;&gt;Implicit destruction&lt;/h2&gt;

&lt;p&gt;This paper intends to substantially overhaul the wording that describes the interaction between objects and their storage. I originally brought this issue up several months back with &lt;a href=&quot;https://github.com/cplusplus/draft/pull/2872&quot;&gt;this pull request&lt;/a&gt; in an attempt to fix it editorially, but it was deemed too large in scope. I finally got started on the paper and drafted up a direction to take, which will hopefully resolve the shortfalls of our current wording.&lt;/p&gt;

&lt;p&gt;This problem stems from the notion that objects don’t exist before their lifetime has started and after it has ended. This allows compilers to make a lot of assumptions and consequently leads to more optimized code, but the manner in which the wording was applied has severely crippled our ability to refer to “not-objects”. We want to be able to place restrictions on storage that an object used to occupy, but simply have no way for doing so. Thus, the direction I plan to take is to define the semantics of storage, allowing us to place restrictions on that storage even if no object exists within it. I don’t have too many of the core definitions completed yet as they require the most time to make them robust, but once that is hashed out, applying it where needed should be smooth sailing.&lt;/p&gt;

&lt;p&gt;Here is a short list of the main changes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Define what a &lt;em&gt;region of storage&lt;/em&gt; is, specify when it is acquired, released, and what kinds of objects may occupy it.&lt;/li&gt;
  &lt;li&gt;Remove the storage duration property from objects, and effectively make that agnostic of the storage they occupy. Instead, associate storage duration with a variable. Dynamic storage duration can removed since such storage isn’t associated with a variable.&lt;/li&gt;
  &lt;li&gt;Specify when &lt;em&gt;reuse&lt;/em&gt; of storage occurs, and its effects upon the objects within that storage.&lt;/li&gt;
  &lt;li&gt;Properly specify when pointers and expressions refer to storage while preserving the notion that they refer to objects (or functions).&lt;/li&gt;
  &lt;li&gt;Specify that when control passes through the definition of a variable, storage is acquired, objects are created, and initialization is performed. Likewise, specify that exit from a scope, program, or thread causes objects within the storage to be destroyed (if any), and the storage to be released.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s a big undertaking, but I’m excited to work on this and see what kind of feedback I get. However, this paper will be more of a long term project, since it will be touching the majority of the wording in the standard. I’ll provide updates in future posts.&lt;/p&gt;

&lt;h2 id=&quot;placement-new-during-constant-evaluation&quot;&gt;Placement new during constant evaluation&lt;/h2&gt;

&lt;p&gt;A paper that I’ve been thinking about working on and finally got around to revolves around a new (heh) C++20 feature: &lt;code&gt;new&lt;/code&gt; expressions are now able to be evaluated during constant evaluation (CE). While the storage they acquire must be released during the CE and it may only call the replaceable global allocation functions, it finally allows for the use of dynamically sized containers within a constant expression.&lt;/p&gt;

&lt;p&gt;This is great! However, there is a restriction here that is completely avoidable. The function &lt;code&gt;std::construct_at&lt;/code&gt; was introduced to allow for objects to be constructed as if by placement &lt;code&gt;new&lt;/code&gt; – nice, but we don’t allow placement &lt;code&gt;new&lt;/code&gt; to be used by itself. This is because a certain implementation can’t resolve what object a &lt;code&gt;void*&lt;/code&gt; points to during CE (thank you Tim Song for the info); and because CE is intended to always yield the same results on all implementations, &lt;code&gt;construct_at&lt;/code&gt; is used to ensure the pointer type passed is always a pointer to object type. I think that &lt;em&gt;at the very least&lt;/em&gt;, congruent placement &lt;code&gt;new&lt;/code&gt; expressions should be allowed by  the principle of this being unnecessarily restrictive. As with all the other papers, I’ll post progress  updates in a future post. I’ve drafted up some wording, and I plan to have this ready sometime around June.&lt;/p&gt;

&lt;h1 id=&quot;information&quot;&gt;Information&lt;/h1&gt;
&lt;p&gt;If you want to get in touch with me, you can message me on the &lt;a href=&quot;http://slack.cpp.al/&quot;&gt;Cpplang slack&lt;/a&gt;, or &lt;a href=&quot;mailto:sdkrystian@gmail.com&quot;&gt;shoot me an email&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">The Rundown Due to the COVID-19 pandemic, my classes have been moved online. It’s certainly an interesting way to teach, but admittedly I can’t say it’s enjoyable or effective. However, it has given me a lot of time to work on various projects, which is a reasonable trade-off (at least in my opinion). I got quite a bit done this month due to the substantial increase in leisure time, and was able to work on several projects that previously didn’t fit into my schedule. Boost.StaticString I spent the first few days of March putting the finishing touches on Boost.StaticString in preparation for the release of Boost 1.73.0, mostly consisting of housekeeping tasks, but also some bug fixes for certain compiler configurations. In particular, a problem arose with GCC 5 regarding its constexpr support, two of which impede using basic_static_string during constant evaluation: throw expressions, and non-static member functions whose return type is the class they are a member of. With respect to the former, consider the following: constexpr int throw_not_evaluated(bool flag) { if (flag) throw 1; return 0; } constexpr int const_eval = throw_not_evaluated(false); View this on Godbolt It is helpful to first establish what the standard has to say regarding the above example. Looking at [dcl.constexpr] p3, we see that throw_not_evaluated contains no constructs that are explicitly prohibited from appearing within a constexpr function in all contexts. Now taking a look at [expr.const] p2 we see: A conditional-expression e is a core constant expression unless the evaluation of e, following the rules of the abstract machine, would evaluate one of the following expressions: […] a throw-expression Boiling down the standardese, this effectively says that throw_not_evaluated(false) is a constant expression unless, when evaluated, it would evaluatethrow 1. This would not occur, meaning that throw_not_evaluated(false) is indeed a constant expression, and we can use it to initialize const_eval. Clang, later versions of GCC, and MSVC all agree on this and compile it without any complaints. However, GCC 5 with the -std=c++14 flag fails to do so, citing: error: expression ‘&amp;lt;throw-expression&amp;gt;’ is not a constant-expression Sounds excusable to me! GCC 5.1 was released in 2015 after all, so you can’t expect every feature to be implemented less than a year after C++14 was finalized, right? It all sounded sane to me, but before going ahead and disabling constexpr for functions that were potentially-throwing, I decided to try a small variation of the original: struct magic { constexpr magic(bool flag) { if (flag) throw 1; return; } constexpr operator int() { return 1; } }; constexpr int magic_eval = magic(false); View this on Godbolt What? It miraculously works. Further, if we construct a magic object within a constexpr function: constexpr int throw_not_evaluated(bool flag) { return magic(flag); } View this on Godbolt What? Gathering the remnants of my sanity, I lifted the BOOST_STATIC_STRING_THROW_IF macros out of all the potentially-throwing functions and replaced them with a class template (for GCC 5) or function template (for all other compilers) that can be constructed/called with the throw_exception&amp;lt;ExceptionType&amp;gt;(message) syntax. I also considered adding a throw_exception_if variation to hide the if statement within shorter functions, but on advisement from Vinnie Falco and Peter Dimov, I didn’t end up doing this to allow for better optimization. Moving on to a simpler GCC 5 issue (but significantly more annoying to diagnose), I discovered that substr was causes an ICE during constant evaluation. Took some time to get to the bottom of it, but I eventually figured out that this was because the return type of substr is basic_static_string. Unfortunately, the only remedy for this was to disable constexpr for the function when GCC 5 is used. Save for these two issues, the rest of the work that had to be done for StaticString was smooth sailing, mostly improvements to the coverage of the insert and replace overloads that take input iterators. In the future I plan to do an overhaul of the documentation, but as of now it’s ready for release, so I’m excited to finally get this project out into the wild. Boost.JSON Most of the time I spent on Boost.JSON in the past month was spent learning the interface and internals of the library, as I will be writing documentation on many of the components in the future. My primary focus was on the value_ref class, which is used as the value type of initializer lists used to represent JSON documents within source code. The reason that this is used instead of value is because std::initializer_list suffers from one fatal flaw: the underlying array that it refers to is const, which means you cannot move its elements. Copying a large JSON document is not trivial, so value_ref is used as a proxy referring to an underlying object that can be moved by an operation using an initializer list. This is achieved by storing a pointer to a function that will appropriately copy/move construct a value when called requested by the target. While looking at how value_ref works, I went ahead and added support for moving from a json::string, since up to that point all strings were stored as string_view internally, thus precluding the ability to move from a value_ref constructed from a json::string. There also was a bug caused by how value_ref handles construction from rvalues, in part due to the unintuitive nature of type deduction. Consider the following: struct value_ref { template&amp;lt;typename T&amp;gt; value_ref(const T&amp;amp;) { ... } template&amp;lt;typename T&amp;gt; value_ref(T&amp;amp;&amp;amp;) { ... } }; json::value jv; const json::value const_jv; value_ref rvalue = std::move(jv); // #1 value_ref const_rvalue = std::move(const_jv); // #2 In both #1 and #2, the constructor value_ref(T&amp;amp;&amp;amp;) is called. This certainly makes sense once you consider the deduction that is performed, however, by glancing at just the declarations themselves, it isn’t obvious, as we’ve all been taught that references to non-const types will not bind to a const object. Where this becomes a problem for value_ref is that the constructor taking a T&amp;amp;&amp;amp; parameter expects to be able to move from the parameter, so it internally stores a non-const void*. Converting a “pointer to const T” to void* isn’t permitted, so you get a hard error. The fix for this was fairly trivial. Standardization Most of my time this month was spent working on standardization related activities, which can be broken up into two somewhat separate categories: Fixing existing wording Working on papers Editorial issues I submitted a fair few pull requests to the draft repository, most of which were general wording cleanups and improvements to the consistency of the wording. With respect to wording consistency, I targeted instances of incorrect cv-qualification notation, definitions of terms within notes, cross-references that name the wrong subclause or are self-referential, and redundant normative wording. These kinds of issues are all over the standard, but I generally stay away from the library wording unless it’s absolutely necessary since it has a subtitle difference in the wording style compared to that of the core language. I ended up writing atool to make grepping for these issues a little easier, which is certainly an improvement over manually inspecting each TeX source. The wording cleanups are the most time consuming, but also are the ones I find the most enjoyable. They all follow the same principle of rephrasing an idea with more accurate wording while not changing the actual meaning – something that often proves to be a challenge. These generally start off as small issues I notice in the text, but then snowball into complete rewrites to make the whole thing consistent. Anyways, if it sparks your interest you can find the various editorial fixes I worked on here. P1997 Of my papers, P1997 is the one that I’m putting the most time into. In short, it proposes to make arrays more “regular”, allowing assignment, initialization from other arrays, array placeholder types, and many more features we bestow upon scalar and class types. The holy grail of changes (not proposed in the paper) would be to allow the passing of arrays by value without decaying to a pointer: fully cementing arrays as first-class types. Unlike the other proposed changes this isn’t a pure extension, so to make it remotely feasible existing declarations of parameters with array type (for the sake of brevity we will abbreviate them as PAT) would have to be deprecated in C++23, removed in C++26, and then reinstated in C++29. For this reason, we are undertaking this as an entirely different paper to allow for the pure extensions to be added in C++23, leaving the removal and reinstatement on the backburner. In order to bring convincing evidence that: The current semantics for PAT are unintuitive and merely syntactic sugar. The deprecation of PAT would not be significantly disruptive to existing codebases. Ted (the co-author) and I decided to see exactly how often PAT appear within normal C++ code. While codesearch.isocpp.org by Andrew Tomazos is a fantastic tool, the search we wanted to conduct was simply not possible with his tool, so we set out to create our own. We needed two things: A dataset Some tool to parse all the source files For the dataset, I wrote a tool to clone the top 4000 C++ Github repositories, and clean out all files that weren’t C++ source files (.cpp, .cxx, .cc, .h, .hpp, .hxx, .ipp, .tpp). Github has a nice API to search for repositories, but it unfortunately limits the results for a single search query to 1000 results, but since I was sorting them by start count, I was able to use it as an index to begin a new search query. After accidentally sending my Github credentials to Ted once and waiting 10 hours for all the repositories to be cloned, we had our dataset: 2.7 million source files, totaling around 32GB. To parse all these files, we opted to use the Clang frontend. Its AST matcher was perfectly suited to the task, so it was just a matter of forming the correct matchers for the three contexts a PAT can appear in (function parameter, non-type template parameter, and catch clause parameter). All the files were parsed in single file mode, since opening and expanding #include directives would make the processing of the files take exponentially longer. Forming the correct match pattern proved to be the most difficult part, as the syntax is not entirely intuitive and often times they simply didn’t find every PAT in our test cases. Doing a single file parse along with wanting to find every PAT possible and suppressing all diagnostics landed us in the land of gotchas. To start, the arrayType matcher only would match declarations that had an array declarator present, i.e. int a[] would be found but T a where T names an array type would not. Eventually I found decayedType, which did exactly what we wanted, so long as we filtered out every result that was a function pointer. This worked for function parameters and non-type template parameters, but not catch clause parameters. In the Clang AST, catch is categorized as a statement that encloses a variable declaration whose type is not considered to be decayed (as far as I could see) so we could only match parameters that used an array declarator. I don’t expect anyone to actually declare PATs in a catch clause, and after running the tool on the dataset exactly zero instances were found, so this is most likely a non-issue. Single file parsing introduced a number of issues all stemming from the fact that none of the #include directives were processed, meaning that there was a large number of types that were unresolved. Consider the following: #include &amp;lt;not_processed.h&amp;gt; using array = unresolved[2]; using array_ref = unk_array&amp;amp;; array_ref ref = {unresolved(), unresolved()}; void f(decltype(ref) param); For reasons that I don’t know, since unresolved has no visible declaration Clang reports that param has a decayed type. I suspect this is because diagnostics are suppressed and some recursive procedure used in the determination of the type named by array_ref returns upon encountering an error at the declaration of array and simply returns unresolved[2] as the type. If you know why this happens, don’t hesitate to ping me! I ended up tracking the number of PAT declarations that use an array declarator separately since I suspect that this number may end up being more accurate. Once the tool was ready to go and we started to run it on the dataset, we encountered issues of the worst kind: assertion failures. I suppose such errors could be expected when abusing a compiler to the extent that we did, but they weren’t particularly enjoyable to fix. I should mention that tool itself is meant to be run on a directory, so once an assertion failed, it would end the entire run on that directory. My initial solution to this was changing the problematic asserts to throw an exception, but the number of failures was ever-growing. Creating a new CompilerInstance for each file did somewhat remedy the situation, but didn’t fix it all. Eventually, we called it good enough and let it run over the entire dataset. Clang itself was in our dataset, with a nasty little surprise taking the form of infinitely recursive template instantiations and debug pragmas that would crash the compiler. Clang relies on the diagnostics to signal when the recursive instantiation limit is reached, but since those were disabled the thread would never terminate. Evil. Once the paper is finished, I’ll report the results in a future post. Implicit destruction This paper intends to substantially overhaul the wording that describes the interaction between objects and their storage. I originally brought this issue up several months back with this pull request in an attempt to fix it editorially, but it was deemed too large in scope. I finally got started on the paper and drafted up a direction to take, which will hopefully resolve the shortfalls of our current wording. This problem stems from the notion that objects don’t exist before their lifetime has started and after it has ended. This allows compilers to make a lot of assumptions and consequently leads to more optimized code, but the manner in which the wording was applied has severely crippled our ability to refer to “not-objects”. We want to be able to place restrictions on storage that an object used to occupy, but simply have no way for doing so. Thus, the direction I plan to take is to define the semantics of storage, allowing us to place restrictions on that storage even if no object exists within it. I don’t have too many of the core definitions completed yet as they require the most time to make them robust, but once that is hashed out, applying it where needed should be smooth sailing. Here is a short list of the main changes: Define what a region of storage is, specify when it is acquired, released, and what kinds of objects may occupy it. Remove the storage duration property from objects, and effectively make that agnostic of the storage they occupy. Instead, associate storage duration with a variable. Dynamic storage duration can removed since such storage isn’t associated with a variable. Specify when reuse of storage occurs, and its effects upon the objects within that storage. Properly specify when pointers and expressions refer to storage while preserving the notion that they refer to objects (or functions). Specify that when control passes through the definition of a variable, storage is acquired, objects are created, and initialization is performed. Likewise, specify that exit from a scope, program, or thread causes objects within the storage to be destroyed (if any), and the storage to be released. It’s a big undertaking, but I’m excited to work on this and see what kind of feedback I get. However, this paper will be more of a long term project, since it will be touching the majority of the wording in the standard. I’ll provide updates in future posts. Placement new during constant evaluation A paper that I’ve been thinking about working on and finally got around to revolves around a new (heh) C++20 feature: new expressions are now able to be evaluated during constant evaluation (CE). While the storage they acquire must be released during the CE and it may only call the replaceable global allocation functions, it finally allows for the use of dynamically sized containers within a constant expression. This is great! However, there is a restriction here that is completely avoidable. The function std::construct_at was introduced to allow for objects to be constructed as if by placement new – nice, but we don’t allow placement new to be used by itself. This is because a certain implementation can’t resolve what object a void* points to during CE (thank you Tim Song for the info); and because CE is intended to always yield the same results on all implementations, construct_at is used to ensure the pointer type passed is always a pointer to object type. I think that at the very least, congruent placement new expressions should be allowed by the principle of this being unnecessarily restrictive. As with all the other papers, I’ll post progress updates in a future post. I’ve drafted up some wording, and I plan to have this ready sometime around June. Information If you want to get in touch with me, you can message me on the Cpplang slack, or shoot me an email.</summary></entry><entry><title type="html">Richard’s March Update</title><link href="http://cppalliance.org/richard/2020/03/31/RichardsMarchUpdate.html" rel="alternate" type="text/html" title="Richard’s March Update" /><published>2020-03-31T00:00:00+00:00</published><updated>2020-03-31T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/03/31/RichardsMarchUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/03/31/RichardsMarchUpdate.html">&lt;h1 id=&quot;coding-in-the-time-of-a-pandemic&quot;&gt;Coding in the time of a Pandemic&lt;/h1&gt;

&lt;p&gt;It has been an interesting month, there having been the minor distraction of a lockdown of our
little country. The borders with Spain and France were closed about three weeks ago and
all residents have been asked to stay at home other than to buy groceries or walk their dogs.
Fortunately I have dogs so I at least have a legitimate reason to see the sun.&lt;/p&gt;

&lt;p&gt;One of the advantages of living in a tiny country is that the government has been able to
secure the supply of 150,000 COVID-19 testing kits, which represents two tests per resident.
They are also working on supplying every resident with masks for use when shopping.
I am hoping to report in my next blog that we are allowed outside subject to a negative
test and the wearing of a mask and gloves.&lt;/p&gt;

&lt;p&gt;Fortunately, until today, our internet has been uninterrupted. Communication with my friends
and colleagues at the C++ Alliance and the wider developer community has continued.&lt;/p&gt;

&lt;h1 id=&quot;boost-release&quot;&gt;Boost Release&lt;/h1&gt;

&lt;p&gt;The Boost 1.73 release is imminent. Thus much of my focus in the latter half of the month has
been on addressing any remaining issues in Beast that represent an easy win in terms of
demonstrating progress between releases.&lt;/p&gt;

&lt;p&gt;This brings to a close my first quarter as a maintainer of the Beast library. I would have
liked to have produced more in terms of feature development and architectural improvements,
but a few interesting things came up which delayed this; some of which I will share with you
here.&lt;/p&gt;

&lt;h1 id=&quot;possibly-interesting-asio-things&quot;&gt;(Possibly) Interesting Asio Things&lt;/h1&gt;

&lt;p&gt;To say that Boost.Beast has a strong dependency on Boost.Asio would be an understatement. It
should therefore come as no surprise that the Beast team spend a lot of time working with
Asio and (certainly in my case) a lot of time working to understand the internals.&lt;/p&gt;

&lt;p&gt;We had cause to reach out to Chris Kohlhoff, Asio’s author, on two occasions in recent
times. If you read my February blog you would have seen the issues we have faced with the
&lt;code&gt;DynamicBuffer&lt;/code&gt; concept. This month it was about the thread-safety of composed operations and
IO objects.&lt;/p&gt;

&lt;p&gt;But first, the result of a question I asked myself:&lt;/p&gt;

&lt;h2 id=&quot;is-it-possible-to-write-an-asynchronous-composed-operation-entirely-as-a-lambda&quot;&gt;Is it possible to write an asynchronous composed operation entirely as a lambda?&lt;/h2&gt;

&lt;p&gt;In short, if you’re using c++14 or better, the answer is happily yes!&lt;/p&gt;

&lt;p&gt;Here is the smallest program I could think of:&lt;/p&gt;

&lt;p&gt;a: Implemented asynchronously&lt;/p&gt;

&lt;p&gt;b: Targeting a POSIX system (just because I happen to know more about POSIX than Windows)&lt;/p&gt;

&lt;p&gt;This program simply copies the contents of &lt;code&gt;stdin&lt;/code&gt; to &lt;code&gt;stdout&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int
main()
{
    asio::io_context ioc;
    auto exec = ioc.get_executor();

    auto in = asio::posix::stream_descriptor(exec, ::dup(STDIN_FILENO));
    auto out = asio::posix::stream_descriptor(exec, ::dup(STDOUT_FILENO));

    async_copy_all(in, out, [](auto&amp;amp;&amp;amp; ec, auto total){
        std::cout &amp;lt;&amp;lt; &quot;\ntransferred &quot; &amp;lt;&amp;lt; total &amp;lt;&amp;lt; &quot; bytes\n&quot;;
        if (ec.failed())
        {
            std::cerr &amp;lt;&amp;lt; &quot;transfer failure: &quot; &amp;lt;&amp;lt; ec.message() &amp;lt;&amp;lt; std::endl;
            std::exit(ec.value());
        }
    });

    ioc.run();

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;People who are unused to writing composed operations (asynchronous operations that fit into
the  ASIO ecosystem), or people who have written them longer ago than last year, might at
this stage feel their hearts sinking in anticipation of the complex horror show awaiting
them when writing the function &lt;code&gt;async_copy_all&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Fortunately, Asio’s new(ish) &lt;code&gt;async_compose&lt;/code&gt; template function makes this reasonably
painless:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template&amp;lt;class InStream, class OutStream, class CompletionToken&amp;gt;
auto
async_copy_all(
    InStream &amp;amp;fd_in,
    OutStream &amp;amp;fd_out,
    CompletionToken &amp;amp;&amp;amp;completion)
{
    return asio::async_compose&amp;lt;
        CompletionToken,
        void(system::error_code const &amp;amp;,std::size_t)&amp;gt;(
            [&amp;amp;fd_in, &amp;amp;fd_out,
                    coro = asio::coroutine(),
                    total = std::size_t(0),
                    store = std::make_unique&amp;lt;char[]&amp;gt;(4096)]
               (auto &amp;amp;self,
                system::error_code ec = {},
                std::size_t bytes_transferred = 0) mutable
        {
            BOOST_ASIO_CORO_REENTER(coro)
            for(;;)
            {
                BOOST_ASIO_CORO_YIELD
                {
                    auto buf = asio::buffer(store.get(), 4096);
                    fd_in.async_read_some(buf, std::move(self));
                }
                if (ec.failed() || bytes_transferred == 0)
                {
                    if (ec == asio::error::eof)
                        ec.clear();
                    return self.complete(ec, total);
                }

                BOOST_ASIO_CORO_YIELD
                {
                    auto buf = asio::buffer(store.get(), bytes_transferred);
                    fd_out.async_write_some(buf, std::move(self));
                }
                total += bytes_transferred;
                if (ec.failed())
                    return self.complete(ec, total);
            }
        },
        completion, fd_in, fd_out);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few things to note in the implementation.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first is that the entire asynchronous operation’s implementation state is captured
in the capture block of the lambda (this is why we need c++14 or higher)&lt;/li&gt;
  &lt;li&gt;Secondly, the lambda is mutable. This is so we can update the state and then &lt;code&gt;move&lt;/code&gt; it
into the completion handler of each internal asynchronous operation.&lt;/li&gt;
  &lt;li&gt;The second and third arguments of the lambda’s function signature are defaulted. This is
because &lt;code&gt;async_compose&lt;/code&gt; will cause the implementation (in this case, our lambda) to be called
once with no arguments (other than &lt;code&gt;self&lt;/code&gt;) during initiation.&lt;/li&gt;
  &lt;li&gt;There is an explicit check for &lt;code&gt;eof&lt;/code&gt; after the yielding call to &lt;code&gt;fd_in.async_read_some&lt;/code&gt;.
In Asio, &lt;code&gt;eof&lt;/code&gt; is one of a few error codes that represents an informational condition
rather than an actual error. Another is &lt;code&gt;connection_aborted&lt;/code&gt;, which can occur during
an &lt;code&gt;accept&lt;/code&gt; operation on a TCP socket. Failing to check for this error-that-is-not-an-error
can result in asio-based servers suddenly going quiet for ‘no apparent reason’.&lt;/li&gt;
  &lt;li&gt;Notice that the un-named object created by &lt;code&gt;async_compose&lt;/code&gt; intercepts every invocation on
it and transfers control to our lambda by prepending a reference to itself to the argument
list. The type of &lt;code&gt;Self&lt;/code&gt; is actually a specialisation of an &lt;code&gt;asio::detail::composed_op&amp;lt;...&amp;gt;&lt;/code&gt;
(as at Boost 1.72). However, since this class is in the detail namespace, this should never
be relied on in any program or library.&lt;/li&gt;
  &lt;li&gt;Note that I create the buffer object &lt;code&gt;buf&lt;/code&gt; in separate statements to the initiations of
the async operations on the streams. This is because the &lt;code&gt;unique_ptr&lt;/code&gt; called &lt;code&gt;store&lt;/code&gt; is going
to be &lt;code&gt;move&lt;/code&gt;d during the initiating function call. Remember that arguments to function calls
are evaluated in unknowable order in c++, so accessing &lt;code&gt;store&lt;/code&gt; in the same statement in
which the entire completion handler has been &lt;code&gt;move&lt;/code&gt;d would result in UB.&lt;/li&gt;
  &lt;li&gt;Finally, &lt;code&gt;async_compose&lt;/code&gt; is passed both the input and output stream (in addition to their
references being captured in the lambda) so that both streams’ associated executors can be
informed that there is outstanding work. It may be surprising to some that the input and
output streams may legally be associated with different executors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Actually, now that I write this, it occurs to me that it is unclear to me what is the
‘associated executor’ of the composed operation we just created. Asio’s documentation is
silent on the subject.&lt;/p&gt;

&lt;p&gt;Inspecting the code while single-stepping through a debug build revealed that the executor is
taken from the first of the &lt;code&gt;io_objects_or_executors&amp;amp;&amp;amp;...&lt;/code&gt; arguments to &lt;code&gt;async_compose&lt;/code&gt; which
itself has an associated executor. If none of them do, then the &lt;code&gt;system_executor&lt;/code&gt; is chosen as
the default executor (more on why this may cause surprises and headaches later). Note that as
always, wrapping the lambda in a call to &lt;code&gt;bind_executor&lt;/code&gt; will force the composed operation’s
intermediate invocations to happen on the bound executor.&lt;/p&gt;

&lt;p&gt;In our case, it is &lt;code&gt;fd_in&lt;/code&gt; which will be providing the executor and as a result, every
invocation of our lambda (except the first) is guaranteed to be happen by being invoked
as if by &lt;code&gt;post(fd_in.get_executor(), &amp;lt;lambda&amp;gt;(...))&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;system_executor-and-what-could-possibly-go-wrong&quot;&gt;&lt;code&gt;system_executor&lt;/code&gt; and “What Could Possibly Go Wrong?”&lt;/h2&gt;

&lt;p&gt;Once upon a time, when I first started using Asio, there were no &lt;code&gt;executor&lt;/code&gt;s at all. In
fact, there were no &lt;code&gt;io_context&lt;/code&gt;s either. There was an &lt;code&gt;io_service&lt;/code&gt; object. At some point
(I don’t remember the exact version of Asio, but it was at least five years ago) the
&lt;code&gt;io_service&lt;/code&gt; was replace with &lt;code&gt;io_context&lt;/code&gt;, an object which did basically the same job.&lt;/p&gt;

&lt;p&gt;More recently, the &lt;code&gt;io_context&lt;/code&gt; represents the shared state of a model of the &lt;code&gt;Executor&lt;/code&gt;
Named Type Requirement (aka Concept). The state of the art is moving towards passing copies
of &lt;code&gt;Executor&lt;/code&gt;s rather than references to &lt;code&gt;io_context&lt;/code&gt;s.&lt;/p&gt;

&lt;p&gt;Asio now contains a concrete type, the &lt;code&gt;executor&lt;/code&gt; which is a type-erased wrapper which
may be assigned any any class which models an &lt;code&gt;Executor&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As you might expect, we are heading into a world where there might be more than one model
of &lt;code&gt;Executor&lt;/code&gt;. In anticipation of this, by default, all Asio IO objects are now associated
with the polymorphic wrapper type &lt;code&gt;executor&lt;/code&gt; rather than a &lt;code&gt;io_context::executor_type&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One such model of &lt;code&gt;Executor&lt;/code&gt; supplied by Asio is the &lt;code&gt;system_executor&lt;/code&gt;, which is actually
chosen as the default associated executor of any completion handler. That is, if you initiate
an asynchronous operation in Asio today, against a hypothetical io_object that does not have
an associated executor and you do not bind your handler to an executor of your own, then
your handler will be invoked as-if by &lt;code&gt;post(asio::system_executor(), &amp;lt;handler&amp;gt;)&lt;/code&gt; - that is,
it will be called on some implementation-defined thread.&lt;/p&gt;

&lt;p&gt;Now that the basics are covered, back to &lt;em&gt;what could possibly go wrong&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Well imagine a hypothetical home-grown IO Object or &lt;em&gt;AsyncStream&lt;/em&gt;. Older versions of the Asio
documentation used to include an example user IO Object, the logging socket.&lt;/p&gt;

&lt;p&gt;The basic premise of our logging socket is that it will do everything a socket will do, plus
log the sending and receiving of data, along with the error codes associated with each read
or write operation.&lt;/p&gt;

&lt;p&gt;Clearly the implementation of this object will contain an asio socket object and some kind of
logger. The internal state must be touched on every asynchronous operation initiation (to
actually initiate the underlying operation and record the event) &lt;em&gt;and&lt;/em&gt; during every
completion handler invocation, in order to update the logger with the results of the
asynchronous operation.&lt;/p&gt;

&lt;p&gt;As we know, invocations of intermediate completion handlers happen on the executor associated
with the final completion handler provided by the user, so in our case, the actions will be
something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;on the initiating thread:
  logging_socket::async_write_some
    logging_socket::async_write_some_op::operator()()
      logging_socket::impl::update_logger(...)
      socket::async_write_some(...)

... time passes...

on a thread associated with the associated executor:
  logging_socket::async_write_some_op::operator()(ec, bytes_transferred)
    logging_socket::impl::update_logger()
    user_completion_handler(ec, bytes_transferred)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The situation will be similar for a write operation.&lt;/p&gt;

&lt;p&gt;Now consider the following code (&lt;code&gt;ls&lt;/code&gt; is an object of our hypothetical type &lt;code&gt;logging_socket&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;  ls.async_write_some(
    get_tx_buffer(),
    net::bind_executor(
      net::system_executor(),
      [](auto ec, auto size){
        /* what happens here is not relevant */
      }));
  ls.async_read_some(
    get_rx_buffer(),
    net::bind_executor(
      net::system_executor(),
      [](auto ec, auto size){
        /* what happens here is not relevant */
      }));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What have I done? Not much, simply initiated a read and a write at the same time - a
perfectly normal state of affairs for a socket. The interesting part is that I have
bound both asynchronous completion handlers to the &lt;code&gt;system_executor&lt;/code&gt;. This means that
each of the handlers will be invoked (without synchronisation) on two arbitrary threads.&lt;/p&gt;

&lt;p&gt;Looking at our pseudo-code above, it becomes clear that there will be a race for the
&lt;code&gt;logging_socket&lt;/code&gt;’s implementation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Between the initiation of the read and the completion of the write, and&lt;/li&gt;
  &lt;li&gt;between the completion of the read and the completion of the write&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again the Asio documentation is silent on the correct method of mitigating this situation.
Two possible workarounds have occurred to me so far:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Never use a &lt;code&gt;system_executor&lt;/code&gt; unless first wrapping it in a &lt;code&gt;strand&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Ensure that all composed operations of IO objects are thread-safe with respect to
mutation of the implementation. If this is made true, it almost inevitably follows that
the entire IO Object may as well be made thread-safe (which Asio IO Objects are not).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I have reached out to Chris for final judgement and will update the blog (and possibly much
of Beast!) in response to a definitive answer.&lt;/p&gt;

&lt;h1 id=&quot;unified-web-client&quot;&gt;Unified Web Client&lt;/h1&gt;

&lt;p&gt;I have been given the go-ahead to make a start on exploring a unified web-client library
which will eventually become a candidate for inclusion into Boost.&lt;/p&gt;

&lt;p&gt;The obvious course of action, building directly on top of Beast is a no-go. If the
library is to be used on platforms such as tablets and phones, or appear in the various
app stores of vendors, there are restrictions on which implementations of communications
libraries may be used. To cut a long story short, vendors want to minimise the risk of
security vulnerabilities being introduced by people’s home-grown communications and
encryption code.&lt;/p&gt;

&lt;p&gt;So my initial focus will be on establishing an object model that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provides a high degree of utility (make simple things simple).&lt;/li&gt;
  &lt;li&gt;Emulates or captures the subtleties of vendor’s Web Client frameworks.&lt;/li&gt;
  &lt;li&gt;Efficiently slots into the Asio asynchronous completion model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, linux and proprietary embedded systems do not have a mandated communications
libraries, so there will certainly be heavy use of Beast in the unconstrained platform-
specific code.&lt;/p&gt;

&lt;p&gt;More information as it becomes available.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Coding in the time of a Pandemic It has been an interesting month, there having been the minor distraction of a lockdown of our little country. The borders with Spain and France were closed about three weeks ago and all residents have been asked to stay at home other than to buy groceries or walk their dogs. Fortunately I have dogs so I at least have a legitimate reason to see the sun. One of the advantages of living in a tiny country is that the government has been able to secure the supply of 150,000 COVID-19 testing kits, which represents two tests per resident. They are also working on supplying every resident with masks for use when shopping. I am hoping to report in my next blog that we are allowed outside subject to a negative test and the wearing of a mask and gloves. Fortunately, until today, our internet has been uninterrupted. Communication with my friends and colleagues at the C++ Alliance and the wider developer community has continued. Boost Release The Boost 1.73 release is imminent. Thus much of my focus in the latter half of the month has been on addressing any remaining issues in Beast that represent an easy win in terms of demonstrating progress between releases. This brings to a close my first quarter as a maintainer of the Beast library. I would have liked to have produced more in terms of feature development and architectural improvements, but a few interesting things came up which delayed this; some of which I will share with you here. (Possibly) Interesting Asio Things To say that Boost.Beast has a strong dependency on Boost.Asio would be an understatement. It should therefore come as no surprise that the Beast team spend a lot of time working with Asio and (certainly in my case) a lot of time working to understand the internals. We had cause to reach out to Chris Kohlhoff, Asio’s author, on two occasions in recent times. If you read my February blog you would have seen the issues we have faced with the DynamicBuffer concept. This month it was about the thread-safety of composed operations and IO objects. But first, the result of a question I asked myself: Is it possible to write an asynchronous composed operation entirely as a lambda? In short, if you’re using c++14 or better, the answer is happily yes! Here is the smallest program I could think of: a: Implemented asynchronously b: Targeting a POSIX system (just because I happen to know more about POSIX than Windows) This program simply copies the contents of stdin to stdout: int main() { asio::io_context ioc; auto exec = ioc.get_executor(); auto in = asio::posix::stream_descriptor(exec, ::dup(STDIN_FILENO)); auto out = asio::posix::stream_descriptor(exec, ::dup(STDOUT_FILENO)); async_copy_all(in, out, [](auto&amp;amp;&amp;amp; ec, auto total){ std::cout &amp;lt;&amp;lt; &quot;\ntransferred &quot; &amp;lt;&amp;lt; total &amp;lt;&amp;lt; &quot; bytes\n&quot;; if (ec.failed()) { std::cerr &amp;lt;&amp;lt; &quot;transfer failure: &quot; &amp;lt;&amp;lt; ec.message() &amp;lt;&amp;lt; std::endl; std::exit(ec.value()); } }); ioc.run(); return 0; } People who are unused to writing composed operations (asynchronous operations that fit into the ASIO ecosystem), or people who have written them longer ago than last year, might at this stage feel their hearts sinking in anticipation of the complex horror show awaiting them when writing the function async_copy_all. Fortunately, Asio’s new(ish) async_compose template function makes this reasonably painless: template&amp;lt;class InStream, class OutStream, class CompletionToken&amp;gt; auto async_copy_all( InStream &amp;amp;fd_in, OutStream &amp;amp;fd_out, CompletionToken &amp;amp;&amp;amp;completion) { return asio::async_compose&amp;lt; CompletionToken, void(system::error_code const &amp;amp;,std::size_t)&amp;gt;( [&amp;amp;fd_in, &amp;amp;fd_out, coro = asio::coroutine(), total = std::size_t(0), store = std::make_unique&amp;lt;char[]&amp;gt;(4096)] (auto &amp;amp;self, system::error_code ec = {}, std::size_t bytes_transferred = 0) mutable { BOOST_ASIO_CORO_REENTER(coro) for(;;) { BOOST_ASIO_CORO_YIELD { auto buf = asio::buffer(store.get(), 4096); fd_in.async_read_some(buf, std::move(self)); } if (ec.failed() || bytes_transferred == 0) { if (ec == asio::error::eof) ec.clear(); return self.complete(ec, total); } BOOST_ASIO_CORO_YIELD { auto buf = asio::buffer(store.get(), bytes_transferred); fd_out.async_write_some(buf, std::move(self)); } total += bytes_transferred; if (ec.failed()) return self.complete(ec, total); } }, completion, fd_in, fd_out); } There are a few things to note in the implementation. The first is that the entire asynchronous operation’s implementation state is captured in the capture block of the lambda (this is why we need c++14 or higher) Secondly, the lambda is mutable. This is so we can update the state and then move it into the completion handler of each internal asynchronous operation. The second and third arguments of the lambda’s function signature are defaulted. This is because async_compose will cause the implementation (in this case, our lambda) to be called once with no arguments (other than self) during initiation. There is an explicit check for eof after the yielding call to fd_in.async_read_some. In Asio, eof is one of a few error codes that represents an informational condition rather than an actual error. Another is connection_aborted, which can occur during an accept operation on a TCP socket. Failing to check for this error-that-is-not-an-error can result in asio-based servers suddenly going quiet for ‘no apparent reason’. Notice that the un-named object created by async_compose intercepts every invocation on it and transfers control to our lambda by prepending a reference to itself to the argument list. The type of Self is actually a specialisation of an asio::detail::composed_op&amp;lt;...&amp;gt; (as at Boost 1.72). However, since this class is in the detail namespace, this should never be relied on in any program or library. Note that I create the buffer object buf in separate statements to the initiations of the async operations on the streams. This is because the unique_ptr called store is going to be moved during the initiating function call. Remember that arguments to function calls are evaluated in unknowable order in c++, so accessing store in the same statement in which the entire completion handler has been moved would result in UB. Finally, async_compose is passed both the input and output stream (in addition to their references being captured in the lambda) so that both streams’ associated executors can be informed that there is outstanding work. It may be surprising to some that the input and output streams may legally be associated with different executors. Actually, now that I write this, it occurs to me that it is unclear to me what is the ‘associated executor’ of the composed operation we just created. Asio’s documentation is silent on the subject. Inspecting the code while single-stepping through a debug build revealed that the executor is taken from the first of the io_objects_or_executors&amp;amp;&amp;amp;... arguments to async_compose which itself has an associated executor. If none of them do, then the system_executor is chosen as the default executor (more on why this may cause surprises and headaches later). Note that as always, wrapping the lambda in a call to bind_executor will force the composed operation’s intermediate invocations to happen on the bound executor. In our case, it is fd_in which will be providing the executor and as a result, every invocation of our lambda (except the first) is guaranteed to be happen by being invoked as if by post(fd_in.get_executor(), &amp;lt;lambda&amp;gt;(...)). system_executor and “What Could Possibly Go Wrong?” Once upon a time, when I first started using Asio, there were no executors at all. In fact, there were no io_contexts either. There was an io_service object. At some point (I don’t remember the exact version of Asio, but it was at least five years ago) the io_service was replace with io_context, an object which did basically the same job. More recently, the io_context represents the shared state of a model of the Executor Named Type Requirement (aka Concept). The state of the art is moving towards passing copies of Executors rather than references to io_contexts. Asio now contains a concrete type, the executor which is a type-erased wrapper which may be assigned any any class which models an Executor. As you might expect, we are heading into a world where there might be more than one model of Executor. In anticipation of this, by default, all Asio IO objects are now associated with the polymorphic wrapper type executor rather than a io_context::executor_type. One such model of Executor supplied by Asio is the system_executor, which is actually chosen as the default associated executor of any completion handler. That is, if you initiate an asynchronous operation in Asio today, against a hypothetical io_object that does not have an associated executor and you do not bind your handler to an executor of your own, then your handler will be invoked as-if by post(asio::system_executor(), &amp;lt;handler&amp;gt;) - that is, it will be called on some implementation-defined thread. Now that the basics are covered, back to what could possibly go wrong? Well imagine a hypothetical home-grown IO Object or AsyncStream. Older versions of the Asio documentation used to include an example user IO Object, the logging socket. The basic premise of our logging socket is that it will do everything a socket will do, plus log the sending and receiving of data, along with the error codes associated with each read or write operation. Clearly the implementation of this object will contain an asio socket object and some kind of logger. The internal state must be touched on every asynchronous operation initiation (to actually initiate the underlying operation and record the event) and during every completion handler invocation, in order to update the logger with the results of the asynchronous operation. As we know, invocations of intermediate completion handlers happen on the executor associated with the final completion handler provided by the user, so in our case, the actions will be something like this: on the initiating thread: logging_socket::async_write_some logging_socket::async_write_some_op::operator()() logging_socket::impl::update_logger(...) socket::async_write_some(...) ... time passes... on a thread associated with the associated executor: logging_socket::async_write_some_op::operator()(ec, bytes_transferred) logging_socket::impl::update_logger() user_completion_handler(ec, bytes_transferred) The situation will be similar for a write operation. Now consider the following code (ls is an object of our hypothetical type logging_socket: ls.async_write_some( get_tx_buffer(), net::bind_executor( net::system_executor(), [](auto ec, auto size){ /* what happens here is not relevant */ })); ls.async_read_some( get_rx_buffer(), net::bind_executor( net::system_executor(), [](auto ec, auto size){ /* what happens here is not relevant */ })); What have I done? Not much, simply initiated a read and a write at the same time - a perfectly normal state of affairs for a socket. The interesting part is that I have bound both asynchronous completion handlers to the system_executor. This means that each of the handlers will be invoked (without synchronisation) on two arbitrary threads. Looking at our pseudo-code above, it becomes clear that there will be a race for the logging_socket’s implementation: Between the initiation of the read and the completion of the write, and between the completion of the read and the completion of the write Again the Asio documentation is silent on the correct method of mitigating this situation. Two possible workarounds have occurred to me so far: Never use a system_executor unless first wrapping it in a strand. Ensure that all composed operations of IO objects are thread-safe with respect to mutation of the implementation. If this is made true, it almost inevitably follows that the entire IO Object may as well be made thread-safe (which Asio IO Objects are not). I have reached out to Chris for final judgement and will update the blog (and possibly much of Beast!) in response to a definitive answer. Unified Web Client I have been given the go-ahead to make a start on exploring a unified web-client library which will eventually become a candidate for inclusion into Boost. The obvious course of action, building directly on top of Beast is a no-go. If the library is to be used on platforms such as tablets and phones, or appear in the various app stores of vendors, there are restrictions on which implementations of communications libraries may be used. To cut a long story short, vendors want to minimise the risk of security vulnerabilities being introduced by people’s home-grown communications and encryption code. So my initial focus will be on establishing an object model that: Provides a high degree of utility (make simple things simple). Emulates or captures the subtleties of vendor’s Web Client frameworks. Efficiently slots into the Asio asynchronous completion model. Of course, linux and proprietary embedded systems do not have a mandated communications libraries, so there will certainly be heavy use of Beast in the unconstrained platform- specific code. More information as it becomes available.</summary></entry></feed>