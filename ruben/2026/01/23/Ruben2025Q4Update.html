<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>A postgres library for Boost | The C++ Alliance</title>
<link href="https://fonts.googleapis.com/css?family=Roboto:400,700" rel="stylesheet">
<!-- Bootstrap core CSS -->
<link href="/css/style.css" rel="stylesheet">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png?v=1">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png?v=1">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png?v=1">
<link rel="manifest" href="/site.webmanifest?v=1">
<link rel="mask-icon" href="/safari-pinned-tab.svg?v=1" color="#a91c20">
<link rel="shortcut icon" href="/favicon.ico?v=1">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#ffffff">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>A postgres library for Boost | The C++ Alliance</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="A postgres library for Boost" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Do you know Boost.MySQL? If you’ve been reading my posts, you probably do. Many people have wondered ‘why not Postgres?’. Well, the time is now. TL;DR: I’m writing the equivalent of Boost.MySQL, but for PostgreSQL. You can find the code here. Since libPQ is already a good library, the NativePG project intends to be more ambitious than Boost.MySQL. In addition to the expected Asio interface, I intend to provide a sans-io API that exposes primitives like message serialization. Throughout this post, I will go into the intended library design and the rationales behind its design. The lowest level: message serialization PostgreSQL clients communicate with the server using a binary protocol on top of TCP, termed the frontend/backend protocol. The protocol defines a set of messages used for interactions. For example, when running a query, the following happens: ┌────────┐ ┌────────┐ │ Client │ │ Server │ └───┬────┘ └───┬────┘ │ │ │ Query │ │ ──────────────────────────────────────────&gt; │ │ │ │ RowDescription │ │ &lt;────────────────────────────────────────── │ │ │ │ DataRow │ │ &lt;────────────────────────────────────────── │ │ │ │ CommandComplete │ │ &lt;────────────────────────────────────────── │ │ │ │ ReadyForQuery │ │ &lt;────────────────────────────────────────── │ │ │ In the lowest layer, this library provides functions to serialize and parse such messages. The goal here is being as efficient as possible. Parsing functions are non-allocating, and use an approach inspired by Boost.Url collections: Parsing database types The PostgreSQL type system is quite rich. In addition to the usual SQL built-in types, it supports advanced scalars like UUIDs, arrays and user-defined aggregates. When running a query, libPQ exposes retrieved data as either raw text or bytes. This is what the server sends in the DataRow packets shown above. To do something useful with the data, users likely need parsing and serializing such types. The next layer of NativePG is in charge of providing such functions. This will likely contain some extension points for users to plug in their types. This is the general form of such functions: system::error_code parse(span&lt;const std::byte&gt; from, T&amp; to, const connection_state&amp;); void serialize(const T&amp; from, dynamic_buffer&amp; to, const connection_state&amp;); Note that some types might require access to session configuration. For instance, dates may be expressed using different wire formats depending on the connection’s runtime settings. At the time of writing, only ints and strings are supported, but this will be extended soon. Composing requests Efficiency in database communication is achieved with pipelining. A network round-trip with the server is worth a thousand allocations in the client. It is thus critical that: The protocol properly supports pipelining. This is the case with PostgreSQL. The client should expose an interface to it, and make it very easy to use. libPQ does the first, and NativePG intends to achieve the second. NativePG pipelines by default. In NativePG, a request object is always a pipeline: // Create a request request req; // These two queries will be executed as part of a pipeline req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); req.add_query(&quot;DELETE FROM libs WHERE author &lt;&gt; $1&quot;, {&quot;Ruben&quot;}); Everything you may ask the server can be added to request. This includes preparing and executing statements, establishing pipeline synchronization points, and so on. It aims to be close enough to the protocol to be powerful, while also exposing high-level functions to make things easier. Reading responses Like request, the core response mechanism aims to be as close to the protocol as possible. Since use cases here are much more varied, there is no single response class, but a concept, instead. This is what a response_handler looks like: struct my_handler { // Check that the handler is compatible with the request, // and prepare any required data structures. Called once at the beginning handler_setup_result setup(const request&amp; req, std::size_t pipeline_offset); // Called once for every message received from the server // (e.g. `RowDescription`, `DataRow`, `CommandComplete`) void on_message(const any_request_message&amp; msg); // The overall result of the operation (error_code + diagnostic string). // Called after the operation has finished. const extended_error&amp; result() const; }; Note that on_message is not allowed to report errors. Even if a handler encounters a problem with a message (imagine finding a NULL for a field where the user isn’t expecting one), this is a user error, rather than a protocol error. Subsequent steps in the pipeline must not be affected by this. This is powerful but very low-level. Using this mechanism, the library exposes an interface to parse the result of a query into a user-supplied struct, using Boost.Describe: struct library { std::int32_t id; std::string name; std::string cpp_version; }; BOOST_DESCRIBE_STRUCT(library, (), (id, name, cpp_version)) // ... std::vector&lt;library&gt; libs; auto handler = nativepg::into(libs); // this is a valid response_handler Network algorithms Given a user request and response handler, how do we send these to the server? We need a set of network algorithms to achieve this. Some of these are trivial: sending a request to the server is an asio::write on the request’s buffer. Others, however, are more involved: Reading a pipeline response needs to verify that the message sequence is what we expected, for security, and handle errors gracefully. The handshake algorithm, in charge of authentication when we connect to the server, needs to respond to server authentication challenges, which may come in different forms. Writing these using asio::async_compose is problematic because: They become tied to Boost.Asio. They are difficult to test. They result in long compile times and code bloat due to templating. At the moment, these are written as finite state machines, similar to how OpenSSL behaves in non-blocking mode: // Reads the response of a pipeline (simplified). // This is a hand-wired generator. class read_response_fsm { public: // User-supplied arguments: request and response read_response_fsm(const request&amp; req, response_handler_ref handler); // Yielded to signal that we should read from the server struct read_args { span&lt;std::byte&gt; buffer; }; // Yielded to signal that we&#39;re done struct done_args { system::error_code result; }; variant&lt;read_args, done_args&gt; resume(connection_state&amp;, system::error_code io_result, std::size_t bytes_transferred); }; The idea is that higher-level code should call resume until it returns a done_args value. This allows de-coupling from the underlying I/O runtime. Since NativePG targets C++20, I’m considering rewriting this as a coroutine. Boost.Capy (currently under development - hopefully part of Boost soon) could be a good candidate for this. Putting everything together: the Asio interface At the end of the day, most users just want a connection object they can easily use. Once all the sans-io parts are working, writing it is pretty straight-forward. This is what end user code looks like: // Create a connection connection conn{co_await asio::this_coro::executor}; // Connect co_await conn.async_connect( {.hostname = &quot;localhost&quot;, .username = &quot;postgres&quot;, .password = &quot;&quot;, .database = &quot;postgres&quot;} ); std::cout &lt;&lt; &quot;Startup complete\n&quot;; // Compose our request and response request req; req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); std::vector&lt;library&gt; libs; // Run the request co_await conn.async_exec(req, into(libs)); Auto-batch connections While connection is good, experience has shown me that it’s still too low-level for most users: Connection establishment is manual with async_connect. No built-in reconnection or health checks. No built-in concurrent execution of requests. That is, async_exec first writes the request, then reads the response. Other requests may not be executed during this period. This limits the connection’s throughput. For this reason, NativePG will provide some higher-level interfaces that will make server communication easier and more efficient. To get a feel of what we need, we should first understand the two main usage patterns that we expect. Most of the time, connections are used in a stateless way. For example, consider querying data from the server: request req; req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); co_await conn.async_exec(req, res); This query is not mutating connection state in any way. Other queries could be inserted before and after it without making any difference. I plan to add a higher-level connection type, similar to redis::connection in Boost.Redis, that automatically batches concurrent requests and handles reconnection. The key differences with connection would be: Several independent tasks can share an auto-batch connection. This is an error for connection. If several requests are queued at the same time, the connection may send them together to the server using a single system call. There is no async_connect in an auto-batch connection. Reconnection is handled automatically. Note that this pattern is not exclusive to read-only or individual queries. Transactions can work by using protocol features: request req; req.set_autosync(false); // All subsequent queries are part of the same transaction req.add_query(&quot;UPDATE table1 SET x = $1 WHERE y = 2&quot;, {42}); req.add_query(&quot;UPDATE table2 SET x = $1 WHERE y = 42&quot;, {2}); req.add_sync(); // The two updates run atomically co_await conn.async_exec(req, res); Connection pools I mentioned there were two main usage scenarios in the library. Sometimes, it is required to use connections in a stateful way: request req; req.add_simple_query(&quot;BEGIN&quot;); // start a transaction manually req.add_query(&quot;SELECT * FROM library WHERE author = $1 FOR UPDATE&quot;, {&quot;Ruben&quot;}); // lock rows co_await conn.async_exec(req, lib); // Do something in the client that depends on lib if (lib.id == &quot;Boost.MySQL&quot;) co_return; // don&#39;t // Now compose another request that depends on what we read from lib req.clear(); req.add_query(&quot;UPDATE library SET status = &#39;deprecated&#39; WHERE id = $1&quot;, {lib.id}); req.add_simple_query(&quot;COMMIT&quot;); co_await conn.async_exec(req, ignore); The key point here is that this pattern requires exclusive access to conn. No other requests should be interleaved between the first and the second async_exec invocations. The best way to solve this is by using a connection pool. This is what client code could look like: co_await pool.async_exec([&amp;] (connection&amp; conn) -&gt; asio::awaitable&lt;system::error_code&gt; { request req; req.add_simple_query(&quot;BEGIN&quot;); req.add_query(&quot;SELECT balance, status FROM accounts WHERE user_id = $1 FOR UPDATE&quot;, {user_id}); account_info acc; co_await conn.async_exec(req, into(acc)); // Check if account has sufficient funds and is active if (acc.balance &lt; payment_amount || acc.status != &quot;active&quot;) co_return error::insufficient_funds; // Call external payment gateway API - this CANNOT be done in SQL auto result = co_await payment_gateway.process_charge(user_id, payment_amount); // Compose next request based on the external API response req.clear(); if (result.success) { req.add_query( &quot;UPDATE accounts SET balance = balance - $1 WHERE user_id = $2&quot;, {payment_amount, user_id} ); req.add_simple_query(&quot;COMMIT&quot;); } co_await conn.async_exec(req, ignore); // The connection is automatically returned to the pool when this coroutine completes co_return result.success ? error_code{} : error::payment_failed; }); I explicitly want to avoid having a connection_pool::async_get_connection() function, like in Boost.MySQL. This function returns a proxy object that grants access to a free connection. When destroyed, the connection is returned to the pool. This pattern looks great on paper, but runs into severe complications in multi-threaded code. The proxy object’s destructor needs to mutate the pool’s state, thus needing at least an asio::dispatch to the pool’s executor, which may or may not be a strand. It is so easy to get wrong that Boost.MySQL added a pool_params::thread_safe boolean option to take care of this automatically, adding extra complexity. Definitely something to avoid. SQL formatting As we’ve seen, the protocol has built-in support for adding parameters to queries (see placeholders like $1). These placeholders are expanded in the server securely. While this covers most cases, sometimes we need to generate SQL that is too dynamic to be handled by the server. For instance, a website might allow multiple optional filters, translating into WHERE clauses that might or might not be present. These use cases require SQL generated in the client. To do so, we need a way of formatting user-supplied values without running into SQL injection vulnerabilities. The final piece of the library becomes a format_sql function akin to the one in Boost.MySQL. Final thoughts While the plan is clear, there is still much to be done here. There are dedicated APIs for high-throughput data copying and push notifications that need to be implemented. Some of the described APIs have a solid working implementation, while others still need some work. All in all, I hope that this library can soon reach a state where it can be useful to people." />
<meta property="og:description" content="Do you know Boost.MySQL? If you’ve been reading my posts, you probably do. Many people have wondered ‘why not Postgres?’. Well, the time is now. TL;DR: I’m writing the equivalent of Boost.MySQL, but for PostgreSQL. You can find the code here. Since libPQ is already a good library, the NativePG project intends to be more ambitious than Boost.MySQL. In addition to the expected Asio interface, I intend to provide a sans-io API that exposes primitives like message serialization. Throughout this post, I will go into the intended library design and the rationales behind its design. The lowest level: message serialization PostgreSQL clients communicate with the server using a binary protocol on top of TCP, termed the frontend/backend protocol. The protocol defines a set of messages used for interactions. For example, when running a query, the following happens: ┌────────┐ ┌────────┐ │ Client │ │ Server │ └───┬────┘ └───┬────┘ │ │ │ Query │ │ ──────────────────────────────────────────&gt; │ │ │ │ RowDescription │ │ &lt;────────────────────────────────────────── │ │ │ │ DataRow │ │ &lt;────────────────────────────────────────── │ │ │ │ CommandComplete │ │ &lt;────────────────────────────────────────── │ │ │ │ ReadyForQuery │ │ &lt;────────────────────────────────────────── │ │ │ In the lowest layer, this library provides functions to serialize and parse such messages. The goal here is being as efficient as possible. Parsing functions are non-allocating, and use an approach inspired by Boost.Url collections: Parsing database types The PostgreSQL type system is quite rich. In addition to the usual SQL built-in types, it supports advanced scalars like UUIDs, arrays and user-defined aggregates. When running a query, libPQ exposes retrieved data as either raw text or bytes. This is what the server sends in the DataRow packets shown above. To do something useful with the data, users likely need parsing and serializing such types. The next layer of NativePG is in charge of providing such functions. This will likely contain some extension points for users to plug in their types. This is the general form of such functions: system::error_code parse(span&lt;const std::byte&gt; from, T&amp; to, const connection_state&amp;); void serialize(const T&amp; from, dynamic_buffer&amp; to, const connection_state&amp;); Note that some types might require access to session configuration. For instance, dates may be expressed using different wire formats depending on the connection’s runtime settings. At the time of writing, only ints and strings are supported, but this will be extended soon. Composing requests Efficiency in database communication is achieved with pipelining. A network round-trip with the server is worth a thousand allocations in the client. It is thus critical that: The protocol properly supports pipelining. This is the case with PostgreSQL. The client should expose an interface to it, and make it very easy to use. libPQ does the first, and NativePG intends to achieve the second. NativePG pipelines by default. In NativePG, a request object is always a pipeline: // Create a request request req; // These two queries will be executed as part of a pipeline req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); req.add_query(&quot;DELETE FROM libs WHERE author &lt;&gt; $1&quot;, {&quot;Ruben&quot;}); Everything you may ask the server can be added to request. This includes preparing and executing statements, establishing pipeline synchronization points, and so on. It aims to be close enough to the protocol to be powerful, while also exposing high-level functions to make things easier. Reading responses Like request, the core response mechanism aims to be as close to the protocol as possible. Since use cases here are much more varied, there is no single response class, but a concept, instead. This is what a response_handler looks like: struct my_handler { // Check that the handler is compatible with the request, // and prepare any required data structures. Called once at the beginning handler_setup_result setup(const request&amp; req, std::size_t pipeline_offset); // Called once for every message received from the server // (e.g. `RowDescription`, `DataRow`, `CommandComplete`) void on_message(const any_request_message&amp; msg); // The overall result of the operation (error_code + diagnostic string). // Called after the operation has finished. const extended_error&amp; result() const; }; Note that on_message is not allowed to report errors. Even if a handler encounters a problem with a message (imagine finding a NULL for a field where the user isn’t expecting one), this is a user error, rather than a protocol error. Subsequent steps in the pipeline must not be affected by this. This is powerful but very low-level. Using this mechanism, the library exposes an interface to parse the result of a query into a user-supplied struct, using Boost.Describe: struct library { std::int32_t id; std::string name; std::string cpp_version; }; BOOST_DESCRIBE_STRUCT(library, (), (id, name, cpp_version)) // ... std::vector&lt;library&gt; libs; auto handler = nativepg::into(libs); // this is a valid response_handler Network algorithms Given a user request and response handler, how do we send these to the server? We need a set of network algorithms to achieve this. Some of these are trivial: sending a request to the server is an asio::write on the request’s buffer. Others, however, are more involved: Reading a pipeline response needs to verify that the message sequence is what we expected, for security, and handle errors gracefully. The handshake algorithm, in charge of authentication when we connect to the server, needs to respond to server authentication challenges, which may come in different forms. Writing these using asio::async_compose is problematic because: They become tied to Boost.Asio. They are difficult to test. They result in long compile times and code bloat due to templating. At the moment, these are written as finite state machines, similar to how OpenSSL behaves in non-blocking mode: // Reads the response of a pipeline (simplified). // This is a hand-wired generator. class read_response_fsm { public: // User-supplied arguments: request and response read_response_fsm(const request&amp; req, response_handler_ref handler); // Yielded to signal that we should read from the server struct read_args { span&lt;std::byte&gt; buffer; }; // Yielded to signal that we&#39;re done struct done_args { system::error_code result; }; variant&lt;read_args, done_args&gt; resume(connection_state&amp;, system::error_code io_result, std::size_t bytes_transferred); }; The idea is that higher-level code should call resume until it returns a done_args value. This allows de-coupling from the underlying I/O runtime. Since NativePG targets C++20, I’m considering rewriting this as a coroutine. Boost.Capy (currently under development - hopefully part of Boost soon) could be a good candidate for this. Putting everything together: the Asio interface At the end of the day, most users just want a connection object they can easily use. Once all the sans-io parts are working, writing it is pretty straight-forward. This is what end user code looks like: // Create a connection connection conn{co_await asio::this_coro::executor}; // Connect co_await conn.async_connect( {.hostname = &quot;localhost&quot;, .username = &quot;postgres&quot;, .password = &quot;&quot;, .database = &quot;postgres&quot;} ); std::cout &lt;&lt; &quot;Startup complete\n&quot;; // Compose our request and response request req; req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); std::vector&lt;library&gt; libs; // Run the request co_await conn.async_exec(req, into(libs)); Auto-batch connections While connection is good, experience has shown me that it’s still too low-level for most users: Connection establishment is manual with async_connect. No built-in reconnection or health checks. No built-in concurrent execution of requests. That is, async_exec first writes the request, then reads the response. Other requests may not be executed during this period. This limits the connection’s throughput. For this reason, NativePG will provide some higher-level interfaces that will make server communication easier and more efficient. To get a feel of what we need, we should first understand the two main usage patterns that we expect. Most of the time, connections are used in a stateless way. For example, consider querying data from the server: request req; req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); co_await conn.async_exec(req, res); This query is not mutating connection state in any way. Other queries could be inserted before and after it without making any difference. I plan to add a higher-level connection type, similar to redis::connection in Boost.Redis, that automatically batches concurrent requests and handles reconnection. The key differences with connection would be: Several independent tasks can share an auto-batch connection. This is an error for connection. If several requests are queued at the same time, the connection may send them together to the server using a single system call. There is no async_connect in an auto-batch connection. Reconnection is handled automatically. Note that this pattern is not exclusive to read-only or individual queries. Transactions can work by using protocol features: request req; req.set_autosync(false); // All subsequent queries are part of the same transaction req.add_query(&quot;UPDATE table1 SET x = $1 WHERE y = 2&quot;, {42}); req.add_query(&quot;UPDATE table2 SET x = $1 WHERE y = 42&quot;, {2}); req.add_sync(); // The two updates run atomically co_await conn.async_exec(req, res); Connection pools I mentioned there were two main usage scenarios in the library. Sometimes, it is required to use connections in a stateful way: request req; req.add_simple_query(&quot;BEGIN&quot;); // start a transaction manually req.add_query(&quot;SELECT * FROM library WHERE author = $1 FOR UPDATE&quot;, {&quot;Ruben&quot;}); // lock rows co_await conn.async_exec(req, lib); // Do something in the client that depends on lib if (lib.id == &quot;Boost.MySQL&quot;) co_return; // don&#39;t // Now compose another request that depends on what we read from lib req.clear(); req.add_query(&quot;UPDATE library SET status = &#39;deprecated&#39; WHERE id = $1&quot;, {lib.id}); req.add_simple_query(&quot;COMMIT&quot;); co_await conn.async_exec(req, ignore); The key point here is that this pattern requires exclusive access to conn. No other requests should be interleaved between the first and the second async_exec invocations. The best way to solve this is by using a connection pool. This is what client code could look like: co_await pool.async_exec([&amp;] (connection&amp; conn) -&gt; asio::awaitable&lt;system::error_code&gt; { request req; req.add_simple_query(&quot;BEGIN&quot;); req.add_query(&quot;SELECT balance, status FROM accounts WHERE user_id = $1 FOR UPDATE&quot;, {user_id}); account_info acc; co_await conn.async_exec(req, into(acc)); // Check if account has sufficient funds and is active if (acc.balance &lt; payment_amount || acc.status != &quot;active&quot;) co_return error::insufficient_funds; // Call external payment gateway API - this CANNOT be done in SQL auto result = co_await payment_gateway.process_charge(user_id, payment_amount); // Compose next request based on the external API response req.clear(); if (result.success) { req.add_query( &quot;UPDATE accounts SET balance = balance - $1 WHERE user_id = $2&quot;, {payment_amount, user_id} ); req.add_simple_query(&quot;COMMIT&quot;); } co_await conn.async_exec(req, ignore); // The connection is automatically returned to the pool when this coroutine completes co_return result.success ? error_code{} : error::payment_failed; }); I explicitly want to avoid having a connection_pool::async_get_connection() function, like in Boost.MySQL. This function returns a proxy object that grants access to a free connection. When destroyed, the connection is returned to the pool. This pattern looks great on paper, but runs into severe complications in multi-threaded code. The proxy object’s destructor needs to mutate the pool’s state, thus needing at least an asio::dispatch to the pool’s executor, which may or may not be a strand. It is so easy to get wrong that Boost.MySQL added a pool_params::thread_safe boolean option to take care of this automatically, adding extra complexity. Definitely something to avoid. SQL formatting As we’ve seen, the protocol has built-in support for adding parameters to queries (see placeholders like $1). These placeholders are expanded in the server securely. While this covers most cases, sometimes we need to generate SQL that is too dynamic to be handled by the server. For instance, a website might allow multiple optional filters, translating into WHERE clauses that might or might not be present. These use cases require SQL generated in the client. To do so, we need a way of formatting user-supplied values without running into SQL injection vulnerabilities. The final piece of the library becomes a format_sql function akin to the one in Boost.MySQL. Final thoughts While the plan is clear, there is still much to be done here. There are dedicated APIs for high-throughput data copying and push notifications that need to be implemented. Some of the described APIs have a solid working implementation, while others still need some work. All in all, I hope that this library can soon reach a state where it can be useful to people." />
<link rel="canonical" href="http://cppalliance.org/ruben/2026/01/23/Ruben2025Q4Update.html" />
<meta property="og:url" content="http://cppalliance.org/ruben/2026/01/23/Ruben2025Q4Update.html" />
<meta property="og:site_name" content="The C++ Alliance" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-01-23T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A postgres library for Boost" />
<meta name="twitter:site" content="@CPPAlliance" />
<script type="application/ld+json">
{"description":"Do you know Boost.MySQL? If you’ve been reading my posts, you probably do. Many people have wondered ‘why not Postgres?’. Well, the time is now. TL;DR: I’m writing the equivalent of Boost.MySQL, but for PostgreSQL. You can find the code here. Since libPQ is already a good library, the NativePG project intends to be more ambitious than Boost.MySQL. In addition to the expected Asio interface, I intend to provide a sans-io API that exposes primitives like message serialization. Throughout this post, I will go into the intended library design and the rationales behind its design. The lowest level: message serialization PostgreSQL clients communicate with the server using a binary protocol on top of TCP, termed the frontend/backend protocol. The protocol defines a set of messages used for interactions. For example, when running a query, the following happens: ┌────────┐ ┌────────┐ │ Client │ │ Server │ └───┬────┘ └───┬────┘ │ │ │ Query │ │ ──────────────────────────────────────────&gt; │ │ │ │ RowDescription │ │ &lt;────────────────────────────────────────── │ │ │ │ DataRow │ │ &lt;────────────────────────────────────────── │ │ │ │ CommandComplete │ │ &lt;────────────────────────────────────────── │ │ │ │ ReadyForQuery │ │ &lt;────────────────────────────────────────── │ │ │ In the lowest layer, this library provides functions to serialize and parse such messages. The goal here is being as efficient as possible. Parsing functions are non-allocating, and use an approach inspired by Boost.Url collections: Parsing database types The PostgreSQL type system is quite rich. In addition to the usual SQL built-in types, it supports advanced scalars like UUIDs, arrays and user-defined aggregates. When running a query, libPQ exposes retrieved data as either raw text or bytes. This is what the server sends in the DataRow packets shown above. To do something useful with the data, users likely need parsing and serializing such types. The next layer of NativePG is in charge of providing such functions. This will likely contain some extension points for users to plug in their types. This is the general form of such functions: system::error_code parse(span&lt;const std::byte&gt; from, T&amp; to, const connection_state&amp;); void serialize(const T&amp; from, dynamic_buffer&amp; to, const connection_state&amp;); Note that some types might require access to session configuration. For instance, dates may be expressed using different wire formats depending on the connection’s runtime settings. At the time of writing, only ints and strings are supported, but this will be extended soon. Composing requests Efficiency in database communication is achieved with pipelining. A network round-trip with the server is worth a thousand allocations in the client. It is thus critical that: The protocol properly supports pipelining. This is the case with PostgreSQL. The client should expose an interface to it, and make it very easy to use. libPQ does the first, and NativePG intends to achieve the second. NativePG pipelines by default. In NativePG, a request object is always a pipeline: // Create a request request req; // These two queries will be executed as part of a pipeline req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); req.add_query(&quot;DELETE FROM libs WHERE author &lt;&gt; $1&quot;, {&quot;Ruben&quot;}); Everything you may ask the server can be added to request. This includes preparing and executing statements, establishing pipeline synchronization points, and so on. It aims to be close enough to the protocol to be powerful, while also exposing high-level functions to make things easier. Reading responses Like request, the core response mechanism aims to be as close to the protocol as possible. Since use cases here are much more varied, there is no single response class, but a concept, instead. This is what a response_handler looks like: struct my_handler { // Check that the handler is compatible with the request, // and prepare any required data structures. Called once at the beginning handler_setup_result setup(const request&amp; req, std::size_t pipeline_offset); // Called once for every message received from the server // (e.g. `RowDescription`, `DataRow`, `CommandComplete`) void on_message(const any_request_message&amp; msg); // The overall result of the operation (error_code + diagnostic string). // Called after the operation has finished. const extended_error&amp; result() const; }; Note that on_message is not allowed to report errors. Even if a handler encounters a problem with a message (imagine finding a NULL for a field where the user isn’t expecting one), this is a user error, rather than a protocol error. Subsequent steps in the pipeline must not be affected by this. This is powerful but very low-level. Using this mechanism, the library exposes an interface to parse the result of a query into a user-supplied struct, using Boost.Describe: struct library { std::int32_t id; std::string name; std::string cpp_version; }; BOOST_DESCRIBE_STRUCT(library, (), (id, name, cpp_version)) // ... std::vector&lt;library&gt; libs; auto handler = nativepg::into(libs); // this is a valid response_handler Network algorithms Given a user request and response handler, how do we send these to the server? We need a set of network algorithms to achieve this. Some of these are trivial: sending a request to the server is an asio::write on the request’s buffer. Others, however, are more involved: Reading a pipeline response needs to verify that the message sequence is what we expected, for security, and handle errors gracefully. The handshake algorithm, in charge of authentication when we connect to the server, needs to respond to server authentication challenges, which may come in different forms. Writing these using asio::async_compose is problematic because: They become tied to Boost.Asio. They are difficult to test. They result in long compile times and code bloat due to templating. At the moment, these are written as finite state machines, similar to how OpenSSL behaves in non-blocking mode: // Reads the response of a pipeline (simplified). // This is a hand-wired generator. class read_response_fsm { public: // User-supplied arguments: request and response read_response_fsm(const request&amp; req, response_handler_ref handler); // Yielded to signal that we should read from the server struct read_args { span&lt;std::byte&gt; buffer; }; // Yielded to signal that we&#39;re done struct done_args { system::error_code result; }; variant&lt;read_args, done_args&gt; resume(connection_state&amp;, system::error_code io_result, std::size_t bytes_transferred); }; The idea is that higher-level code should call resume until it returns a done_args value. This allows de-coupling from the underlying I/O runtime. Since NativePG targets C++20, I’m considering rewriting this as a coroutine. Boost.Capy (currently under development - hopefully part of Boost soon) could be a good candidate for this. Putting everything together: the Asio interface At the end of the day, most users just want a connection object they can easily use. Once all the sans-io parts are working, writing it is pretty straight-forward. This is what end user code looks like: // Create a connection connection conn{co_await asio::this_coro::executor}; // Connect co_await conn.async_connect( {.hostname = &quot;localhost&quot;, .username = &quot;postgres&quot;, .password = &quot;&quot;, .database = &quot;postgres&quot;} ); std::cout &lt;&lt; &quot;Startup complete\\n&quot;; // Compose our request and response request req; req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); std::vector&lt;library&gt; libs; // Run the request co_await conn.async_exec(req, into(libs)); Auto-batch connections While connection is good, experience has shown me that it’s still too low-level for most users: Connection establishment is manual with async_connect. No built-in reconnection or health checks. No built-in concurrent execution of requests. That is, async_exec first writes the request, then reads the response. Other requests may not be executed during this period. This limits the connection’s throughput. For this reason, NativePG will provide some higher-level interfaces that will make server communication easier and more efficient. To get a feel of what we need, we should first understand the two main usage patterns that we expect. Most of the time, connections are used in a stateless way. For example, consider querying data from the server: request req; req.add_query(&quot;SELECT * FROM libs WHERE author = $1&quot;, {&quot;Ruben&quot;}); co_await conn.async_exec(req, res); This query is not mutating connection state in any way. Other queries could be inserted before and after it without making any difference. I plan to add a higher-level connection type, similar to redis::connection in Boost.Redis, that automatically batches concurrent requests and handles reconnection. The key differences with connection would be: Several independent tasks can share an auto-batch connection. This is an error for connection. If several requests are queued at the same time, the connection may send them together to the server using a single system call. There is no async_connect in an auto-batch connection. Reconnection is handled automatically. Note that this pattern is not exclusive to read-only or individual queries. Transactions can work by using protocol features: request req; req.set_autosync(false); // All subsequent queries are part of the same transaction req.add_query(&quot;UPDATE table1 SET x = $1 WHERE y = 2&quot;, {42}); req.add_query(&quot;UPDATE table2 SET x = $1 WHERE y = 42&quot;, {2}); req.add_sync(); // The two updates run atomically co_await conn.async_exec(req, res); Connection pools I mentioned there were two main usage scenarios in the library. Sometimes, it is required to use connections in a stateful way: request req; req.add_simple_query(&quot;BEGIN&quot;); // start a transaction manually req.add_query(&quot;SELECT * FROM library WHERE author = $1 FOR UPDATE&quot;, {&quot;Ruben&quot;}); // lock rows co_await conn.async_exec(req, lib); // Do something in the client that depends on lib if (lib.id == &quot;Boost.MySQL&quot;) co_return; // don&#39;t // Now compose another request that depends on what we read from lib req.clear(); req.add_query(&quot;UPDATE library SET status = &#39;deprecated&#39; WHERE id = $1&quot;, {lib.id}); req.add_simple_query(&quot;COMMIT&quot;); co_await conn.async_exec(req, ignore); The key point here is that this pattern requires exclusive access to conn. No other requests should be interleaved between the first and the second async_exec invocations. The best way to solve this is by using a connection pool. This is what client code could look like: co_await pool.async_exec([&amp;] (connection&amp; conn) -&gt; asio::awaitable&lt;system::error_code&gt; { request req; req.add_simple_query(&quot;BEGIN&quot;); req.add_query(&quot;SELECT balance, status FROM accounts WHERE user_id = $1 FOR UPDATE&quot;, {user_id}); account_info acc; co_await conn.async_exec(req, into(acc)); // Check if account has sufficient funds and is active if (acc.balance &lt; payment_amount || acc.status != &quot;active&quot;) co_return error::insufficient_funds; // Call external payment gateway API - this CANNOT be done in SQL auto result = co_await payment_gateway.process_charge(user_id, payment_amount); // Compose next request based on the external API response req.clear(); if (result.success) { req.add_query( &quot;UPDATE accounts SET balance = balance - $1 WHERE user_id = $2&quot;, {payment_amount, user_id} ); req.add_simple_query(&quot;COMMIT&quot;); } co_await conn.async_exec(req, ignore); // The connection is automatically returned to the pool when this coroutine completes co_return result.success ? error_code{} : error::payment_failed; }); I explicitly want to avoid having a connection_pool::async_get_connection() function, like in Boost.MySQL. This function returns a proxy object that grants access to a free connection. When destroyed, the connection is returned to the pool. This pattern looks great on paper, but runs into severe complications in multi-threaded code. The proxy object’s destructor needs to mutate the pool’s state, thus needing at least an asio::dispatch to the pool’s executor, which may or may not be a strand. It is so easy to get wrong that Boost.MySQL added a pool_params::thread_safe boolean option to take care of this automatically, adding extra complexity. Definitely something to avoid. SQL formatting As we’ve seen, the protocol has built-in support for adding parameters to queries (see placeholders like $1). These placeholders are expanded in the server securely. While this covers most cases, sometimes we need to generate SQL that is too dynamic to be handled by the server. For instance, a website might allow multiple optional filters, translating into WHERE clauses that might or might not be present. These use cases require SQL generated in the client. To do so, we need a way of formatting user-supplied values without running into SQL injection vulnerabilities. The final piece of the library becomes a format_sql function akin to the one in Boost.MySQL. Final thoughts While the plan is clear, there is still much to be done here. There are dedicated APIs for high-throughput data copying and push notifications that need to be implemented. Some of the described APIs have a solid working implementation, while others still need some work. All in all, I hope that this library can soon reach a state where it can be useful to people.","@type":"BlogPosting","url":"http://cppalliance.org/ruben/2026/01/23/Ruben2025Q4Update.html","headline":"A postgres library for Boost","dateModified":"2026-01-23T00:00:00+00:00","datePublished":"2026-01-23T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://cppalliance.org/ruben/2026/01/23/Ruben2025Q4Update.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



<link href="/css/prism.css" rel="stylesheet">


<link href='/feed.xml' rel='alternate' type='application/atom+xml'>

<!-- Twitter Card Start -->




  
    <meta name="twitter:image" content="https://cppalliance.org/images/logo.png">
  

<!-- Twitter Card End -->

<script defer data-domain="cppalliance.org" src="https://plausible.io/js/script.js"></script>

</head>

<body id='body' class="line-numbers">

  <!-- Navigation -->
  <nav class='nav dark'>
    <a href='/'>
      <img class='logo' alt='cpp-alliance-logo' src='/images/logo.svg' />
    </a>
    <div class='hamburger' id='nav-hamburger'>
      <span class='hamburger-line'></span>
      <span class='hamburger-line'></span>
      <span class='hamburger-line'></span>
    </div>
    <div class='nav-items' id='nav-items'>
      <div class="nav-item"><a class="nav-link nav-link-mobile" href="/">Home</a></div>
      <div class="nav-item"><a class="nav-link nav-link-mobile" href="/#mission">Mission</a></div>
      <div class="nav-item"><a class="nav-link nav-link-mobile" href="/#team">Team</a></div>
      <div class="nav-item"><a class="nav-link nav-link-mobile" href="/#news">News</a></div>
      <div class="nav-item"><a class="nav-link nav-link-mobile" href="/#links">Links</a></div>
      <div class="nav-item"><a class="nav-link nav-link-mobile" href="/#faq">FAQ</a></div>
      <div class="nav-item"><a class="nav-link nav-link-mobile" href="/#contact">Contact</a></div>
      <div class='socials'>
        <div class='connect-content'>
          <div class='row row-sm'>
            <div class='col-fourth col-fourth-sm social-link'>
              <a class='social-icon nav-link-mobile' href="https://github.com/CPPAlliance">
                <div class='social-icon-img-wrapper'>
                  <img class='social-icon-img github' alt='github-logo' src='/images/icons/github.svg' />
                </div>
                <span class='social-icon-text'>GitHub</span>
              </a>
            </div>
            <div class='col-fourth col-fourth-sm social-link'>
              <a class='social-icon nav-link-mobile' href="https://www.facebook.com/CPPAlliance/">
                <div class='social-icon-img-wrapper'>
                  <img class='social-icon-img facebook' alt='facebook-logo' src='/images/icons/facebook.svg' />
                </div>
                <span class='social-icon-text'>Facebook</span>
              </a>
            </div>
            <div class='col-fourth col-fourth-sm social-link'>
              <a class='social-icon nav-link-mobile' href="https://x.com/cppalliance">
                <div class='social-icon-img-wrapper'>
                  <img class='social-icon-img twitter' alt='x-logo' src='/images/icons/twitter.svg' />
                </div>
                <span class='social-icon-text'>X</span>
              </a>
            </div>
            <div class='col-fourth col-fourth-sm social-link'>
              <a class='social-icon nav-link-mobile' href="https://www.linkedin.com/in/cppalliance/">
                <div class='social-icon-img-wrapper'>
                  <img class='social-icon-img linkedin' alt='linkedin-logo' src='/images/icons/linkedin.svg' />
                </div>
                <span class='social-icon-text'>LinkedIn</span>
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </nav>






  <div class='post'>
  <div class='current-article'>
    

    <section class='section article'>
      

      <article>
      <div class="title-section center">
        <h2 class='text-l news-title no-border'>A postgres library for Boost</h2>
        
        
                
                
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                    
        

        

        
          <div class='author d-iblock'>
            <a class='link author-img-link' href="/people/ruben">
              <img class='author-img' src='/images/people/ruben.jpg' alt='Portrait of Ruben Perez' />
            </a>
            <span class='text-xxs author-name'>
                <a class='link' href="/people/ruben">
                  Ruben Perez
                </a> &middot; Jan 23, 2026
            </span>
          </div>
        
      </div>
        <div class='text-xxs content-text generated-content'>
          <p>Do you know Boost.MySQL? If you’ve been reading my posts, you probably do.
Many people have wondered ‘why not Postgres?’. Well, the time is now.
TL;DR: I’m writing the equivalent of Boost.MySQL, but for PostgreSQL.
You can find the code <a href="https://github.com/anarthal/nativepg">here</a>.</p>

<p>Since libPQ is already a good library, the NativePG project intends
to be more ambitious than Boost.MySQL. In addition to the expected
Asio interface, I intend to provide a sans-io API that exposes primitives
like message serialization.</p>

<p>Throughout this post, I will go into the intended library design and the rationales
behind its design.</p>

<h2 id="the-lowest-level-message-serialization">The lowest level: message serialization</h2>

<p>PostgreSQL clients communicate with the server using
a binary protocol on top of TCP, termed <a href="https://www.postgresql.org/docs/current/protocol.html">the frontend/backend protocol</a>.
The protocol defines a set of messages used for interactions. For example, when running a query, the following happens:</p>

<pre><code>┌────────┐                                    ┌────────┐
│ Client │                                    │ Server │
└───┬────┘                                    └───┬────┘
    │                                             │
    │  Query                                      │
    │ ──────────────────────────────────────────&gt; │
    │                                             │
    │                        RowDescription       │
    │ &lt;────────────────────────────────────────── │
    │                                             │
    │                              DataRow        │
    │ &lt;────────────────────────────────────────── │
    │                                             │
    │                        CommandComplete      │
    │ &lt;────────────────────────────────────────── │
    │                                             │
    │                        ReadyForQuery        │
    │ &lt;────────────────────────────────────────── │
    │                                             │
</code></pre>

<p>In the lowest layer, this library provides functions to serialize and parse
such messages. The goal here is being as efficient as possible.
Parsing functions are non-allocating, and use an approach inspired by
Boost.Url collections:</p>

<h2 id="parsing-database-types">Parsing database types</h2>

<p>The PostgreSQL type system is quite rich. In addition to the usual SQL built-in types,
it supports advanced scalars like UUIDs, arrays and user-defined aggregates.</p>

<p>When running a query, libPQ exposes retrieved data as either raw text or bytes.
This is what the server sends in the <code>DataRow</code> packets shown above.
To do something useful with the data, users likely need parsing and serializing
such types.</p>

<p>The next layer of NativePG is in charge of providing such functions.
This will likely contain some extension points for users to plug in their types.
This is the general form of such functions:</p>

<pre><code class="language-cpp">system::error_code parse(span&lt;const std::byte&gt; from, T&amp; to, const connection_state&amp;);
void serialize(const T&amp; from, dynamic_buffer&amp; to, const connection_state&amp;);
</code></pre>

<p>Note that some types might require access to session configuration.
For instance, dates may be expressed using different wire formats depending
on the connection’s runtime settings.</p>

<p>At the time of writing, only ints and strings are supported,
but this will be extended soon.</p>

<h2 id="composing-requests">Composing requests</h2>

<p>Efficiency in database communication is achieved with pipelining.
A network round-trip with the server is worth a thousand allocations in the client.
It is thus critical that:</p>

<ul>
  <li>The protocol properly supports pipelining. This is the case with PostgreSQL.</li>
  <li>The client should expose an interface to it, and make it very easy to use.
libPQ does the first, and NativePG intends to achieve the second.</li>
</ul>

<p>NativePG pipelines by default. In NativePG, a <code>request</code> object is always
a pipeline:</p>

<pre><code class="language-cpp">// Create a request
request req;

// These two queries will be executed as part of a pipeline
req.add_query("SELECT * FROM libs WHERE author = $1", {"Ruben"});
req.add_query("DELETE FROM libs WHERE author &lt;&gt; $1", {"Ruben"});
</code></pre>

<p>Everything you may ask the server can be added to <code>request</code>.
This includes preparing and executing statements, establishing
pipeline synchronization points, and so on.
It aims to be close enough to the protocol to be powerful,
while also exposing high-level functions to make things easier.</p>

<h2 id="reading-responses">Reading responses</h2>

<p>Like <code>request</code>, the core response mechanism aims to be as close
to the protocol as possible. Since use cases here are much more varied,
there is no single <code>response</code> class, but a concept, instead.
This is what a <code>response_handler</code> looks like:</p>

<pre><code class="language-cpp">
struct my_handler {
    // Check that the handler is compatible with the request,
    // and prepare any required data structures. Called once at the beginning
    handler_setup_result setup(const request&amp; req, std::size_t pipeline_offset);

    // Called once for every message received from the server
    // (e.g. `RowDescription`, `DataRow`, `CommandComplete`)
    void on_message(const any_request_message&amp; msg);

    // The overall result of the operation (error_code + diagnostic string).
    // Called after the operation has finished.
    const extended_error&amp; result() const;
};

</code></pre>

<p>Note that <code>on_message</code> is not allowed to report errors.
Even if a handler encounters a problem with a message
(imagine finding a <code>NULL</code> for a field where the user isn’t expecting one),
this is a user error, rather than a protocol error.
Subsequent steps in the pipeline must not be affected by this.</p>

<p>This is powerful but very low-level. Using this mechanism, the library
exposes an interface to parse the result of a query into a user-supplied
struct, using Boost.Describe:</p>

<pre><code class="language-cpp">struct library
{
    std::int32_t id;
    std::string name;
    std::string cpp_version;
};
BOOST_DESCRIBE_STRUCT(library, (), (id, name, cpp_version))

// ...
std::vector&lt;library&gt; libs;
auto handler = nativepg::into(libs); // this is a valid response_handler
</code></pre>

<h2 id="network-algorithms">Network algorithms</h2>

<p>Given a user request and response handler, how do we send these to the server?
We need a set of network algorithms to achieve this. Some of these are trivial:
sending a request to the server is an <code>asio::write</code> on the request’s buffer.
Others, however, are more involved:</p>

<ul>
  <li>Reading a pipeline response needs to verify that the message
sequence is what we expected, for security, and handle errors gracefully.</li>
  <li>The handshake algorithm, in charge of authentication when we connect to the
server, needs to respond to server authentication challenges, which may
come in different forms.</li>
</ul>

<p>Writing these using <code>asio::async_compose</code> is problematic because:</p>

<ul>
  <li>They become tied to Boost.Asio.</li>
  <li>They are difficult to test.</li>
  <li>They result in long compile times and code bloat due to templating.</li>
</ul>

<p>At the moment, these are written as finite state machines, similar to
how OpenSSL behaves in non-blocking mode:</p>

<pre><code class="language-cpp">// Reads the response of a pipeline (simplified).
// This is a hand-wired generator.
class read_response_fsm {
public:
    // User-supplied arguments: request and response
    read_response_fsm(const request&amp; req, response_handler_ref handler);

    // Yielded to signal that we should read from the server
    struct read_args { span&lt;std::byte&gt; buffer; };

    // Yielded to signal that we're done
    struct done_args { system::error_code result; };

    variant&lt;read_args, done_args&gt;
    resume(connection_state&amp;, system::error_code io_result, std::size_t bytes_transferred);
};
</code></pre>

<p>The idea is that higher-level code should call <code>resume</code> until it returns
a <code>done_args</code> value. This allows de-coupling from the underlying I/O runtime.</p>

<p>Since NativePG targets C++20, I’m considering rewriting this as a coroutine.
Boost.Capy (currently under development - hopefully part of Boost soon)
could be a good candidate for this.</p>

<h2 id="putting-everything-together-the-asio-interface">Putting everything together: the Asio interface</h2>

<p>At the end of the day, most users just want a <code>connection</code> object they can easily
use. Once all the sans-io parts are working, writing it is pretty straight-forward.
This is what end user code looks like:</p>

<pre><code class="language-cpp">// Create a connection
connection conn{co_await asio::this_coro::executor};

// Connect
co_await conn.async_connect(
    {.hostname = "localhost", .username = "postgres", .password = "", .database = "postgres"}
);
std::cout &lt;&lt; "Startup complete\n";

// Compose our request and response
request req;
req.add_query("SELECT * FROM libs WHERE author = $1", {"Ruben"});
std::vector&lt;library&gt; libs;

// Run the request
co_await conn.async_exec(req, into(libs));
</code></pre>

<h2 id="auto-batch-connections">Auto-batch connections</h2>

<p>While <code>connection</code> is good, experience has shown me that it’s still
too low-level for most users:</p>

<ul>
  <li>Connection establishment is manual with <code>async_connect</code>.</li>
  <li>No built-in reconnection or health checks.</li>
  <li>No built-in concurrent execution of requests.
That is, <code>async_exec</code> first writes the request, then reads the response.
Other requests may not be executed during this period.
This limits the connection’s throughput.</li>
</ul>

<p>For this reason, NativePG will provide some higher-level interfaces
that will make server communication easier and more efficient.
To get a feel of what we need, we should first understand
the two main usage patterns that we expect.</p>

<p>Most of the time, connections are used in a <strong>stateless</strong> way.
For example, consider querying data from the server:</p>

<pre><code class="language-cpp">request req;
req.add_query("SELECT * FROM libs WHERE author = $1", {"Ruben"});
co_await conn.async_exec(req, res);
</code></pre>

<p>This query is not mutating connection state in any way.
Other queries could be inserted before and after it without
making any difference.</p>

<p>I plan to add a higher-level connection type, similar to
<code>redis::connection</code> in Boost.Redis, that automatically
batches concurrent requests and handles reconnection.
The key differences with <code>connection</code> would be:</p>

<ul>
  <li>Several independent tasks can share an auto-batch connection.
This is an error for <code>connection</code>.</li>
  <li>If several requests are queued at the same time,
the connection may send them together to the server using a single system call.</li>
  <li>There is no <code>async_connect</code> in an auto-batch connection.
Reconnection is handled automatically.</li>
</ul>

<p>Note that this pattern is not exclusive to read-only or
individual queries. Transactions can work by using protocol features:</p>

<pre><code class="language-cpp">request req;
req.set_autosync(false); // All subsequent queries are part of the same transaction
req.add_query("UPDATE table1 SET x = $1 WHERE y = 2", {42});
req.add_query("UPDATE table2 SET x = $1 WHERE y = 42", {2});
req.add_sync(); // The two updates run atomically
co_await conn.async_exec(req, res);
</code></pre>

<h2 id="connection-pools">Connection pools</h2>

<p>I mentioned there were two main usage scenarios in the library.
Sometimes, it is required to use connections in a <strong>stateful</strong> way:</p>

<pre><code class="language-cpp">request req;
req.add_simple_query("BEGIN"); // start a transaction manually
req.add_query("SELECT * FROM library WHERE author = $1 FOR UPDATE", {"Ruben"}); // lock rows
co_await conn.async_exec(req, lib);

// Do something in the client that depends on lib
if (lib.id == "Boost.MySQL")
    co_return; // don't

// Now compose another request that depends on what we read from lib
req.clear();
req.add_query("UPDATE library SET status = 'deprecated' WHERE id = $1", {lib.id});
req.add_simple_query("COMMIT");
co_await conn.async_exec(req, ignore);
</code></pre>

<p>The key point here is that this pattern requires exclusive access to <code>conn</code>.
No other requests should be interleaved between the first and the second
<code>async_exec</code> invocations.</p>

<p>The best way to solve this is by using a connection pool.
This is what client code could look like:</p>

<pre><code class="language-cpp">co_await pool.async_exec([&amp;] (connection&amp; conn) -&gt; asio::awaitable&lt;system::error_code&gt; {
    request req;
    req.add_simple_query("BEGIN");
    req.add_query("SELECT balance, status FROM accounts WHERE user_id = $1 FOR UPDATE", {user_id});

    account_info acc;
    co_await conn.async_exec(req, into(acc));

    // Check if account has sufficient funds and is active
    if (acc.balance &lt; payment_amount || acc.status != "active")
        co_return error::insufficient_funds;

    // Call external payment gateway API - this CANNOT be done in SQL
    auto result = co_await payment_gateway.process_charge(user_id, payment_amount);

    // Compose next request based on the external API response
    req.clear();
    if (result.success) {
        req.add_query(
            "UPDATE accounts SET balance = balance - $1 WHERE user_id = $2",
            {payment_amount, user_id}
        );
        req.add_simple_query("COMMIT");
    }
    co_await conn.async_exec(req, ignore);

    // The connection is automatically returned to the pool when this coroutine completes
    co_return result.success ? error_code{} : error::payment_failed;
});
</code></pre>

<p>I explicitly want to avoid having a <code>connection_pool::async_get_connection()</code>
function, like in Boost.MySQL. This function returns a proxy object that grants access
to a free connection. When destroyed, the connection is returned to the pool.
This pattern looks great on paper, but runs into severe complications in
multi-threaded code. The proxy object’s destructor needs to mutate the pool’s state,
thus needing at least an <code>asio::dispatch</code> to the pool’s executor, which may or may not
be a strand. It is so easy to get wrong that Boost.MySQL added a <code>pool_params::thread_safe</code> boolean
option to take care of this automatically, adding extra complexity. Definitely something to avoid.</p>

<h2 id="sql-formatting">SQL formatting</h2>

<p>As we’ve seen, the protocol has built-in support for adding
parameters to queries (see placeholders like <code>$1</code>). These placeholders
are expanded in the server securely.</p>

<p>While this covers most cases, sometimes we need to generate SQL
that is too dynamic to be handled by the server. For instance,
a website might allow multiple optional filters, translating into
<code>WHERE</code> clauses that might or might not be present.</p>

<p>These use cases require SQL generated in the client. To do so,
we need a way of formatting user-supplied values without
running into SQL injection vulnerabilities. The final piece
of the library becomes a <code>format_sql</code> function akin to the
one in Boost.MySQL.</p>

<h2 id="final-thoughts">Final thoughts</h2>

<p>While the plan is clear, there is still much to be done here.
There are dedicated APIs for high-throughput data copying and
push notifications that need to be implemented. Some of the described
APIs have a solid working implementation, while others still need
some work. All in all, I hope that this library can soon reach a state
where it can be useful to people.</p>

        </div>
      </article>
    </section>
  </div>

  <section class="section news bottom-layout" id='news'>
    <div class='section-title'>
      <h2 class='header text-xl recent-post-header'>All Posts by This Author</h2>
    </div>
    <div class='news-content formatted-text'>
      <ul>
        
        
        <li class='news-list-item '>
          <span class='text-xs news-date'>01/23/2026</span>
          <a class='text-l news-title link' href="/ruben/2026/01/23/Ruben2025Q4Update.html">A postgres library for Boost</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class='news-list-item '>
          <span class='text-xs news-date'>10/07/2025</span>
          <a class='text-l news-title link' href="/ruben/2025/10/07/Ruben2025Q3Update.html">Levelling up Boost.Redis</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class='news-list-item '>
          <span class='text-xs news-date'>07/10/2025</span>
          <a class='text-l news-title link' href="/ruben/2025/07/10/Ruben2025Q2Update.html">Ready, Set, Redis!</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class='news-list-item '>
          <span class='text-xs news-date'>04/10/2025</span>
          <a class='text-l news-title link' href="/ruben/2025/04/10/Ruben2025Q1Update.html">Moving Boost forward: Asio, coroutines, and maybe even modules</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class='news-list-item '>
          <span class='text-xs news-date'>01/06/2025</span>
          <a class='text-l news-title link' href="/ruben/2025/01/06/Ruben2024Q4Update.html">Boost.MySQL 1.87 and the new Boost citizens</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class='news-list-item '>
          <span class='text-xs news-date'>10/20/2024</span>
          <a class='text-l news-title link' href="/ruben/2024/10/20/Ruben2024Q3Update.html">Boost.MySQL 1.87 features: with_params and with_diagnostics</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class='news-list-item '>
          <span class='text-xs news-date'>10/27/2023</span>
          <a class='text-l news-title link' href="/ruben/2023/10/27/Rubens2023Q3Update.html">Ruben's Q3 2023 Update</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li>
          <a class='text-l all link' href="/news">View All Posts...</a>
        </li>
      </ul>
    </div>
  </section>

</div>


  <footer class='footer'>
    <p class='text-xxs footer-text'>
      <span class='line'>&copy; 2024 The C Plus Plus Alliance, Inc.</span>
      <span class='line'>Contact us at: <a href='mailto:%69%6E%66%6F@%63%70%70%61%6C%6C%69%61%6E%63%65.%6F%72%67'>info@cppalliance.org</a></span>
    </p>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src='/js/main.js'></script>
  
  <script src='/js/prism.js'></script>
  

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-76438364-18', 'auto');
    ga('send', 'pageview');
  </script>

</body>
</html>
